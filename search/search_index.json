{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Compilers Morteza Zakeri \u2020 \u2020 Ph.D. Student, Iran University of Science and Technology, Tehran, Iran (m-zakeri@live.com). Version 0.1.2 (29 March 2021) \u251c Download [PDF] version Abstract\u2014 O ur IUST-Compiler Course is now more practical than ever. This repository contains several code snippets that I developed to teach the ANTLR compiler generator at Iran University of Science and Technology (UST). Grammars have been written in ANTRL v4 format. For each grammar, the source code of Lexer and Parser is available in Python 3.x. The repository is assumed to be updated regularly. It would be appreciated if you use this repository by forking it. For any question please contact me m-zakeri[at]live.com . Introduction The course is intended to teach the students the basic techniques that underlie the practice of Compiler Construction. The course will introduce the theory and tools that can be standardly employed to perform syntax-directed translation of a high-level programming language into executable code. These techniques can also be employed in broader areas of application, whenever we need a syntax-directed analysis of symbolic expressions and languages and their translation into a lower-level description. They have multiple uses for man-machine interaction, including verification and program analysis. Objectives To learn the overall compiler architecture, To learn various parsing methods and techniques, To learn low-level code generation and optimization, To learn an intellectual paradigm in system programming and testing. Motivations Compiler construction is a microcosm of computer science! You dive into the heart of the system when you are building a compiler. Examples The following outputs could be generated by code snippets in this repository. Three addresses codes Figure 1 shows how a single pass compiler can generate three address code for assignment statements with minimum numbers of temporary variable, started with T : Figure 1: Examples of three addresses codes generated by ANTLR for AssignmentStatement grammar. Abstract Syntax Tree Abstract syntax trees (ASTs) are a useful abstraction when dealing with programming languages as an object for analysis or manipulation (e.g. compilation). Figure 2 demonstrates how a single pass compiler can generate abstract syntax three (AST) for assignment statements. Figure 2: Examples of abstract syntax tree (AST) generated by ANTLR for AssignmentStatement grammar. The above tree corresponds to the following expression: a1 := (2 + 12 * 3) / (6 - 19) Structure The following describes the structure of the repository: grammars gram1 : ANTLR hello world grammar. Expr1 : Simple grammar for handling mathematical expressions without any attribute and action. Expr2 : Simple attributed grammar for handling mathematical expressions with code() attribute. Expr3 : Currently, it is the same Expr2 grammar. AssignmentStatement1.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . AssignmentStatement2.g4 : It is the same AssignmentStatement1.g4 grammar plus attributes for code and type of rules. AssignmentStatement3.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . It provides semantic rules to perform type checking and semantic routines to generate intermediate representation. AssignmentStatement4.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . It provides semantic rules to perform type checking and semantic routines to generate intermediate representation. It has been implemented to generate intermediate representation (three addresses codes) with minimum number of \"temp\" variables. CPP14_v2 : ANTLR grammar for C++14 forked from the official ANTRL website. Some bugs have been fixed and also the rule identifiers have been added to the grammar rules. EMail.g4 : Lexical grammar to validate email addresses. EMail2.g4 : Lexical grammar to validate email addresses, fixing bugs in EMail.g4 language_apps The language_apps package currently contains Lexer and Parser codes for each grammar in directory grammars , with a main driver script to demonstrate the type checking and intermediate code generation based on semantic rules and semantic routines . terminal_batch_scripts The termina_batch_script directory contains several batch script to run ANTLR in terminal (Windows) to generate target code in JAVA language. The code snippets in this directory belong to my first experiences with ANTLR. Read more IUST compiler course official webpage ANTLR slides: PART 1: Introduction ANTLR slides: PART 2: Getting started in Java ANTLR slides: PART 3: Getting started in C#","title":"Home"},{"location":"#compilers","text":"Morteza Zakeri \u2020 \u2020 Ph.D. Student, Iran University of Science and Technology, Tehran, Iran (m-zakeri@live.com). Version 0.1.2 (29 March 2021) \u251c Download [PDF] version Abstract\u2014 O ur IUST-Compiler Course is now more practical than ever. This repository contains several code snippets that I developed to teach the ANTLR compiler generator at Iran University of Science and Technology (UST). Grammars have been written in ANTRL v4 format. For each grammar, the source code of Lexer and Parser is available in Python 3.x. The repository is assumed to be updated regularly. It would be appreciated if you use this repository by forking it. For any question please contact me m-zakeri[at]live.com .","title":"Compilers"},{"location":"#introduction","text":"The course is intended to teach the students the basic techniques that underlie the practice of Compiler Construction. The course will introduce the theory and tools that can be standardly employed to perform syntax-directed translation of a high-level programming language into executable code. These techniques can also be employed in broader areas of application, whenever we need a syntax-directed analysis of symbolic expressions and languages and their translation into a lower-level description. They have multiple uses for man-machine interaction, including verification and program analysis.","title":"Introduction"},{"location":"#objectives","text":"To learn the overall compiler architecture, To learn various parsing methods and techniques, To learn low-level code generation and optimization, To learn an intellectual paradigm in system programming and testing.","title":"Objectives"},{"location":"#motivations","text":"Compiler construction is a microcosm of computer science! You dive into the heart of the system when you are building a compiler.","title":"Motivations"},{"location":"#examples","text":"The following outputs could be generated by code snippets in this repository.","title":"Examples"},{"location":"#three-addresses-codes","text":"Figure 1 shows how a single pass compiler can generate three address code for assignment statements with minimum numbers of temporary variable, started with T : Figure 1: Examples of three addresses codes generated by ANTLR for AssignmentStatement grammar.","title":"Three addresses codes"},{"location":"#abstract-syntax-tree","text":"Abstract syntax trees (ASTs) are a useful abstraction when dealing with programming languages as an object for analysis or manipulation (e.g. compilation). Figure 2 demonstrates how a single pass compiler can generate abstract syntax three (AST) for assignment statements. Figure 2: Examples of abstract syntax tree (AST) generated by ANTLR for AssignmentStatement grammar. The above tree corresponds to the following expression: a1 := (2 + 12 * 3) / (6 - 19)","title":"Abstract Syntax Tree"},{"location":"#structure","text":"The following describes the structure of the repository:","title":"Structure"},{"location":"#grammars","text":"gram1 : ANTLR hello world grammar. Expr1 : Simple grammar for handling mathematical expressions without any attribute and action. Expr2 : Simple attributed grammar for handling mathematical expressions with code() attribute. Expr3 : Currently, it is the same Expr2 grammar. AssignmentStatement1.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . AssignmentStatement2.g4 : It is the same AssignmentStatement1.g4 grammar plus attributes for code and type of rules. AssignmentStatement3.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . It provides semantic rules to perform type checking and semantic routines to generate intermediate representation. AssignmentStatement4.g4 : The grammar to handle multiple assignment statements and mathematical expressions in programming languages like Pascal and C/C++ . It provides semantic rules to perform type checking and semantic routines to generate intermediate representation. It has been implemented to generate intermediate representation (three addresses codes) with minimum number of \"temp\" variables. CPP14_v2 : ANTLR grammar for C++14 forked from the official ANTRL website. Some bugs have been fixed and also the rule identifiers have been added to the grammar rules. EMail.g4 : Lexical grammar to validate email addresses. EMail2.g4 : Lexical grammar to validate email addresses, fixing bugs in EMail.g4","title":"grammars"},{"location":"#language_apps","text":"The language_apps package currently contains Lexer and Parser codes for each grammar in directory grammars , with a main driver script to demonstrate the type checking and intermediate code generation based on semantic rules and semantic routines .","title":"language_apps"},{"location":"#terminal_batch_scripts","text":"The termina_batch_script directory contains several batch script to run ANTLR in terminal (Windows) to generate target code in JAVA language. The code snippets in this directory belong to my first experiences with ANTLR.","title":"terminal_batch_scripts"},{"location":"#read-more","text":"IUST compiler course official webpage ANTLR slides: PART 1: Introduction ANTLR slides: PART 2: Getting started in Java ANTLR slides: PART 3: Getting started in C#","title":"Read more"},{"location":"announcements/","text":"Announcements September 10, 2022 : I am designing some new applications to be used as compiler teaching projects. Please let me know if you have any good ideas/proposals.","title":"Announcements"},{"location":"announcements/#announcements","text":"September 10, 2022 : I am designing some new applications to be used as compiler teaching projects. Please let me know if you have any good ideas/proposals.","title":"Announcements"},{"location":"antlr_tutorials/antlr_advanced/","text":"ANTLR advanced tutorials By: Morteza Zakeri Last update: April 30, 2022 Compiler background We first define some terms used in compiler literature when analyzing the program based on the ANTLR vocabulary. Compiler pass. Each time that the walk method of ParseTreeWalker class is called, it visits all nodes of the parse tree. In compiler literature, we called this process a pass. The ANTLR pass can be annotated with listener classes to perform specific analysis or transformation. An analysis pass refers to the pass in which some information is obtained from the source code, but the source code is not changed, or no new code is generated. A transformation pass refers to the pass in which the program source code is modified or new codes are generated. As we discussed in the next sections, refactoring automation consists of both the analysis and transformation passes. Single v.s. multiple pass. Often to perform specific analyses or transformations, the program should be visited multiple times. Indeed, such tasks required multiple passes. For instance, if a class attribute is defined after it is used in a method, which is possible in Java programming language, to find the definition of the field and then modify its usage, we should visit the program twice. The reason is that the program tokens are read from left to right, and then when traversing the parse tree, the node only is visited once in the order they appear in the program text. For a given task, if we visit a node and require the information obtained from the next nodes, we cannot complete the task in one pass. In such a case, a pass is required to obtain the necessary information from the next nodes, and another pass is required to use this information for the current node. Most refactoring operations we described in this chapter require multiple passes to complete the refactoring process. For each pass, we develop a separate listener class and pass it to ParseTreeWalker class. Summary An analysis that is performed by traversing the program once is called a single pass-analysis. An analysis that is performed by traversing the program source code twice or more is called multiple analysis passes. Todo: To be completed ...","title":"Advanced"},{"location":"antlr_tutorials/antlr_advanced/#antlr-advanced-tutorials","text":"By: Morteza Zakeri Last update: April 30, 2022","title":"ANTLR advanced tutorials"},{"location":"antlr_tutorials/antlr_advanced/#compiler-background","text":"We first define some terms used in compiler literature when analyzing the program based on the ANTLR vocabulary. Compiler pass. Each time that the walk method of ParseTreeWalker class is called, it visits all nodes of the parse tree. In compiler literature, we called this process a pass. The ANTLR pass can be annotated with listener classes to perform specific analysis or transformation. An analysis pass refers to the pass in which some information is obtained from the source code, but the source code is not changed, or no new code is generated. A transformation pass refers to the pass in which the program source code is modified or new codes are generated. As we discussed in the next sections, refactoring automation consists of both the analysis and transformation passes. Single v.s. multiple pass. Often to perform specific analyses or transformations, the program should be visited multiple times. Indeed, such tasks required multiple passes. For instance, if a class attribute is defined after it is used in a method, which is possible in Java programming language, to find the definition of the field and then modify its usage, we should visit the program twice. The reason is that the program tokens are read from left to right, and then when traversing the parse tree, the node only is visited once in the order they appear in the program text. For a given task, if we visit a node and require the information obtained from the next nodes, we cannot complete the task in one pass. In such a case, a pass is required to obtain the necessary information from the next nodes, and another pass is required to use this information for the current node. Most refactoring operations we described in this chapter require multiple passes to complete the refactoring process. For each pass, we develop a separate listener class and pass it to ParseTreeWalker class.","title":"Compiler background"},{"location":"antlr_tutorials/antlr_advanced/#summary","text":"An analysis that is performed by traversing the program once is called a single pass-analysis. An analysis that is performed by traversing the program source code twice or more is called multiple analysis passes. Todo: To be completed ...","title":"Summary"},{"location":"antlr_tutorials/antlr_basics/","text":"ANTLR basic tutorials By: Morteza Zakeri Last update: April 30, 2022 Introduction The ANTLR tool generates a top-down parser from the grammar rules defined with the ANTLR meta-grammar (Parr and Fisher 2011). The initial version of ANTLR generated the target parser source code in Java. In the current version (version 4), the parser source code can be generated in a wide range of programming languages listed on the ANTLR official website (Parr 2022a). For simplicity, we generate the parser in Python 3, which provides us to run the tool on every platform having Python 3 installed on it. Another reason to use Python is that we can integrate the developed program easily with other libraries available in Python, such as machine learning and optimization libraries. Finally, I found that there is no comprehensive tutorial on using ANTLR with the Python backend. To use ANTLR in other programming languages, specifically Java and C#, refer to the ANTLR slides I created before this tutorial. The ANTLR tool is a small \u201c.jar\u201d file that must be run from the command line to generate the parser codes. The ANTLR tool jar file can be downloaded from here . Generating parser As mentioned, to generate a parser for a programming language, the grammar specification described with ANTLR meta-grammar is required. ANTLR grammar files are named with the \u201c.g4\u201d suffix. We obtain the grammar of Java 8 to build our parser for the Java programming language. The grammar can be downloaded from ANTLR 4 grammar repository on GitHub: https://github.com/antlr/grammars-v4 . Once the ANTLR tool and required grammar files are prepared, we can generate the parser for that with the following command: > java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -o . JavaLexer.g4 > java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -visitor -listener -o . JavaLabeledParser.g4 The first command generates the lexer from the JavaLexer.g4 description file and the second command generates the parser from the JavaLabeledParser.g4 description file. It is worth noting that the lexer and parser can be written in one file. In such a case, a single command generates all required codes in one step. The grammar files used in the above command are also available in grammars directory of the CodART repository. You may see that I have made some modifications to the Parser rules. In the above commands, the antlr-4.9.3-complete.jar is the ANTLR tool that requires Java to be executed. -Dlanguage denotes the destination language that the ANTLR parser (and lexer) source code is generated in which. In our case, we set it to Python3. After executing the ANTLR parser generation commands, eight files, including parser source code and other required information, are generated. Figure 1 shows the generated files. The \u201c.py\u201d contains lexer and parser source code that can parse any Java input file. The -visitor -listener switches in the second command result in generating two separate source files, JavaLabledParserListener.py and JavaLabledParserVistor.py , which provide interfaces to implement the required codes for a specific language application. Our application is source code refactoring which uses the listener mechanism to implement necessary actions transforming the program to the refactored version. The parse tree structure in and listener mechanism are discussed in the next sections. Figure 1. Generated files by ANTLR. It should be noted that to use the generated classes in Figure 1, for developing a specific program, we need to install the appropriate ANTLR runtime library. For creating ANTLR-based programs in Python, the command pip install antlr-python3-runtime can be used. It installed all runtime dependencies required to program using the ANTLR library. ANTLR parse tree The generated parser by ANTLR is responsible for parsing every Java source code file and generating the parse tree or designating the syntax errors in the input file. The parse tree for real-world programs with thousands of lines of code has a non-trivial structure. ANTLR developers have provided some IDE plugins that can visualize the parse tree to better understand the structure of the parse tree generated by ANTLR. We use Pycharm IDE developed by Jetbrains to work with Python code. Figure 2 shows how we can install the ANTLR plugin in PyCharm. The plugin source code is available on the GitHub repo . When the plugin is installed, the ANTLR preview widow is applied at the bottom of the PyCharm IDE. In addition, the IDE can be recognized as \u201c.g4\u201d files and some other options added to the IDE. The main option is the ability to test a grammar rule and visualize the corresponding parse tree to that rule. Figure 2. Installing the ANTLR plugin in the PyCharm IDE. In order to use the ANTLR preview tab, the ANTLR grammar should be opened in the PyCharm IDE. We then select a rule (typically the start rule) of our grammar, right-click on the rule, and select the \u201cTest Rule rule_name \u201d option from the opened menu, shown in Figure 3. We now write our sample input program in the left panel of the ANTLR preview, and the parse tree is shown in the right panel. Figure 3. Test the grammar rule in the ANTLR PyCharm plugin. Figure 4 shows a simple Java class and the corresponding parse tree generated by the ANTLR. The leaves of the parse tree are program tokens, while the intermediate nodes are grammar rules that the evaluating program is derived from them. Also, the root of the tree is the grammar rule, which we selected to start parsing. It means that we can select and test every rule independently. However, a complete Java program can only parse from the start rule of the given grammar, i.e., the compilaionUnit rule. Figure 4. Test the grammar rule in the ANTLR PyCharm plugin. It should be mentioned that the ANTLR Preview window is based on a grammar interpreter, not on the actual generated parser described in the previous section. It means that grammar attributes such as actions and predicates will not be evaluated during live preview because the interpreter is language agnostic. For the same reasons, if the generated parser and/or lexer classes extend a custom implementation of the base parser/lexer classes, the custom code will not be run during the live preview. In addition to the parse tree visualization, the ANTLR plugin provides facilities such as profiling, code generation, etc., described in here (Parr 2022b). For example, the profile tab shows the execution time of each rule in the parser for a given input string. I want to emphasize that visualizing the parse tree with the ANTLR plugin is really helpful when developing code and fixing bugs described in the next section of this tutorial. Traversing the parse tree programmatically ANTLR is not a simple parser generator. It provides a depth-first parse tree visiting and a callback mechanism called listener to implement the required program analysis or transformation passes. The depth-first search is performed by instantiating an object from the ANTLR ParseTreeWalker class and calling the walk method, which takes an instance of ParseTree as an input argument and traverses it. Obviously, if we visit the parse tree with the depth-first search algorithm, all program tokens are visited in the same order that they appeared in the source code file. However, the depth-first search contains additional information about when a node in the tree is visited and when the visiting all nodes in its subtree is finished. Therefore, we can add the required actions when visiting a node to perform a special task. For example, according to Figure 4, for counting the number of classes in a code snippet, we can define a counter variable, initialize it to zero, and increase it whenever the walker visits the \u201cclassDeclartion\u201d node. ANTLR provides two callback functions for each node in the parse tree. One is called by the walker when it is entered into a node, i.e., visit the node, but the children are not visited yet. Another is called when all nodes in the subtree of the visited node have been visited, and the walker is exiting the node. These callback functions are available in the listener class generated by the ANTLR for every rule in a given grammar. In our example for counting the number of classes, we implement all required logic in the body of enterClassDeclartion method of the JavaLabledParserListener class. We called these logic codes grammar\u2019s actions since, indeed, they are bunded to a grammar rule. It is worth noting that we can add these actions codes in the grammar file ( .g4 file) to form an attributed grammar. Embedding actions in grammar increase the efficiency of the analyzing process. However, when we need many complex actions, the listener mechanism provides a better way to implement them. Indeed, ANTLR 4 emphasizes separating the language applications from the language grammar by using the listener mechanism. Listing 1 shows the implementation program for counting the number of classes using the ANTLR listener mechanism. The DesignMetrics class inherits from JavaLabeledParserListener class which is the default listener class generated by ANTLR. We only implement the enterClassDeclartion method, which increases the value of the __dsc counter each time the walker visits a Java class. # module: JavaLabledParserListener.py __version__ = \"0.1.0\" __author__ = \"Morteza\" from antlr4 import * if __name__ is not None and \".\" in __name__: from .JavaLabeledParser import JavaLabeledParser else: from JavaLabeledParser import JavaLabeledParser class JavaLabeledParserListener(ParseTreeListener): # \u2026 def enterClassDeclaration(self, ctx:JavaLabeledParser.ClassDeclarationContext): pass # \u2026 class DesignMetrics(JavaLabeledParserListener): def __init__(self): self.__dsc:int = 0 # Keep design size in classes @property def get_design_size(self): return self.__dsc def enterClassDeclaration(self, ctx:JavaLabeledParser.ClassDeclarationContext): self.__dsc += 1 Listing 1: Programs that count the number of classes in a Java source code. Wiring the modules To complete our simple analysis task, first, the parse tree for a given input should be constructed. Then, the DesignMetrics class should be instantiated and passed to an object of ParseTreeWalker class. We created a driver module in Python beside the generated code by ANTLR to connect different parts of our program and complete our task. Listing 2 shows the implementation of the main driver for a program that counts the number of classes in Java source codes. # Module: main_driver.py __version__ = \"0.1.0\" __author__ = \"Morteza\" from antlr4 import * from JavaLabledLexer import JavaLabledLexer from JavaLeabledParser import JavaLabledParser from JavaLabledParserListener import DesignMetrics def main(args): # Step 1: Load input source into the stream object stream = FileStream(args.file, encoding='utf8') # Step 2: Create an instance of AssignmentStLexer lexer = JavaLabledLexer(stream) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream(lexer) # Step 4: Create an instance of the AssignmentStParser parser = JavaLabledParser(token_stream) # Step 5: Create parse tree parse_tree = parser.compilationUnit() # Step 6: Create an instance of DesignMetrics listener class my_listener = DesignMetrics() # Step 7: Create a walker to traverse the parse tree and callback our listener walker = ParseTreeWalker() walker.walk(t=parse_tree, listener=my_listener) # Step 8: Getting the results print(f'DSC={my_listener.get_design_size}') Listing 2: Main driver module for the program in Listing 1 Conclusion and remarks In this tutorial, we described the basic concepts regarding using the ANTLR tool to generate and walk phase three and implement custom program analysis applications with the help of the ANTLR listener mechanism. The most important point is that we used the real-world programming languages grammars to show the parsing and analyzing process. The discussed topics form the underlying concepts of our approach for automated refactoring used in CodART. Check out the ANTLR advanced tutorial to find out how we can use ANTLR for reliable and efficient program transformation. References Parr T ANTLR (ANother Tool for Language Recognition). https://www.antlr.org. Accessed 10 Jan 2022a Parr T IntelliJ Idea Plugin for ANTLR v4. https://github.com/antlr/intellij-plugin-v4. Accessed 10 Jan 2022b Parr T, Fisher K (2011) LL(*): the foundation of the ANTLR parser generator. Proc 32nd ACM SIGPLAN Conf Program Lang Des Implement 425\u2013436. https://doi.org/http://doi.acm.org/10.1145/1993498.1993548","title":"Basic"},{"location":"antlr_tutorials/antlr_basics/#antlr-basic-tutorials","text":"By: Morteza Zakeri Last update: April 30, 2022","title":"ANTLR basic tutorials"},{"location":"antlr_tutorials/antlr_basics/#introduction","text":"The ANTLR tool generates a top-down parser from the grammar rules defined with the ANTLR meta-grammar (Parr and Fisher 2011). The initial version of ANTLR generated the target parser source code in Java. In the current version (version 4), the parser source code can be generated in a wide range of programming languages listed on the ANTLR official website (Parr 2022a). For simplicity, we generate the parser in Python 3, which provides us to run the tool on every platform having Python 3 installed on it. Another reason to use Python is that we can integrate the developed program easily with other libraries available in Python, such as machine learning and optimization libraries. Finally, I found that there is no comprehensive tutorial on using ANTLR with the Python backend. To use ANTLR in other programming languages, specifically Java and C#, refer to the ANTLR slides I created before this tutorial. The ANTLR tool is a small \u201c.jar\u201d file that must be run from the command line to generate the parser codes. The ANTLR tool jar file can be downloaded from here .","title":"Introduction"},{"location":"antlr_tutorials/antlr_basics/#generating-parser","text":"As mentioned, to generate a parser for a programming language, the grammar specification described with ANTLR meta-grammar is required. ANTLR grammar files are named with the \u201c.g4\u201d suffix. We obtain the grammar of Java 8 to build our parser for the Java programming language. The grammar can be downloaded from ANTLR 4 grammar repository on GitHub: https://github.com/antlr/grammars-v4 . Once the ANTLR tool and required grammar files are prepared, we can generate the parser for that with the following command: > java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -o . JavaLexer.g4 > java -Xmx500M -cp antlr-4.9.3-complete.jar org.antlr.v4.Tool -Dlanguage=Python3 -visitor -listener -o . JavaLabeledParser.g4 The first command generates the lexer from the JavaLexer.g4 description file and the second command generates the parser from the JavaLabeledParser.g4 description file. It is worth noting that the lexer and parser can be written in one file. In such a case, a single command generates all required codes in one step. The grammar files used in the above command are also available in grammars directory of the CodART repository. You may see that I have made some modifications to the Parser rules. In the above commands, the antlr-4.9.3-complete.jar is the ANTLR tool that requires Java to be executed. -Dlanguage denotes the destination language that the ANTLR parser (and lexer) source code is generated in which. In our case, we set it to Python3. After executing the ANTLR parser generation commands, eight files, including parser source code and other required information, are generated. Figure 1 shows the generated files. The \u201c.py\u201d contains lexer and parser source code that can parse any Java input file. The -visitor -listener switches in the second command result in generating two separate source files, JavaLabledParserListener.py and JavaLabledParserVistor.py , which provide interfaces to implement the required codes for a specific language application. Our application is source code refactoring which uses the listener mechanism to implement necessary actions transforming the program to the refactored version. The parse tree structure in and listener mechanism are discussed in the next sections. Figure 1. Generated files by ANTLR. It should be noted that to use the generated classes in Figure 1, for developing a specific program, we need to install the appropriate ANTLR runtime library. For creating ANTLR-based programs in Python, the command pip install antlr-python3-runtime can be used. It installed all runtime dependencies required to program using the ANTLR library.","title":"Generating parser"},{"location":"antlr_tutorials/antlr_basics/#antlr-parse-tree","text":"The generated parser by ANTLR is responsible for parsing every Java source code file and generating the parse tree or designating the syntax errors in the input file. The parse tree for real-world programs with thousands of lines of code has a non-trivial structure. ANTLR developers have provided some IDE plugins that can visualize the parse tree to better understand the structure of the parse tree generated by ANTLR. We use Pycharm IDE developed by Jetbrains to work with Python code. Figure 2 shows how we can install the ANTLR plugin in PyCharm. The plugin source code is available on the GitHub repo . When the plugin is installed, the ANTLR preview widow is applied at the bottom of the PyCharm IDE. In addition, the IDE can be recognized as \u201c.g4\u201d files and some other options added to the IDE. The main option is the ability to test a grammar rule and visualize the corresponding parse tree to that rule. Figure 2. Installing the ANTLR plugin in the PyCharm IDE. In order to use the ANTLR preview tab, the ANTLR grammar should be opened in the PyCharm IDE. We then select a rule (typically the start rule) of our grammar, right-click on the rule, and select the \u201cTest Rule rule_name \u201d option from the opened menu, shown in Figure 3. We now write our sample input program in the left panel of the ANTLR preview, and the parse tree is shown in the right panel. Figure 3. Test the grammar rule in the ANTLR PyCharm plugin. Figure 4 shows a simple Java class and the corresponding parse tree generated by the ANTLR. The leaves of the parse tree are program tokens, while the intermediate nodes are grammar rules that the evaluating program is derived from them. Also, the root of the tree is the grammar rule, which we selected to start parsing. It means that we can select and test every rule independently. However, a complete Java program can only parse from the start rule of the given grammar, i.e., the compilaionUnit rule. Figure 4. Test the grammar rule in the ANTLR PyCharm plugin. It should be mentioned that the ANTLR Preview window is based on a grammar interpreter, not on the actual generated parser described in the previous section. It means that grammar attributes such as actions and predicates will not be evaluated during live preview because the interpreter is language agnostic. For the same reasons, if the generated parser and/or lexer classes extend a custom implementation of the base parser/lexer classes, the custom code will not be run during the live preview. In addition to the parse tree visualization, the ANTLR plugin provides facilities such as profiling, code generation, etc., described in here (Parr 2022b). For example, the profile tab shows the execution time of each rule in the parser for a given input string. I want to emphasize that visualizing the parse tree with the ANTLR plugin is really helpful when developing code and fixing bugs described in the next section of this tutorial.","title":"ANTLR parse tree"},{"location":"antlr_tutorials/antlr_basics/#traversing-the-parse-tree-programmatically","text":"ANTLR is not a simple parser generator. It provides a depth-first parse tree visiting and a callback mechanism called listener to implement the required program analysis or transformation passes. The depth-first search is performed by instantiating an object from the ANTLR ParseTreeWalker class and calling the walk method, which takes an instance of ParseTree as an input argument and traverses it. Obviously, if we visit the parse tree with the depth-first search algorithm, all program tokens are visited in the same order that they appeared in the source code file. However, the depth-first search contains additional information about when a node in the tree is visited and when the visiting all nodes in its subtree is finished. Therefore, we can add the required actions when visiting a node to perform a special task. For example, according to Figure 4, for counting the number of classes in a code snippet, we can define a counter variable, initialize it to zero, and increase it whenever the walker visits the \u201cclassDeclartion\u201d node. ANTLR provides two callback functions for each node in the parse tree. One is called by the walker when it is entered into a node, i.e., visit the node, but the children are not visited yet. Another is called when all nodes in the subtree of the visited node have been visited, and the walker is exiting the node. These callback functions are available in the listener class generated by the ANTLR for every rule in a given grammar. In our example for counting the number of classes, we implement all required logic in the body of enterClassDeclartion method of the JavaLabledParserListener class. We called these logic codes grammar\u2019s actions since, indeed, they are bunded to a grammar rule. It is worth noting that we can add these actions codes in the grammar file ( .g4 file) to form an attributed grammar. Embedding actions in grammar increase the efficiency of the analyzing process. However, when we need many complex actions, the listener mechanism provides a better way to implement them. Indeed, ANTLR 4 emphasizes separating the language applications from the language grammar by using the listener mechanism. Listing 1 shows the implementation program for counting the number of classes using the ANTLR listener mechanism. The DesignMetrics class inherits from JavaLabeledParserListener class which is the default listener class generated by ANTLR. We only implement the enterClassDeclartion method, which increases the value of the __dsc counter each time the walker visits a Java class. # module: JavaLabledParserListener.py __version__ = \"0.1.0\" __author__ = \"Morteza\" from antlr4 import * if __name__ is not None and \".\" in __name__: from .JavaLabeledParser import JavaLabeledParser else: from JavaLabeledParser import JavaLabeledParser class JavaLabeledParserListener(ParseTreeListener): # \u2026 def enterClassDeclaration(self, ctx:JavaLabeledParser.ClassDeclarationContext): pass # \u2026 class DesignMetrics(JavaLabeledParserListener): def __init__(self): self.__dsc:int = 0 # Keep design size in classes @property def get_design_size(self): return self.__dsc def enterClassDeclaration(self, ctx:JavaLabeledParser.ClassDeclarationContext): self.__dsc += 1 Listing 1: Programs that count the number of classes in a Java source code.","title":"Traversing the parse tree programmatically"},{"location":"antlr_tutorials/antlr_basics/#wiring-the-modules","text":"To complete our simple analysis task, first, the parse tree for a given input should be constructed. Then, the DesignMetrics class should be instantiated and passed to an object of ParseTreeWalker class. We created a driver module in Python beside the generated code by ANTLR to connect different parts of our program and complete our task. Listing 2 shows the implementation of the main driver for a program that counts the number of classes in Java source codes. # Module: main_driver.py __version__ = \"0.1.0\" __author__ = \"Morteza\" from antlr4 import * from JavaLabledLexer import JavaLabledLexer from JavaLeabledParser import JavaLabledParser from JavaLabledParserListener import DesignMetrics def main(args): # Step 1: Load input source into the stream object stream = FileStream(args.file, encoding='utf8') # Step 2: Create an instance of AssignmentStLexer lexer = JavaLabledLexer(stream) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream(lexer) # Step 4: Create an instance of the AssignmentStParser parser = JavaLabledParser(token_stream) # Step 5: Create parse tree parse_tree = parser.compilationUnit() # Step 6: Create an instance of DesignMetrics listener class my_listener = DesignMetrics() # Step 7: Create a walker to traverse the parse tree and callback our listener walker = ParseTreeWalker() walker.walk(t=parse_tree, listener=my_listener) # Step 8: Getting the results print(f'DSC={my_listener.get_design_size}') Listing 2: Main driver module for the program in Listing 1","title":"Wiring the modules"},{"location":"antlr_tutorials/antlr_basics/#conclusion-and-remarks","text":"In this tutorial, we described the basic concepts regarding using the ANTLR tool to generate and walk phase three and implement custom program analysis applications with the help of the ANTLR listener mechanism. The most important point is that we used the real-world programming languages grammars to show the parsing and analyzing process. The discussed topics form the underlying concepts of our approach for automated refactoring used in CodART. Check out the ANTLR advanced tutorial to find out how we can use ANTLR for reliable and efficient program transformation.","title":"Conclusion and remarks"},{"location":"antlr_tutorials/antlr_basics/#references","text":"Parr T ANTLR (ANother Tool for Language Recognition). https://www.antlr.org. Accessed 10 Jan 2022a Parr T IntelliJ Idea Plugin for ANTLR v4. https://github.com/antlr/intellij-plugin-v4. Accessed 10 Jan 2022b Parr T, Fisher K (2011) LL(*): the foundation of the ANTLR parser generator. Proc 32nd ACM SIGPLAN Conf Program Lang Des Implement 425\u2013436. https://doi.org/http://doi.acm.org/10.1145/1993498.1993548","title":"References"},{"location":"antlr_tutorials/antlr_slides/","text":"ANTLR basic tutorials (slides) By: Morteza Zakeri Last update: May 1, 2022 Introduction to ANTLR: Part I Antlr part1 introduction from Morteza Zakeri Introduction to ANTLR: Part II Antlr part2 getting_started_in_java from Morteza Zakeri Introduction to ANTLR: Part III Antlr part3 getting_started_in_c_sharp from Morteza Zakeri","title":"Slides"},{"location":"antlr_tutorials/antlr_slides/#antlr-basic-tutorials-slides","text":"By: Morteza Zakeri Last update: May 1, 2022","title":"ANTLR basic tutorials (slides)"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-i","text":"Antlr part1 introduction from Morteza Zakeri","title":"Introduction to ANTLR: Part I"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-ii","text":"Antlr part2 getting_started_in_java from Morteza Zakeri","title":"Introduction to ANTLR: Part II"},{"location":"antlr_tutorials/antlr_slides/#introduction-to-antlr-part-iii","text":"Antlr part3 getting_started_in_c_sharp from Morteza Zakeri","title":"Introduction to ANTLR: Part III"},{"location":"assignments/programming_assignment/","text":"Programming assignments Current semester Archive Semester 971 Semester 962 Semester 961","title":"Programming assignments"},{"location":"assignments/programming_assignment/#programming-assignments","text":"","title":"Programming assignments"},{"location":"assignments/programming_assignment/#current-semester","text":"","title":"Current semester"},{"location":"assignments/programming_assignment/#archive","text":"","title":"Archive"},{"location":"assignments/programming_assignment/#semester-971","text":"","title":"Semester 971"},{"location":"assignments/programming_assignment/#semester-962","text":"","title":"Semester 962"},{"location":"assignments/programming_assignment/#semester-961","text":"","title":"Semester 961"},{"location":"assignments/writing_assignments/","text":"Writing assignments Current semester Archive Semester 971 HW1 HW2 HW3 (Dr. Parsa Questions) Semester 962 HW1 HW2 HW3 HW4","title":"Writing assignments"},{"location":"assignments/writing_assignments/#writing-assignments","text":"","title":"Writing assignments"},{"location":"assignments/writing_assignments/#current-semester","text":"","title":"Current semester"},{"location":"assignments/writing_assignments/#archive","text":"","title":"Archive"},{"location":"assignments/writing_assignments/#semester-971","text":"HW1 HW2 HW3 (Dr. Parsa Questions)","title":"Semester 971"},{"location":"assignments/writing_assignments/#semester-962","text":"HW1 HW2 HW3 HW4","title":"Semester 962"},{"location":"language_applications/assignment_statement1main/","text":"Assignment statement grammar (version 1) Main script for grammar AssignmentStatement1 (version 1) author Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) date 20201029 Required Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x Changelog v2.0.0 A lexer and parser for simple grammar without any attribute or listener Refs Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ main ( args ) Create lexer and parser Parameters: Name Type Description Default args str required return None required Source code in language_apps\\assignment_statement_v1\\assignment_statement1main.py def main ( args ): \"\"\" Create lexer and parser Args: args (str): return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement1Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # # quit() # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement1Parser ( token_stream ) # parser._interp.predictionMode = PredictionMode.SLL #x = DescriptiveErrorListener() #parser.addErrorListener() # Step 5: Create parse tree parse_tree = parser . start () # print(parse_tree) # quit() # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) print ( f 'Number of \"+\" operators: { my_listener . get_count () } ' ) # print(parse_tree.getText()) quit () # return lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"Assignment statement 1"},{"location":"language_applications/assignment_statement1main/#assignment-statement-grammar-version-1","text":"Main script for grammar AssignmentStatement1 (version 1)","title":"Assignment statement grammar (version 1)"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--author","text":"Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)","title":"author"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--date","text":"20201029","title":"date"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--required","text":"Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x","title":"Required"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--changelog","text":"","title":"Changelog"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--v200","text":"A lexer and parser for simple grammar without any attribute or listener","title":"v2.0.0"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main--refs","text":"Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Refs"},{"location":"language_applications/assignment_statement1main/#language_apps.assignment_statement_v1.assignment_statement1main.main","text":"Create lexer and parser Parameters: Name Type Description Default args str required return None required Source code in language_apps\\assignment_statement_v1\\assignment_statement1main.py def main ( args ): \"\"\" Create lexer and parser Args: args (str): return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement1Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # # quit() # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement1Parser ( token_stream ) # parser._interp.predictionMode = PredictionMode.SLL #x = DescriptiveErrorListener() #parser.addErrorListener() # Step 5: Create parse tree parse_tree = parser . start () # print(parse_tree) # quit() # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) print ( f 'Number of \"+\" operators: { my_listener . get_count () } ' ) # print(parse_tree.getText()) quit () # return lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"main()"},{"location":"language_applications/assignment_statement2main/","text":"Assignment statement grammar (version 2) Tree address code generation pass ANTLR 4.x listener and visitor implementation for intermediate code generation (Three addresses code) @author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201017 Compiler generator: ANTLR4.x Target language(s): Python3.x, -Changelog: -- v2.1.0 --- Add support for AST intermediate representation using module ast_pass --- Change compiler_pass module to three_address_code_pass -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules. Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ ThreeAddressCodeGenerator2Listener ( AssignmentStatement2Listener ) Type checking and generating three address language_apps (optimizing number of temporary variables) Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGenerator2Listener ( AssignmentStatement2Listener ): \"\"\" Type checking and generating three address language_apps (optimizing number of temporary variables) \"\"\" def __init__ ( self ): print ( 'Listener2 call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def remove_temp ( self ): self . temp_counter -= 1 def get_temp ( self ): return 'T' + str ( self . temp_counter ) @classmethod def is_temp ( cls , variable ): if variable [ 0 ] == 'T' : return True return False # ------------------ # Rule number def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): ctx . type_attr = 'float' ctx . value_attr = float ( ctx . getText ()) def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): ctx . type_attr = 'int' ctx . value_attr = int ( ctx . getText ()) # ------------------ # Rule factor def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): ctx . type_attr = 'string' ctx . value_attr = ctx . getText () def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . type_attr = ctx . number () . type_attr ctx . value_attr = ctx . number () . value_attr # ------------------ # Rule term def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot divide {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr = ctx . factor () . type_attr ctx . value_attr = ctx . factor () . value_attr # ------------------ # Rule expr def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot subtract {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr = ctx . term () . type_attr ctx . value_attr = ctx . term () . value_attr # ------------------ # Rule expr def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) ThreeAddressCodeGenerator2Visitor ( AssignmentStatement2Visitor ) Type checking and generating three address language_apps (optimizing number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGenerator2Visitor ( AssignmentStatement2Visitor ): \"\"\" Type checking and generating three address language_apps (optimizing number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism \"\"\" def __init__ ( self ): print ( 'Visitor2 call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def remove_temp ( self ): self . temp_counter -= 1 def get_temp ( self ): return 'T' + str ( self . temp_counter ) @classmethod def is_temp ( cls , variable ): if variable [ 0 ] == 'T' : return True return False def visitStart ( self , ctx : AssignmentStatement2Parser . StartContext ): self . visit ( tree = ctx . prog ()) def visitProg ( self , ctx : AssignmentStatement2Parser . ProgContext ): if ctx . getChildCount () == 2 : self . visit ( tree = ctx . prog ()) ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . assign ()) return ctx . type_attr , ctx . value_attr def visitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . expr ()) print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) return ctx . type_attr , ctx . value_attr # ------------------ # Rule expr def visitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr , ctx . value_attr = self . visit ( ctx . term ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule term def visitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . factor ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule factor def visitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): return self . visit ( tree = ctx . expr ()) def visitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): return 'string' , ctx . ID () . getText () def visitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): return self . visit ( tree = ctx . number ()) # ------------------ # Rule number def visitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): return 'float' , float ( ctx . FLOAT () . getText ()) def visitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): return 'int' , int ( ctx . INT () . getText ()) ThreeAddressCodeGeneratorListener ( AssignmentStatement2Listener ) Type checking and generating three address language_apps (not optimized) Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGeneratorListener ( AssignmentStatement2Listener ): \"\"\" Type checking and generating three address language_apps (not optimized) \"\"\" def __init__ ( self ): print ( 'Listener call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) # ------------------ # Rule number def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): ctx . type_attr = 'float' ctx . value_attr = float ( ctx . getText ()) def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): ctx . type_attr = 'int' ctx . value_attr = int ( ctx . getText ()) # ------------------ # Rule factor def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): ctx . type_attr = 'string' ctx . value_attr = str ( ctx . getText ()) def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . type_attr = ctx . number () . type_attr ctx . value_attr = ctx . number () . value_attr # ------------------ # Rule term def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot divide {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr = ctx . factor () . type_attr ctx . value_attr = ctx . factor () . value_attr # ------------------ # Rule expr def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot subtract {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr = ctx . term () . type_attr ctx . value_attr = ctx . term () . value_attr # ------------------ # Rule expr def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) ThreeAddressCodeGeneratorVisitor ( AssignmentStatement2Visitor ) Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGeneratorVisitor ( AssignmentStatement2Visitor ): \"\"\" Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism \"\"\" def __init__ ( self ): print ( 'Visitor call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def visitStart ( self , ctx : AssignmentStatement2Parser . StartContext ): self . visit ( tree = ctx . prog ()) def visitProg ( self , ctx : AssignmentStatement2Parser . ProgContext ): if ctx . getChildCount () == 2 : self . visit ( tree = ctx . prog ()) ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . assign ()) return ctx . type_attr , ctx . value_attr def visitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . expr ()) print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) return ctx . type_attr , ctx . value_attr # ------------------ # Rule expr def visitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr , ctx . value_attr = self . visit ( ctx . term ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule term def visitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . factor ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule factor def visitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): return self . visit ( tree = ctx . expr ()) def visitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): return 'string' , ctx . ID () . getText () def visitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): return self . visit ( tree = ctx . number ()) # ------------------ # Rule number def visitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): return 'float' , float ( ctx . FLOAT () . getText ()) def visitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): return 'int' , int ( ctx . INT () . getText ()) Abstract syntax tree (AST) generation pass ANTLR 4.x listener and visitor implementation for intermediate code generation (abstract syntax trees) @author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201117 Compiler generator: ANTRL4.x Target language(s): Python3.x, -Changelog: -- v2.1.0 --- Add support for AST visualization with dummy nodes --- Add support for AST intermediate representation using module ast_pass --- Change compiler_pass module to three_address_code_pass -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules. Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ ASTListener ( AssignmentStatement2Listener ) Source code in language_apps\\assignment_statement_v2\\abstract_syntax_tree_pass.py class ASTListener ( AssignmentStatement2Listener ): \"\"\" \"\"\" def __init__ ( self ): self . ast = AST () # Data structure for holding the abstract syntax tree self . q = queue . Queue () # Use to print and visualize AST self . g = nx . DiGraph () # Use to visualize AST # self.q.empty() # print('Q=', ) def print_tree ( self , node = None , level = 1 ): if node is None : # print() return # if not self.q.empty(): # print('Parent:', self.q.get().value) # print('\\t'*level, end='') print () while node is not None : current_node = node print ( current_node . value , end = '' ) # alt+196 = \u2500\u2500\u2500, alt+178=\u2593 if node . child is not None : # self.q.put(node) self . g . add_edge ( current_node , node . child , edge_type = 'C' , color = 'red' ) self . q . put ( node . child ) else : tn = TreeNode ( value = '\u2593' , child = None , brother = None ) self . g . add_edge ( current_node , tn , edge_type = 'C' , color = 'red' ) node = node . brother if node is not None : print ( ' \\t \u2500\u2500\u2500 \\t ' , end = '' ) self . g . add_edge ( current_node , node , edge_type = 'B' , color = 'blue' ) else : tn = TreeNode ( value = '\u2593' , child = None , brother = None ) self . g . add_edge ( current_node , tn , edge_type = 'B' , color = 'blue' ) if not self . q . empty (): self . print_tree ( node = self . q . get (), level = level + 1 ) def print_tree2 ( self , node = None ): pass def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): idPntr = self . ast . make_node ( value = ctx . ID () . getText (), child = None , brother = ctx . expr () . value_attr ) assPntr = self . ast . make_node ( value = \":=\" , child = idPntr , brother = None ) ctx . value_attr = assPntr self . ast . root = assPntr self . print_tree ( node = self . ast . root , level = 1 ) def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): self . ast . add_brother ( ctx . expr () . value_attr , ctx . term () . value_attr ) exprPntr = self . ast . make_node ( value = \"+\" , child = ctx . expr () . value_attr , brother = None ) ctx . value_attr = exprPntr def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): self . ast . add_brother ( ctx . expr () . value_attr , ctx . term () . value_attr ) exprPntr = self . ast . make_node ( value = \"-\" , child = ctx . expr () . value_attr , brother = None ) ctx . value_attr = exprPntr def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . value_attr = ctx . term () . value_attr # ---------------------- def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): self . ast . add_brother ( ctx . term () . value_attr , ctx . factor () . value_attr ) termPntr = self . ast . make_node ( value = \"*\" , child = ctx . term () . value_attr , brother = None ) ctx . value_attr = termPntr def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): self . ast . add_brother ( ctx . term () . value_attr , ctx . factor () . value_attr ) termPntr = self . ast . make_node ( value = \"/\" , child = ctx . term () . value_attr , brother = None ) ctx . value_attr = termPntr def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . value_attr = ctx . factor () . value_attr # --------------------- def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): idPntr = self . ast . make_node ( value = ctx . ID () . getText (), child = None , brother = None ) ctx . value_attr = idPntr def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . value_attr = ctx . number () . value_attr # ---------------------- def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): numberPntr = self . ast . make_node ( value = ctx . FLOAT () . getText (), child = None , brother = None ) ctx . value_attr = numberPntr def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): numberPntr = self . ast . make_node ( value = ctx . INT () . getText (), child = None , brother = None ) ctx . value_attr = numberPntr Main driver Main script for grammar AssignmentStatement2 (version 2) Contains attributes for holding rule type and rule intermediate representations (AST and Three-addresses codes) author Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) date 20201029 Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x Install pygraphviz To draw AST as a binary tree you need to install the pygraphviz 1- download and install graphviz (for Windows/ Linux) 2- add graphviz to system path 3- install pygraphviz using the following command python -m pip install --global-option=build_ext --global-option=\"-IC:\\Program Files\\Graphviz\\include\" --global-option=\"-LC:\\Program Files\\Graphviz\\lib\" pygraphviz Changelog v2.1.1 Add visualization with Graphviz. v2.1.0 Add support for AST intermediate representation using module ast_pass Change compiler_pass module to three_address_code_pass v2.0.0 Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules. Refs Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ draw ( g = None ) Draw abstract syntax tree Parameters: Name Type Description Default g nx.DiGraph) None Returns: Type Description None Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def draw ( g : nx . DiGraph = None ): \"\"\" Draw abstract syntax tree Args: g (nx.DiGraph) : Returns: None \"\"\" pos = graphviz_layout ( G = g , prog = 'dot' , # prog='circo', ) # pos = hierarchy_pos(G=g,) # pos = nx.kamada_kawai_layout(G=g) # pos = nx.bipartite_layout(G=g, nodes=g.nodes) # pos = nx.spectral_layout(G=g) # pos = nx.spiral_layout(G=g) # pos = nx.spiral_layout(G=g) colors = [ g [ u ][ v ][ 'color' ] for u , v in g . edges ] nx . draw ( g , with_labels = False , node_size = 500 , node_color = 'black' , edge_color = colors , pos = pos , ) edge_labels = nx . get_edge_attributes ( g , 'edge_type' ) # print('#', edge_labels) nx . draw_networkx_edge_labels ( g , pos , edge_labels = edge_labels , ) node_labels = {} for node in g . nodes (): # set the node name as the key and the label as its value node_labels [ node ] = node . value nx . draw_networkx_labels ( g , pos , node_labels , font_size = 12 , font_color = 'w' ) plt . savefig ( '../../docs/figs/ast4.png' ) plt . show () draw_graphviz ( g = None ) Visualize abstract syntax tree with Graphviz Arges: g (nx.DiGraph): The abstract syntax tree to be converted to the dot file Returns: Type Description None References: [1] https://graphviz.org/Gallery/directed/psg.html Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def draw_graphviz ( g : nx . DiGraph = None ): \"\"\" Visualize abstract syntax tree with Graphviz Arges: g (nx.DiGraph): The abstract syntax tree to be converted to the dot file Returns: None References: [1] https://graphviz.org/Gallery/directed/psg.html \"\"\" pydot_graph = nx . drawing . nx_pydot . to_pydot ( g ) # nx.drawing.nx_pydot.write_dot(func_graph, self.cfg_path + str(self.domain_name) + '.dot') pydot_graph2 = pydot . Dot ( \"\" , graph_type = \"digraph\" ) nid = 0 for u , v in g . edges : if u . value == u ' \\u2593 ' : # u.value = 'NULL' node_u = pydot . Node ( name = f 'node_id_ { nid } ' , label = u . value , shape = 'box' ) nid += 1 else : node_u = pydot . Node ( name = u . value , label = u . value , shape = 'box' ) if v . value == u ' \\u2593 ' : # v.value = 'NULL' node_v = pydot . Node ( name = f 'node_id_ { nid } ' , label = v . value , shape = 'box' ) nid += 1 else : node_v = pydot . Node ( name = v . value , label = v . value , shape = 'box' ) print ( u . value , v . value ) # edge_obj_dict = dict() # edge_obj_dict.update({'color': g[u][v]['color']}) # edge_obj_dict.update({'label': g[u][v]['edge_type']}) edge_ = pydot . Edge ( src = node_u , dst = node_v , color = g [ u ][ v ][ 'color' ], label = g [ u ][ v ][ 'edge_type' ]) pydot_graph2 . add_node ( node_u ) pydot_graph2 . add_node ( node_v ) pydot_graph2 . add_edge ( edge_ ) pydot_graph2 . write ( '../../docs/figs/ast2gv.dot' , encoding = 'utf-8' , ) # pydot_graph2.write_png('../../docs/figs/ast2.png') result = subprocess . run ( [ 'dot' , '-Tpng' , '../../docs/figs/ast2gv.dot' , '-o' , '../../docs/figs/ast2gv.png' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) print ( result . returncode ) error_ = result . stderr . decode ( 'utf-8' ) print ( error_ ) hierarchy_pos ( G , root = None , width = 1.0 , vert_gap = 0.2 , vert_loc = 0 , xcenter = 0.5 ) From Joel's answer at https://stackoverflow.com/a/29597209/2966723. Licensed under Creative Commons Attribution-Share Alike If the graph is a tree this will return the positions to plot this in a hierarchical layout. Parameters: Name Type Description Default G nx.Graph the graph (must be a tree) required root nx.Node the root node of current branch None width float horizontal space allocated for this branch - avoids overlap with other branches 1.0 vert_gap float gap between levels of hierarchy 0.2 vert_loc float vertical location of root 0 xcenter float horizontal location of root 0.5 Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def hierarchy_pos ( G , root = None , width = 1. , vert_gap = 0.2 , vert_loc = 0 , xcenter = 0.5 ): \"\"\" From Joel's answer at https://stackoverflow.com/a/29597209/2966723. Licensed under Creative Commons Attribution-Share Alike If the graph is a tree this will return the positions to plot this in a hierarchical layout. Args: G (nx.Graph): the graph (must be a tree) root (nx.Node): the root node of current branch - if the tree is directed and this is not given, the root will be found and used - if the tree is directed and this is given, then the positions will be just for the descendants of this node. - if the tree is undirected and not given,then a random choice will be used. width (float): horizontal space allocated for this branch - avoids overlap with other branches vert_gap (float): gap between levels of hierarchy vert_loc (float): vertical location of root xcenter (float): horizontal location of root \"\"\" if not nx . is_tree ( G ): raise TypeError ( 'cannot use hierarchy_pos on a graph that is not a tree' ) if root is None : if isinstance ( G , nx . DiGraph ): root = next ( iter ( nx . topological_sort ( G ))) # allows back compatibility with nx version 1.11 else : root = random . choice ( list ( G . nodes )) def _hierarchy_pos ( G , root , width = 1. , vert_gap = 0.2 , vert_loc = 0 , xcenter = 0.5 , pos = None , parent = None ): \"\"\" see hierarchy_pos docstring for most arguments pos: a dict saying where all nodes go if they have been assigned parent: parent of this branch. - only affects it if non-directed \"\"\" if pos is None : pos = { root : ( xcenter , vert_loc )} else : pos [ root ] = ( xcenter , vert_loc ) children = list ( G . neighbors ( root )) if not isinstance ( G , nx . DiGraph ) and parent is not None : children . remove ( parent ) if len ( children ) != 0 : dx = width / len ( children ) nextx = xcenter - width / 2 - dx / 2 for child in children : nextx += dx pos = _hierarchy_pos ( G , child , width = dx , vert_gap = vert_gap , vert_loc = vert_loc - vert_gap , xcenter = nextx , pos = pos , parent = root ) return pos return _hierarchy_pos ( G , root , width , vert_gap , vert_loc , xcenter ) main ( args ) Create lexer and parser and execute AST listener Parameters: Name Type Description Default args Arg required Returns: Type Description None Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def main ( args ): \"\"\" Create lexer and parser and execute AST listener Args: args (Arg): Returns: None \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) print ( 'Input language_apps: \\n {0} ' . format ( stream )) print ( 'Result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement2Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement2Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener code_generator_listener = ThreeAddressCodeGeneratorListener () # code_generator_listener = ThreeAddressCodeGenerator2Listener() # ast_generator = ASTListener() # Step 7(a): Walk parse tree with a customized listener (Automatically) walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = code_generator_listener ) # or # walker.walk(t=parse_tree, listener=ast_generator) # print('\\nG=', ast_generator.g.edges) # draw(g=ast_generator.g) # draw_graphviz(g=ast_generator.g) # Step 7(b): Walk parse tree with a customize visitor (Manually) # code_generator_vistor = ThreeAddressCodeGeneratorVisitor() # code_generator_vistor = ThreeAddressCodeGenerator2Visitor() # code_generator_vistor.visitStart(ctx=parse_tree.getRuleContext())","title":"Assignment statement 2"},{"location":"language_applications/assignment_statement2main/#assignment-statement-grammar-version-2","text":"","title":"Assignment statement grammar (version 2)"},{"location":"language_applications/assignment_statement2main/#tree-address-code-generation-pass","text":"ANTLR 4.x listener and visitor implementation for intermediate code generation (Three addresses code) @author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201017 Compiler generator: ANTLR4.x Target language(s): Python3.x, -Changelog: -- v2.1.0 --- Add support for AST intermediate representation using module ast_pass --- Change compiler_pass module to three_address_code_pass -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules. Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Tree address code generation pass"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGenerator2Listener","text":"Type checking and generating three address language_apps (optimizing number of temporary variables) Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGenerator2Listener ( AssignmentStatement2Listener ): \"\"\" Type checking and generating three address language_apps (optimizing number of temporary variables) \"\"\" def __init__ ( self ): print ( 'Listener2 call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def remove_temp ( self ): self . temp_counter -= 1 def get_temp ( self ): return 'T' + str ( self . temp_counter ) @classmethod def is_temp ( cls , variable ): if variable [ 0 ] == 'T' : return True return False # ------------------ # Rule number def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): ctx . type_attr = 'float' ctx . value_attr = float ( ctx . getText ()) def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): ctx . type_attr = 'int' ctx . value_attr = int ( ctx . getText ()) # ------------------ # Rule factor def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): ctx . type_attr = 'string' ctx . value_attr = ctx . getText () def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . type_attr = ctx . number () . type_attr ctx . value_attr = ctx . number () . value_attr # ------------------ # Rule term def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot divide {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr = ctx . factor () . type_attr ctx . value_attr = ctx . factor () . value_attr # ------------------ # Rule expr def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot subtract {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr = ctx . term () . type_attr ctx . value_attr = ctx . term () . value_attr # ------------------ # Rule expr def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr ))","title":"ThreeAddressCodeGenerator2Listener"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGenerator2Visitor","text":"Type checking and generating three address language_apps (optimizing number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGenerator2Visitor ( AssignmentStatement2Visitor ): \"\"\" Type checking and generating three address language_apps (optimizing number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism \"\"\" def __init__ ( self ): print ( 'Visitor2 call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def remove_temp ( self ): self . temp_counter -= 1 def get_temp ( self ): return 'T' + str ( self . temp_counter ) @classmethod def is_temp ( cls , variable ): if variable [ 0 ] == 'T' : return True return False def visitStart ( self , ctx : AssignmentStatement2Parser . StartContext ): self . visit ( tree = ctx . prog ()) def visitProg ( self , ctx : AssignmentStatement2Parser . ProgContext ): if ctx . getChildCount () == 2 : self . visit ( tree = ctx . prog ()) ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . assign ()) return ctx . type_attr , ctx . value_attr def visitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . expr ()) print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) return ctx . type_attr , ctx . value_attr # ------------------ # Rule expr def visitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . expr () . value_attr ): ctx . value_attr = ctx . expr () . value_attr if self . is_temp ( ctx . term () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr , ctx . value_attr = self . visit ( ctx . term ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule term def visitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' if self . is_temp ( ctx . term () . value_attr ): ctx . value_attr = ctx . term () . value_attr if self . is_temp ( ctx . factor () . value_attr ): self . remove_temp () elif self . is_temp ( ctx . factor () . value_attr ): ctx . value_attr = ctx . factor () . value_attr else : ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . factor ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule factor def visitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): return self . visit ( tree = ctx . expr ()) def visitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): return 'string' , ctx . ID () . getText () def visitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): return self . visit ( tree = ctx . number ()) # ------------------ # Rule number def visitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): return 'float' , float ( ctx . FLOAT () . getText ()) def visitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): return 'int' , int ( ctx . INT () . getText ())","title":"ThreeAddressCodeGenerator2Visitor"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGeneratorListener","text":"Type checking and generating three address language_apps (not optimized) Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGeneratorListener ( AssignmentStatement2Listener ): \"\"\" Type checking and generating three address language_apps (not optimized) \"\"\" def __init__ ( self ): print ( 'Listener call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) # ------------------ # Rule number def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): ctx . type_attr = 'float' ctx . value_attr = float ( ctx . getText ()) def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): ctx . type_attr = 'int' ctx . value_attr = int ( ctx . getText ()) # ------------------ # Rule factor def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): ctx . type_attr = 'string' ctx . value_attr = str ( ctx . getText ()) def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . type_attr = ctx . number () . type_attr ctx . value_attr = ctx . number () . value_attr # ------------------ # Rule term def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot divide {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr = ctx . factor () . type_attr ctx . value_attr = ctx . factor () . value_attr # ------------------ # Rule expr def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot subtract {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr = ctx . term () . type_attr ctx . value_attr = ctx . term () . value_attr # ------------------ # Rule expr def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr = ctx . expr () . type_attr ctx . value_attr = ctx . expr () . value_attr print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr ))","title":"ThreeAddressCodeGeneratorListener"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.three_address_code_pass.ThreeAddressCodeGeneratorVisitor","text":"Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism Source code in language_apps\\assignment_statement_v2\\three_address_code_pass.py class ThreeAddressCodeGeneratorVisitor ( AssignmentStatement2Visitor ): \"\"\" Type checking and generating three address language_apps (not optimized regarding to the number of temporary variables) Utilizing ANTLR 4.x Visitor mechanism \"\"\" def __init__ ( self ): print ( 'Visitor call!' ) self . temp_counter = 0 def create_temp ( self ): self . temp_counter += 1 return 'T' + str ( self . temp_counter ) def visitStart ( self , ctx : AssignmentStatement2Parser . StartContext ): self . visit ( tree = ctx . prog ()) def visitProg ( self , ctx : AssignmentStatement2Parser . ProgContext ): if ctx . getChildCount () == 2 : self . visit ( tree = ctx . prog ()) ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . assign ()) return ctx . type_attr , ctx . value_attr def visitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . expr ()) print ( 'Assign statement: \" {0} = {1} \" \\n Assign type: \" {2} \"' . format ( ctx . ID () . getText (), ctx . value_attr , ctx . type_attr )) return ctx . type_attr , ctx . value_attr # ------------------ # Rule expr def visitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr + ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} + {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_minusContext ): ctx . expr () . type_attr , ctx . expr () . value_attr = self . visit ( tree = ctx . expr ()) ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) if ctx . expr () . type_attr != ctx . term () . type_attr : print ( 'Semantic error: Cannot plus {0} and {1} ' . format ( ctx . expr () . type_attr , ctx . term () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . expr () . value_attr - ctx . term () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} - {2} ' . format ( ctx . value_attr , ctx . expr () . value_attr , ctx . term () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . type_attr , ctx . value_attr = self . visit ( ctx . term ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule term def visitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = ctx . term () . value_attr * ctx . factor () . value_attr else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} * {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): ctx . term () . type_attr , ctx . term () . value_attr = self . visit ( tree = ctx . term ()) ctx . factor () . type_attr , ctx . factor () . value_attr = self . visit ( tree = ctx . factor ()) if ctx . term () . type_attr != ctx . factor () . type_attr : print ( 'Semantic error: Cannot multiply {0} and {1} ' . format ( ctx . term () . type_attr , ctx . factor () . type_attr )) quit ( - 1 ) else : if ctx . term () . type_attr == 'float' : ctx . type_attr = 'float' ctx . value_attr = ctx . term () . value_attr / ctx . factor () . value_attr elif ctx . term () . type_attr == 'int' : ctx . type_attr = 'int' ctx . value_attr = int ( ctx . term () . value_attr / ctx . factor () . value_attr ) else : ctx . type_attr = 'string' ctx . value_attr = self . create_temp () print ( ' {0} = {1} / {2} ' . format ( ctx . value_attr , ctx . term () . value_attr , ctx . factor () . value_attr )) return ctx . type_attr , ctx . value_attr def visitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . type_attr , ctx . value_attr = self . visit ( tree = ctx . factor ()) return ctx . type_attr , ctx . value_attr # ------------------ # Rule factor def visitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): return self . visit ( tree = ctx . expr ()) def visitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): return 'string' , ctx . ID () . getText () def visitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): return self . visit ( tree = ctx . number ()) # ------------------ # Rule number def visitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): return 'float' , float ( ctx . FLOAT () . getText ()) def visitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): return 'int' , int ( ctx . INT () . getText ())","title":"ThreeAddressCodeGeneratorVisitor"},{"location":"language_applications/assignment_statement2main/#abstract-syntax-tree-ast-generation-pass","text":"ANTLR 4.x listener and visitor implementation for intermediate code generation (abstract syntax trees) @author: Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) @date: 20201117 Compiler generator: ANTRL4.x Target language(s): Python3.x, -Changelog: -- v2.1.0 --- Add support for AST visualization with dummy nodes --- Add support for AST intermediate representation using module ast_pass --- Change compiler_pass module to three_address_code_pass -- v2.0.0 --- Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules. Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Abstract syntax tree (AST) generation pass"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.abstract_syntax_tree_pass.ASTListener","text":"Source code in language_apps\\assignment_statement_v2\\abstract_syntax_tree_pass.py class ASTListener ( AssignmentStatement2Listener ): \"\"\" \"\"\" def __init__ ( self ): self . ast = AST () # Data structure for holding the abstract syntax tree self . q = queue . Queue () # Use to print and visualize AST self . g = nx . DiGraph () # Use to visualize AST # self.q.empty() # print('Q=', ) def print_tree ( self , node = None , level = 1 ): if node is None : # print() return # if not self.q.empty(): # print('Parent:', self.q.get().value) # print('\\t'*level, end='') print () while node is not None : current_node = node print ( current_node . value , end = '' ) # alt+196 = \u2500\u2500\u2500, alt+178=\u2593 if node . child is not None : # self.q.put(node) self . g . add_edge ( current_node , node . child , edge_type = 'C' , color = 'red' ) self . q . put ( node . child ) else : tn = TreeNode ( value = '\u2593' , child = None , brother = None ) self . g . add_edge ( current_node , tn , edge_type = 'C' , color = 'red' ) node = node . brother if node is not None : print ( ' \\t \u2500\u2500\u2500 \\t ' , end = '' ) self . g . add_edge ( current_node , node , edge_type = 'B' , color = 'blue' ) else : tn = TreeNode ( value = '\u2593' , child = None , brother = None ) self . g . add_edge ( current_node , tn , edge_type = 'B' , color = 'blue' ) if not self . q . empty (): self . print_tree ( node = self . q . get (), level = level + 1 ) def print_tree2 ( self , node = None ): pass def exitAssign ( self , ctx : AssignmentStatement2Parser . AssignContext ): idPntr = self . ast . make_node ( value = ctx . ID () . getText (), child = None , brother = ctx . expr () . value_attr ) assPntr = self . ast . make_node ( value = \":=\" , child = idPntr , brother = None ) ctx . value_attr = assPntr self . ast . root = assPntr self . print_tree ( node = self . ast . root , level = 1 ) def exitExpr_term_plus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): self . ast . add_brother ( ctx . expr () . value_attr , ctx . term () . value_attr ) exprPntr = self . ast . make_node ( value = \"+\" , child = ctx . expr () . value_attr , brother = None ) ctx . value_attr = exprPntr def exitExpr_term_minus ( self , ctx : AssignmentStatement2Parser . Expr_term_plusContext ): self . ast . add_brother ( ctx . expr () . value_attr , ctx . term () . value_attr ) exprPntr = self . ast . make_node ( value = \"-\" , child = ctx . expr () . value_attr , brother = None ) ctx . value_attr = exprPntr def exitTerm4 ( self , ctx : AssignmentStatement2Parser . Term4Context ): ctx . value_attr = ctx . term () . value_attr # ---------------------- def exitTerm_fact_mutiply ( self , ctx : AssignmentStatement2Parser . Term_fact_mutiplyContext ): self . ast . add_brother ( ctx . term () . value_attr , ctx . factor () . value_attr ) termPntr = self . ast . make_node ( value = \"*\" , child = ctx . term () . value_attr , brother = None ) ctx . value_attr = termPntr def exitTerm_fact_divide ( self , ctx : AssignmentStatement2Parser . Term_fact_divideContext ): self . ast . add_brother ( ctx . term () . value_attr , ctx . factor () . value_attr ) termPntr = self . ast . make_node ( value = \"/\" , child = ctx . term () . value_attr , brother = None ) ctx . value_attr = termPntr def exitFactor3 ( self , ctx : AssignmentStatement2Parser . Factor3Context ): ctx . value_attr = ctx . factor () . value_attr # --------------------- def exitFact_expr ( self , ctx : AssignmentStatement2Parser . Fact_exprContext ): ctx . value_attr = ctx . expr () . value_attr def exitFact_id ( self , ctx : AssignmentStatement2Parser . Fact_idContext ): idPntr = self . ast . make_node ( value = ctx . ID () . getText (), child = None , brother = None ) ctx . value_attr = idPntr def exitFact_number ( self , ctx : AssignmentStatement2Parser . Fact_numberContext ): ctx . value_attr = ctx . number () . value_attr # ---------------------- def exitNumber_float ( self , ctx : AssignmentStatement2Parser . Number_floatContext ): numberPntr = self . ast . make_node ( value = ctx . FLOAT () . getText (), child = None , brother = None ) ctx . value_attr = numberPntr def exitNumber_int ( self , ctx : AssignmentStatement2Parser . Number_intContext ): numberPntr = self . ast . make_node ( value = ctx . INT () . getText (), child = None , brother = None ) ctx . value_attr = numberPntr","title":"ASTListener"},{"location":"language_applications/assignment_statement2main/#main-driver","text":"Main script for grammar AssignmentStatement2 (version 2) Contains attributes for holding rule type and rule intermediate representations (AST and Three-addresses codes)","title":"Main driver"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--author","text":"Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)","title":"author"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--date","text":"20201029 Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x","title":"date"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--install-pygraphviz","text":"To draw AST as a binary tree you need to install the pygraphviz 1- download and install graphviz (for Windows/ Linux) 2- add graphviz to system path 3- install pygraphviz using the following command python -m pip install --global-option=build_ext --global-option=\"-IC:\\Program Files\\Graphviz\\include\" --global-option=\"-LC:\\Program Files\\Graphviz\\lib\" pygraphviz","title":"Install pygraphviz"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--changelog","text":"","title":"Changelog"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v211","text":"Add visualization with Graphviz.","title":"v2.1.1"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v210","text":"Add support for AST intermediate representation using module ast_pass Change compiler_pass module to three_address_code_pass","title":"v2.1.0"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--v200","text":"Add attributes for grammar rules which are used to hold type and intermediate language_apps of rules.","title":"v2.0.0"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main--refs","text":"Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Refs"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.draw","text":"Draw abstract syntax tree Parameters: Name Type Description Default g nx.DiGraph) None Returns: Type Description None Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def draw ( g : nx . DiGraph = None ): \"\"\" Draw abstract syntax tree Args: g (nx.DiGraph) : Returns: None \"\"\" pos = graphviz_layout ( G = g , prog = 'dot' , # prog='circo', ) # pos = hierarchy_pos(G=g,) # pos = nx.kamada_kawai_layout(G=g) # pos = nx.bipartite_layout(G=g, nodes=g.nodes) # pos = nx.spectral_layout(G=g) # pos = nx.spiral_layout(G=g) # pos = nx.spiral_layout(G=g) colors = [ g [ u ][ v ][ 'color' ] for u , v in g . edges ] nx . draw ( g , with_labels = False , node_size = 500 , node_color = 'black' , edge_color = colors , pos = pos , ) edge_labels = nx . get_edge_attributes ( g , 'edge_type' ) # print('#', edge_labels) nx . draw_networkx_edge_labels ( g , pos , edge_labels = edge_labels , ) node_labels = {} for node in g . nodes (): # set the node name as the key and the label as its value node_labels [ node ] = node . value nx . draw_networkx_labels ( g , pos , node_labels , font_size = 12 , font_color = 'w' ) plt . savefig ( '../../docs/figs/ast4.png' ) plt . show ()","title":"draw()"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.draw_graphviz","text":"Visualize abstract syntax tree with Graphviz Arges: g (nx.DiGraph): The abstract syntax tree to be converted to the dot file Returns: Type Description None References: [1] https://graphviz.org/Gallery/directed/psg.html Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def draw_graphviz ( g : nx . DiGraph = None ): \"\"\" Visualize abstract syntax tree with Graphviz Arges: g (nx.DiGraph): The abstract syntax tree to be converted to the dot file Returns: None References: [1] https://graphviz.org/Gallery/directed/psg.html \"\"\" pydot_graph = nx . drawing . nx_pydot . to_pydot ( g ) # nx.drawing.nx_pydot.write_dot(func_graph, self.cfg_path + str(self.domain_name) + '.dot') pydot_graph2 = pydot . Dot ( \"\" , graph_type = \"digraph\" ) nid = 0 for u , v in g . edges : if u . value == u ' \\u2593 ' : # u.value = 'NULL' node_u = pydot . Node ( name = f 'node_id_ { nid } ' , label = u . value , shape = 'box' ) nid += 1 else : node_u = pydot . Node ( name = u . value , label = u . value , shape = 'box' ) if v . value == u ' \\u2593 ' : # v.value = 'NULL' node_v = pydot . Node ( name = f 'node_id_ { nid } ' , label = v . value , shape = 'box' ) nid += 1 else : node_v = pydot . Node ( name = v . value , label = v . value , shape = 'box' ) print ( u . value , v . value ) # edge_obj_dict = dict() # edge_obj_dict.update({'color': g[u][v]['color']}) # edge_obj_dict.update({'label': g[u][v]['edge_type']}) edge_ = pydot . Edge ( src = node_u , dst = node_v , color = g [ u ][ v ][ 'color' ], label = g [ u ][ v ][ 'edge_type' ]) pydot_graph2 . add_node ( node_u ) pydot_graph2 . add_node ( node_v ) pydot_graph2 . add_edge ( edge_ ) pydot_graph2 . write ( '../../docs/figs/ast2gv.dot' , encoding = 'utf-8' , ) # pydot_graph2.write_png('../../docs/figs/ast2.png') result = subprocess . run ( [ 'dot' , '-Tpng' , '../../docs/figs/ast2gv.dot' , '-o' , '../../docs/figs/ast2gv.png' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) print ( result . returncode ) error_ = result . stderr . decode ( 'utf-8' ) print ( error_ )","title":"draw_graphviz()"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.hierarchy_pos","text":"From Joel's answer at https://stackoverflow.com/a/29597209/2966723. Licensed under Creative Commons Attribution-Share Alike If the graph is a tree this will return the positions to plot this in a hierarchical layout. Parameters: Name Type Description Default G nx.Graph the graph (must be a tree) required root nx.Node the root node of current branch None width float horizontal space allocated for this branch - avoids overlap with other branches 1.0 vert_gap float gap between levels of hierarchy 0.2 vert_loc float vertical location of root 0 xcenter float horizontal location of root 0.5 Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def hierarchy_pos ( G , root = None , width = 1. , vert_gap = 0.2 , vert_loc = 0 , xcenter = 0.5 ): \"\"\" From Joel's answer at https://stackoverflow.com/a/29597209/2966723. Licensed under Creative Commons Attribution-Share Alike If the graph is a tree this will return the positions to plot this in a hierarchical layout. Args: G (nx.Graph): the graph (must be a tree) root (nx.Node): the root node of current branch - if the tree is directed and this is not given, the root will be found and used - if the tree is directed and this is given, then the positions will be just for the descendants of this node. - if the tree is undirected and not given,then a random choice will be used. width (float): horizontal space allocated for this branch - avoids overlap with other branches vert_gap (float): gap between levels of hierarchy vert_loc (float): vertical location of root xcenter (float): horizontal location of root \"\"\" if not nx . is_tree ( G ): raise TypeError ( 'cannot use hierarchy_pos on a graph that is not a tree' ) if root is None : if isinstance ( G , nx . DiGraph ): root = next ( iter ( nx . topological_sort ( G ))) # allows back compatibility with nx version 1.11 else : root = random . choice ( list ( G . nodes )) def _hierarchy_pos ( G , root , width = 1. , vert_gap = 0.2 , vert_loc = 0 , xcenter = 0.5 , pos = None , parent = None ): \"\"\" see hierarchy_pos docstring for most arguments pos: a dict saying where all nodes go if they have been assigned parent: parent of this branch. - only affects it if non-directed \"\"\" if pos is None : pos = { root : ( xcenter , vert_loc )} else : pos [ root ] = ( xcenter , vert_loc ) children = list ( G . neighbors ( root )) if not isinstance ( G , nx . DiGraph ) and parent is not None : children . remove ( parent ) if len ( children ) != 0 : dx = width / len ( children ) nextx = xcenter - width / 2 - dx / 2 for child in children : nextx += dx pos = _hierarchy_pos ( G , child , width = dx , vert_gap = vert_gap , vert_loc = vert_loc - vert_gap , xcenter = nextx , pos = pos , parent = root ) return pos return _hierarchy_pos ( G , root , width , vert_gap , vert_loc , xcenter )","title":"hierarchy_pos()"},{"location":"language_applications/assignment_statement2main/#language_apps.assignment_statement_v2.assignment_statement2main.main","text":"Create lexer and parser and execute AST listener Parameters: Name Type Description Default args Arg required Returns: Type Description None Source code in language_apps\\assignment_statement_v2\\assignment_statement2main.py def main ( args ): \"\"\" Create lexer and parser and execute AST listener Args: args (Arg): Returns: None \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) print ( 'Input language_apps: \\n {0} ' . format ( stream )) print ( 'Result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement2Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement2Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener code_generator_listener = ThreeAddressCodeGeneratorListener () # code_generator_listener = ThreeAddressCodeGenerator2Listener() # ast_generator = ASTListener() # Step 7(a): Walk parse tree with a customized listener (Automatically) walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = code_generator_listener ) # or # walker.walk(t=parse_tree, listener=ast_generator) # print('\\nG=', ast_generator.g.edges) # draw(g=ast_generator.g) # draw_graphviz(g=ast_generator.g) # Step 7(b): Walk parse tree with a customize visitor (Manually) # code_generator_vistor = ThreeAddressCodeGeneratorVisitor() # code_generator_vistor = ThreeAddressCodeGenerator2Visitor() # code_generator_vistor.visitStart(ctx=parse_tree.getRuleContext())","title":"main()"},{"location":"language_applications/assignment_statement3main/","text":"Assignment statement grammar (version 3) Main script for grammar AssignmentStatement3 (version 3) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes) author Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) date 20201028 Required Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x Changelog v3.0 Add semantic rules to perferm type checking Add semantic routines to generate intermediate representation (three addresses codes) Refs Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ main ( args ) Create lexer and parser for language application Parameters: Name Type Description Default args string command line arguments required return None required Source code in language_apps\\assignment_statement_v3\\assignment_statement3main.py def main ( args ): \"\"\" Create lexer and parser for language application Args: args (string): command line arguments return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() print ( 'Input stream:' ) print ( stream ) print ( 'Compiler result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement3Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement3Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) quit () lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"Assignment statement 3"},{"location":"language_applications/assignment_statement3main/#assignment-statement-grammar-version-3","text":"Main script for grammar AssignmentStatement3 (version 3) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes)","title":"Assignment statement grammar (version 3)"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--author","text":"Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)","title":"author"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--date","text":"20201028","title":"date"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--required","text":"Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x","title":"Required"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--changelog","text":"","title":"Changelog"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--v30","text":"Add semantic rules to perferm type checking Add semantic routines to generate intermediate representation (three addresses codes)","title":"v3.0"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main--refs","text":"Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Refs"},{"location":"language_applications/assignment_statement3main/#language_apps.assignment_statement_v3.assignment_statement3main.main","text":"Create lexer and parser for language application Parameters: Name Type Description Default args string command line arguments required return None required Source code in language_apps\\assignment_statement_v3\\assignment_statement3main.py def main ( args ): \"\"\" Create lexer and parser for language application Args: args (string): command line arguments return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() print ( 'Input stream:' ) print ( stream ) print ( 'Compiler result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement3Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement3Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) quit () lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"main()"},{"location":"language_applications/assignment_statement4main/","text":"Assignment statement grammar (version 4) Main script for grammar AssignmentStatement4 (version 4) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes) Also, generates intermediate representation (three addresses codes) with minimum number of 'temp' variables author Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/) date 20201029 Required Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x Changelog v4.0 Generate intermediate representation (three addresses codes) with minimum number of 'temp' variables v3.0 Add semantic rules to perferm type checking Add semantic routines to generate intermediate representation (three addresses codes) Refs Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/ main ( args ) Create lexer and parser for language application Parameters: Name Type Description Default args string command line arguments required return None required Source code in language_apps\\assignment_statement_v4\\assignment_statement4main.py def main ( args ): \"\"\" Create lexer and parser for language application Args: args (string): command line arguments return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() print ( 'Input stream:' ) print ( stream ) print ( 'Compiler result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement4Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement4Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) quit () lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"Assignment statement 4"},{"location":"language_applications/assignment_statement4main/#assignment-statement-grammar-version-4","text":"Main script for grammar AssignmentStatement4 (version 4) Contains semantic rules to perform type checking and semantic routines to generate intermediate representation (three addresses codes) Also, generates intermediate representation (three addresses codes) with minimum number of 'temp' variables","title":"Assignment statement grammar (version 4)"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--author","text":"Morteza Zakeri, (http://webpages.iust.ac.ir/morteza_zakeri/)","title":"author"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--date","text":"20201029","title":"date"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--required","text":"Compiler generator: ANTLR 4.x Target language(s): Python 3.8.x","title":"Required"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--changelog","text":"","title":"Changelog"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--v40","text":"Generate intermediate representation (three addresses codes) with minimum number of 'temp' variables","title":"v4.0"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--v30","text":"Add semantic rules to perferm type checking Add semantic routines to generate intermediate representation (three addresses codes)","title":"v3.0"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main--refs","text":"Reference: Compiler book by Dr. Saeed Parsa (http://parsa.iust.ac.ir/) Course website: http://parsa.iust.ac.ir/courses/compilers/ Laboratory website: http://reverse.iust.ac.ir/","title":"Refs"},{"location":"language_applications/assignment_statement4main/#language_apps.assignment_statement_v4.assignment_statement4main.main","text":"Create lexer and parser for language application Parameters: Name Type Description Default args string command line arguments required return None required Source code in language_apps\\assignment_statement_v4\\assignment_statement4main.py def main ( args ): \"\"\" Create lexer and parser for language application Args: args (string): command line arguments return (None): \"\"\" # Step 1: Load input source into stream stream = FileStream ( args . file , encoding = 'utf8' ) # input_stream = StdinStream() print ( 'Input stream:' ) print ( stream ) print ( 'Compiler result:' ) # Step 2: Create an instance of AssignmentStLexer lexer = AssignmentStatement4Lexer ( stream ) # Step 3: Convert the input source into a list of tokens token_stream = CommonTokenStream ( lexer ) # Step 4: Create an instance of the AssignmentStParser parser = AssignmentStatement4Parser ( token_stream ) # Step 5: Create parse tree parse_tree = parser . start () # Step 6: Create an instance of AssignmentStListener my_listener = MyListener () walker = ParseTreeWalker () walker . walk ( t = parse_tree , listener = my_listener ) quit () lexer . reset () token = lexer . nextToken () while token . type != Token . EOF : print ( 'Token text: ' , token . text , 'Token line: ' , token . line ) token = lexer . nextToken ()","title":"main()"},{"location":"language_applications/main/","text":"Main There are four language application in this repository assignment_statement_v1 assignment_statement_v2 assignment_statement_v3 assignment_statement_v4 The main module of IUST Compiler project. Refer to language_apps package to find classroom code snippets. Main Welcome to Compiler course This file contains the main script for all code snippets Source code in iust_compilers_teaching\\main.py class Main : \"\"\"Welcome to Compiler course This file contains the main script for all code snippets \"\"\" @classmethod def print_welcome ( cls , name ) -> None : \"\"\" Print welcome message :param name: :return: \"\"\" print ( f 'Welcome to our dragon course { name } .' ) def tokenize_name ( self , method_name ): method_name = 'getSdfsdfsdtudentNsdfdsfumber' identifier_parts = list () # First: split based-on CamelCase matches = re . finditer ( '.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)' , method_name ) camel_cases = [ m . group ( 0 ) for m in matches ] # Second: split based-on underscore character '_' for case in camel_cases : case = case . lower () case = case . split ( '_' ) identifier_parts . extend ( case ) print ( f 'Method name tokens { camel_cases } .' ) print_welcome ( name ) classmethod Print welcome message :param name: :return: Source code in iust_compilers_teaching\\main.py @classmethod def print_welcome ( cls , name ) -> None : \"\"\" Print welcome message :param name: :return: \"\"\" print ( f 'Welcome to our dragon course { name } .' )","title":"Main"},{"location":"language_applications/main/#main_1","text":"There are four language application in this repository assignment_statement_v1 assignment_statement_v2 assignment_statement_v3 assignment_statement_v4 The main module of IUST Compiler project. Refer to language_apps package to find classroom code snippets.","title":"Main"},{"location":"language_applications/main/#main.Main","text":"Welcome to Compiler course This file contains the main script for all code snippets Source code in iust_compilers_teaching\\main.py class Main : \"\"\"Welcome to Compiler course This file contains the main script for all code snippets \"\"\" @classmethod def print_welcome ( cls , name ) -> None : \"\"\" Print welcome message :param name: :return: \"\"\" print ( f 'Welcome to our dragon course { name } .' ) def tokenize_name ( self , method_name ): method_name = 'getSdfsdfsdtudentNsdfdsfumber' identifier_parts = list () # First: split based-on CamelCase matches = re . finditer ( '.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)' , method_name ) camel_cases = [ m . group ( 0 ) for m in matches ] # Second: split based-on underscore character '_' for case in camel_cases : case = case . lower () case = case . split ( '_' ) identifier_parts . extend ( case ) print ( f 'Method name tokens { camel_cases } .' )","title":"Main"},{"location":"language_applications/main/#main.Main.print_welcome","text":"Print welcome message :param name: :return: Source code in iust_compilers_teaching\\main.py @classmethod def print_welcome ( cls , name ) -> None : \"\"\" Print welcome message :param name: :return: \"\"\" print ( f 'Welcome to our dragon course { name } .' )","title":"print_welcome()"},{"location":"lectures/","text":"\ud83c\udf93 Compiler Design Course Pamphlets \ud83d\udcda Welcome to the repository for the Compiler Design Course Pamphlets! \ud83c\udf89 This repository contains a collection of pamphlets that I've created to help others understand some of the fundamental concepts in compiler design. I've taken the time to edit and rewrite these pamphlets in a more friendly and engaging way, making them easier to understand and more approachable to beginners. \ud83d\udcdd \ud83d\udcd6 Table of Contents Introduction Compiler Basic Bootstrapping Regular Expresion and Lexical Analysis Manual Construction of Lexers 05 Automatic construction of Lexers Syntax Analysis Types of Grammar Ambiguity Problems and Eliminating the Ambiguity Top Down Bottom Down \ud83d\udcda Course Topics These pamphlets cover a wide range of topics in compiler design, including: Course Overview Overall Compiler Architecture and Design: Explore the fundamental structure and design principles underlying compilers. Compiler Internals and Compiler Generators: Gain an in-depth understanding of the internal workings of compilers and explore tools for generating compilers. Understanding Compiler Functionality: Learn what compilers do and unravel the intricacies of their functionality. Insights into Compilation Processes: Grasp the inner workings of compilation processes and understand how they translate high-level code into executable machine code. Hands-On Compiler Construction: Engage in practical exercises to build your own compilers, applying theoretical concepts to real-world scenarios. Applied Formal Language and Automata: Explore formal language theory and automata, understanding their application in the context of compiler construction. Various Parsing Methods and Techniques: Dive into different parsing methods and techniques employed in the analysis of source code. Low-Level Code Generation and Optimization: Learn about the intricacies of generating efficient low-level code and optimizing it for enhanced performance. Intellectual Paradigms in System Programming and Testing: Gain insights into the intellectual paradigms that underpin system programming and the crucial role of testing in compiler development. Embark on this educational journey, where you will not only gain theoretical knowledge but also acquire practical skills in building and understanding compilers. Let's explore the intricate world of compiler design together. \ud83d\udc69\u200d\ud83d\udcbb Contributing I welcome contributions to these pamphlets! If you have any suggestions or improvements, please feel free to submit a pull request. I'm always looking for ways to improve these pamphlets and make them more helpful to others. \ud83d\ude4c","title":"\ud83c\udf93 Compiler Design Course Pamphlets \ud83d\udcda"},{"location":"lectures/#compiler-design-course-pamphlets","text":"Welcome to the repository for the Compiler Design Course Pamphlets! \ud83c\udf89 This repository contains a collection of pamphlets that I've created to help others understand some of the fundamental concepts in compiler design. I've taken the time to edit and rewrite these pamphlets in a more friendly and engaging way, making them easier to understand and more approachable to beginners. \ud83d\udcdd","title":"\ud83c\udf93 Compiler Design Course Pamphlets \ud83d\udcda"},{"location":"lectures/#table-of-contents","text":"Introduction Compiler Basic Bootstrapping Regular Expresion and Lexical Analysis Manual Construction of Lexers 05 Automatic construction of Lexers Syntax Analysis Types of Grammar Ambiguity Problems and Eliminating the Ambiguity Top Down Bottom Down","title":"\ud83d\udcd6 Table of Contents"},{"location":"lectures/#course-topics","text":"These pamphlets cover a wide range of topics in compiler design, including:","title":"\ud83d\udcda Course Topics"},{"location":"lectures/#course-overview","text":"Overall Compiler Architecture and Design: Explore the fundamental structure and design principles underlying compilers. Compiler Internals and Compiler Generators: Gain an in-depth understanding of the internal workings of compilers and explore tools for generating compilers. Understanding Compiler Functionality: Learn what compilers do and unravel the intricacies of their functionality. Insights into Compilation Processes: Grasp the inner workings of compilation processes and understand how they translate high-level code into executable machine code. Hands-On Compiler Construction: Engage in practical exercises to build your own compilers, applying theoretical concepts to real-world scenarios. Applied Formal Language and Automata: Explore formal language theory and automata, understanding their application in the context of compiler construction. Various Parsing Methods and Techniques: Dive into different parsing methods and techniques employed in the analysis of source code. Low-Level Code Generation and Optimization: Learn about the intricacies of generating efficient low-level code and optimizing it for enhanced performance. Intellectual Paradigms in System Programming and Testing: Gain insights into the intellectual paradigms that underpin system programming and the crucial role of testing in compiler development. Embark on this educational journey, where you will not only gain theoretical knowledge but also acquire practical skills in building and understanding compilers. Let's explore the intricate world of compiler design together.","title":"Course Overview"},{"location":"lectures/#contributing","text":"I welcome contributions to these pamphlets! If you have any suggestions or improvements, please feel free to submit a pull request. I'm always looking for ways to improve these pamphlets and make them more helpful to others. \ud83d\ude4c","title":"\ud83d\udc69\u200d\ud83d\udcbb Contributing"},{"location":"lectures/00_Introduction/","text":"Introduction What is Compiler? A compiler is a software program that takes source code written in a high-level programming language and converts it into a lower-level language that can be executed by a machine. The output of this process is often referred to as object code or machine code. The purpose of a compiler is to enable the execution of a program on a specific hardware platform. a simplified overview of how a compiler works: Source Code: Programmers write human-readable code in a high-level programming language like C, C++, Java, or Python. Compilation: The source code is fed into a compiler. The compiler analyzes the code, performs various optimizations, and generates an equivalent machine code or intermediate code. Object Code/Executable: The output of the compiler is typically a binary file or set of files containing machine code or intermediate code. This can be directly executed by the computer's hardware or by a virtual machine, depending on the programming language. Linking (optional): In some languages, the compilation process involves linking, where multiple compiled files or libraries are combined to create the final executable. The process of compiling code involves several steps: Lexical Analysis : The compiler splits the source code into lexemes, which are individual code fragments that represent specific patterns in the code. The lexemes are then tokenized in preparation for syntax and semantic analyses. Syntax Analysis : The compiler verifies that the code's syntax is correct, based on the rules for the source language. This process is also referred to as parsing. During this step, the compiler typically creates abstract syntax trees that represent the logical structures of specific code elements. Semantic Analysis : The compiler verifies the validity of the code's logic. This step goes beyond syntax analysis by validating the code's accuracy. For example, the semantic analysis might check whether variables have been assigned the right types or have been properly declared. Intermediate Code Generation : After the code passes through all three analysis phases, the compiler generates an intermediate representation (IR) of the source code. The IR code makes it easier to translate the source code into a different format. However, it must accurately represent the source code in every respect, without omitting any functionality. Optimization : The compiler optimizes the IR code in preparation for the final code generation. The type and extent of optimization depends on the compiler. Some compilers let users configure the degree of optimization. Output Code Generation : The compiler generates the final output code, using the optimized IR code. Compilers are important because they enable developers to write code in high-level programming languages, which are easier to understand and more human-readable than machine code. They also provide portability, as the machine code generated can be run on many different operating systems and hardware architectures. Additionally, compilers can provide programmer security by preventing memory-related errors, such as buffer overflows, by analyzing and optimizing the code. However, it's important to note that the code produced by a compiler is platform-dependent. This means that compiled code produces a machine-readable and machine-specific executable file that only the particular type of machine is able to execute. For example, code compiled on a Windows machine won\u2019t run on a Mac or Linux system without being recompiled. The main advantages of using a compiler include: Performance: Compiled code often runs faster than interpreted code because it's already translated into machine code. Platform Independence: In some cases, the compiled code can be executed on different platforms without modification, especially if it's compiled to an intermediate code that's interpreted by a virtual machine. Compiled Code VS Interpreted Code Compiled code and interpreted code represent two different approaches to executing computer programs. Here are the key differences between them: Compiled Code: Translation Process: Compilation: The entire source code is translated into machine code or an intermediate code by a compiler before execution. Output: The result of compilation is often an executable file containing machine code that can be directly executed by the computer's hardware. Execution: Direct Execution: The compiled code is executed directly by the computer's processor. Performance: Generally, compiled code tends to be faster than interpreted code because it is pre-translated into machine code. Examples: Common examples of compiled languages include C, C++, and Rust. Portability: Compiled code is often less portable than interpreted code, as the compiled binary may be specific to the architecture or platform for which it was compiled. Interpreted Code: Translation Process: Interpretation: The source code is translated and executed line by line or statement by statement by an interpreter. Output: No separate compilation step is required, and the source code is directly interpreted during execution. Execution: Interpretation: The interpreter reads and executes the source code directly without generating an intermediate machine code or binary file. Performance: Interpreted code can be slower than compiled code because it is translated on-the-fly during execution. Examples: Examples of interpreted languages include Python, JavaScript, and Ruby. Portability: Interpreted code is often more portable as it can be run on any system with the appropriate interpreter. However, the interpreter itself needs to be available for each platform. in summary, A compiler translates a programming language (source language) into executable code (target language)","title":"Unit 0 Introduction"},{"location":"lectures/00_Introduction/#introduction","text":"","title":" Introduction"},{"location":"lectures/00_Introduction/#_1","text":"","title":""},{"location":"lectures/00_Introduction/#what-is-compiler","text":"A compiler is a software program that takes source code written in a high-level programming language and converts it into a lower-level language that can be executed by a machine. The output of this process is often referred to as object code or machine code. The purpose of a compiler is to enable the execution of a program on a specific hardware platform. a simplified overview of how a compiler works: Source Code: Programmers write human-readable code in a high-level programming language like C, C++, Java, or Python. Compilation: The source code is fed into a compiler. The compiler analyzes the code, performs various optimizations, and generates an equivalent machine code or intermediate code. Object Code/Executable: The output of the compiler is typically a binary file or set of files containing machine code or intermediate code. This can be directly executed by the computer's hardware or by a virtual machine, depending on the programming language. Linking (optional): In some languages, the compilation process involves linking, where multiple compiled files or libraries are combined to create the final executable. The process of compiling code involves several steps: Lexical Analysis : The compiler splits the source code into lexemes, which are individual code fragments that represent specific patterns in the code. The lexemes are then tokenized in preparation for syntax and semantic analyses. Syntax Analysis : The compiler verifies that the code's syntax is correct, based on the rules for the source language. This process is also referred to as parsing. During this step, the compiler typically creates abstract syntax trees that represent the logical structures of specific code elements. Semantic Analysis : The compiler verifies the validity of the code's logic. This step goes beyond syntax analysis by validating the code's accuracy. For example, the semantic analysis might check whether variables have been assigned the right types or have been properly declared. Intermediate Code Generation : After the code passes through all three analysis phases, the compiler generates an intermediate representation (IR) of the source code. The IR code makes it easier to translate the source code into a different format. However, it must accurately represent the source code in every respect, without omitting any functionality. Optimization : The compiler optimizes the IR code in preparation for the final code generation. The type and extent of optimization depends on the compiler. Some compilers let users configure the degree of optimization. Output Code Generation : The compiler generates the final output code, using the optimized IR code. Compilers are important because they enable developers to write code in high-level programming languages, which are easier to understand and more human-readable than machine code. They also provide portability, as the machine code generated can be run on many different operating systems and hardware architectures. Additionally, compilers can provide programmer security by preventing memory-related errors, such as buffer overflows, by analyzing and optimizing the code. However, it's important to note that the code produced by a compiler is platform-dependent. This means that compiled code produces a machine-readable and machine-specific executable file that only the particular type of machine is able to execute. For example, code compiled on a Windows machine won\u2019t run on a Mac or Linux system without being recompiled. The main advantages of using a compiler include: Performance: Compiled code often runs faster than interpreted code because it's already translated into machine code. Platform Independence: In some cases, the compiled code can be executed on different platforms without modification, especially if it's compiled to an intermediate code that's interpreted by a virtual machine.","title":"What is Compiler?"},{"location":"lectures/00_Introduction/#compiled-code-vs-interpreted-code","text":"","title":"Compiled Code VS Interpreted Code"},{"location":"lectures/00_Introduction/#_2","text":"Compiled code and interpreted code represent two different approaches to executing computer programs. Here are the key differences between them:","title":""},{"location":"lectures/00_Introduction/#compiled-code","text":"Translation Process: Compilation: The entire source code is translated into machine code or an intermediate code by a compiler before execution. Output: The result of compilation is often an executable file containing machine code that can be directly executed by the computer's hardware. Execution: Direct Execution: The compiled code is executed directly by the computer's processor. Performance: Generally, compiled code tends to be faster than interpreted code because it is pre-translated into machine code. Examples: Common examples of compiled languages include C, C++, and Rust. Portability: Compiled code is often less portable than interpreted code, as the compiled binary may be specific to the architecture or platform for which it was compiled.","title":"Compiled Code:"},{"location":"lectures/00_Introduction/#interpreted-code","text":"Translation Process: Interpretation: The source code is translated and executed line by line or statement by statement by an interpreter. Output: No separate compilation step is required, and the source code is directly interpreted during execution. Execution: Interpretation: The interpreter reads and executes the source code directly without generating an intermediate machine code or binary file. Performance: Interpreted code can be slower than compiled code because it is translated on-the-fly during execution. Examples: Examples of interpreted languages include Python, JavaScript, and Ruby. Portability: Interpreted code is often more portable as it can be run on any system with the appropriate interpreter. However, the interpreter itself needs to be available for each platform. in summary, A compiler translates a programming language (source language) into executable code (target language)","title":"Interpreted Code:"},{"location":"lectures/00_Introduction/#_3","text":"","title":""},{"location":"lectures/01_Compiler-Basic/","text":"Compiler Basic Compiler Formalization In compiler design, it's essential to precisely describe the input (source code) and output (machine code) of a compiler. Input: Source code (programming language) Output: Assembly (machine) code Describing Programming Language Syntax Programming languages have two crucial components: syntax and semantics. Syntax Syntax defines which strings of symbols are valid expressions in the language. Example (C++): int z = 0; // Valid statement std::cout << \"z+1=\" << z++ << std::endl; // Valid statement Semantics Semantics defines the meaning or behavior of valid expressions. Describing Syntax using BNF Grammar Backus-Naur Form (BNF) is a formal notation used to describe the syntax of programming languages. Example BNF Grammar: <Statement> ::= <AssignmentSt> | <ForST> | '{' <StatementList> '}' | \ud835\udf16 <AssignmentSt> ::= <Id> '=' <Exper> <ForST> ::= 'for' <Id> '=' <Expr> 'to' <Expr> <DoPart> <DoPart> ::= 'do' <Statement> <StatementList> ::= <Statement> ';' <StatementList> | <Statement> Compilation Steps Compiling a program involves several steps, and it's crucial to understand the process. Intermediate Representations (IRs): Compiler uses different program Intermediate Representations. These IRs facilitate necessary program manipulations (analysis, optimization, code generation). Example Compilation Steps in C++: #include <iostream> int main() { int z = 0; std::cout << \"z+1=\" << z++ << std::endl; return 0; } Compiler Phases Compiler Architecture: Parser: Responsible for analyzing the syntactic structure of the source code and generating a parse tree or abstract syntax tree (AST). Scanner: Tokenizes the source code, breaking it down into a sequence of tokens for the parser to analyze. Type Checker: Ensures that the types of expressions and statements are consistent and adherent to the language's rules. Code Generators: Translate the intermediate representation (IR) or AST into machine code or another target language. Error Manager: Handles and reports errors that occur during the compilation process. Note: The order of these components may vary depending on the specific compiler design Note: The parser acts as a \"driver\" in the compiler design, meaning it plays a central role in coordinating the compilation process. Note: It analyzes the syntax of the source code, constructs a hierarchical representation (such as a parse tree), and passes this structure to subsequent stages of the compiler. Steps in System Context: Preprocessing: Expanding macros and collecting program sources. Compilation: Parsing the preprocessed source code and generating an intermediate representation. Linking: Joining together object files and resolving external references. Loading: Mapping virtual addresses to physical address space. Compile Passes Definition: A compile pass is a stage in the compilation process where the source program undergoes transformations specified by its phases, producing intermediate output. Single-pass Compiler: Scans the complete source code only once. Example: Pascal compiler. Multi-pass Compiler: Processes the source code multiple times, improving the code pass by pass until the final pass emits the final code. Compiler Errors A good compiler assists the programmer in locating and tracking down errors: Compile-time Errors: Occur during program compilation. Lexical errors Syntactic errors Semantic errors Run-time Errors: Occur while the program is running after being successfully compiled. Crashes Logical errors Types of Errors in Compilers Common programming errors can occur at various levels: Lexical Errors: Misspellings of identifiers, keywords, or operators. Syntactic Errors: Misplaced semicolons, extra or missing braces. Semantic Errors: Type mismatches between operators and operands. Logical Errors: Incorrect reasoning by the programmer. Error Recovery Definition: Error recovery is a process that takes action against errors to reduce negative effects as much as possible. Common error-recovery strategies for parser error handling: Panic-mode Phrase-level Error-productions Global-correction Compiler Types Decompiler: Translates from a low-level language to a higher-level one. Cross-compiler: Produces code for a different CPU or operating system. Transpiler (Source-to-Source Compiler): Translates between high-level languages. Bootstrap Compiler: Temporary compiler used for compiling a more permanent or better-optimized compiler. Compiler-compiler: Produces a compiler (or part of one) in a generic and reusable way (e.g., ANTLR , FLEX , YACC , BISON ). reminder Interpreter An interpreter is another computer program like compiler that executes instructions written in a programming language immediately statement by statement. Just-In-Time (JIT) Compilation Definition The JIT compiler, or dynamic translator, reads bytecodes (in a bytecode-compiled system) in many sections (or in full, rarely) and compiles them dynamically into machine code. This can be done per-file, per-function, or even on any arbitrary code fragment. As a result, the program can run faster. Provides lazy/late compiling. The code can be compiled when it is about to be executed and then cached and reused later without needing to be recompiled. Advantages of JIT Compilation Improved Performance: JIT compilation can result in faster execution of the program since machine code is generated dynamically. Lazy Compilation: Compilation is deferred until the code is about to be executed. This allows for better optimization decisions based on runtime information. Caching and Reuse: Once code is compiled, it can be cached and reused in subsequent executions, reducing the need for repeated compilation. Adaptability: JIT compilation allows for adaptability to the runtime environment, optimizing the code based on the specific characteristics of the executing system. Just-In-Time (JIT) compilation is a technique employed in the field of compiler design to enhance the runtime performance of programs. Unlike traditional ahead-of-time (AOT) compilation, where source code is translated into machine code before execution, JIT compilation occurs dynamically during program execution. This approach combines elements of interpretation and compilation, seeking to leverage the advantages of both. In JIT compilation, the source code is initially translated into an intermediate representation, often referred to as bytecode or an intermediate language. This intermediate code is not directly executed by the hardware but is designed to be more portable and platform-independent than the original source code. During runtime, as the program is executed, the JIT compiler analyzes the intermediate code and translates it into machine code that is specific to the underlying hardware architecture. This process occurs on-the-fly, just before the corresponding code is executed, hence the term \"just-in-time.\" The generated machine code is then executed directly by the hardware, resulting in potentially improved performance compared to interpreting the original source or intermediate code. Use Cases Bytecode-Compiled Systems: Commonly used in virtual machines that execute bytecode, such as Java Virtual Machine (JVM) or Common Language Runtime (CLR) in .NET. Execution Speed Optimization: JIT compilation is employed to improve the execution speed of programs, especially in environments where interpretation of high-level code would be too slow. Memory Efficiency: By compiling only the necessary portions of code during runtime, JIT compilation can contribute to more efficient memory usage.","title":"Unit 1 Compiler basics"},{"location":"lectures/01_Compiler-Basic/#compiler-basic","text":"","title":" Compiler Basic"},{"location":"lectures/01_Compiler-Basic/#_1","text":"","title":""},{"location":"lectures/01_Compiler-Basic/#compiler-formalization","text":"In compiler design, it's essential to precisely describe the input (source code) and output (machine code) of a compiler. Input: Source code (programming language) Output: Assembly (machine) code","title":"Compiler Formalization"},{"location":"lectures/01_Compiler-Basic/#describing-programming-language-syntax","text":"Programming languages have two crucial components: syntax and semantics.","title":"Describing Programming Language Syntax"},{"location":"lectures/01_Compiler-Basic/#syntax","text":"Syntax defines which strings of symbols are valid expressions in the language.","title":"Syntax"},{"location":"lectures/01_Compiler-Basic/#example-c","text":"int z = 0; // Valid statement std::cout << \"z+1=\" << z++ << std::endl; // Valid statement","title":"Example (C++):"},{"location":"lectures/01_Compiler-Basic/#semantics","text":"Semantics defines the meaning or behavior of valid expressions.","title":"Semantics"},{"location":"lectures/01_Compiler-Basic/#describing-syntax-using-bnf-grammar","text":"Backus-Naur Form (BNF) is a formal notation used to describe the syntax of programming languages.","title":"Describing Syntax using BNF Grammar"},{"location":"lectures/01_Compiler-Basic/#example-bnf-grammar","text":"<Statement> ::= <AssignmentSt> | <ForST> | '{' <StatementList> '}' | \ud835\udf16 <AssignmentSt> ::= <Id> '=' <Exper> <ForST> ::= 'for' <Id> '=' <Expr> 'to' <Expr> <DoPart> <DoPart> ::= 'do' <Statement> <StatementList> ::= <Statement> ';' <StatementList> | <Statement>","title":"Example BNF Grammar:"},{"location":"lectures/01_Compiler-Basic/#compilation-steps","text":"Compiling a program involves several steps, and it's crucial to understand the process. Intermediate Representations (IRs): Compiler uses different program Intermediate Representations. These IRs facilitate necessary program manipulations (analysis, optimization, code generation).","title":"Compilation Steps"},{"location":"lectures/01_Compiler-Basic/#example-compilation-steps-in-c","text":"#include <iostream> int main() { int z = 0; std::cout << \"z+1=\" << z++ << std::endl; return 0; }","title":"Example Compilation Steps in C++:"},{"location":"lectures/01_Compiler-Basic/#compiler-phases","text":"","title":"Compiler Phases"},{"location":"lectures/01_Compiler-Basic/#_2","text":"","title":""},{"location":"lectures/01_Compiler-Basic/#compiler-architecture","text":"Parser: Responsible for analyzing the syntactic structure of the source code and generating a parse tree or abstract syntax tree (AST). Scanner: Tokenizes the source code, breaking it down into a sequence of tokens for the parser to analyze. Type Checker: Ensures that the types of expressions and statements are consistent and adherent to the language's rules. Code Generators: Translate the intermediate representation (IR) or AST into machine code or another target language. Error Manager: Handles and reports errors that occur during the compilation process. Note: The order of these components may vary depending on the specific compiler design Note: The parser acts as a \"driver\" in the compiler design, meaning it plays a central role in coordinating the compilation process. Note: It analyzes the syntax of the source code, constructs a hierarchical representation (such as a parse tree), and passes this structure to subsequent stages of the compiler.","title":"Compiler Architecture:"},{"location":"lectures/01_Compiler-Basic/#steps-in-system-context","text":"Preprocessing: Expanding macros and collecting program sources. Compilation: Parsing the preprocessed source code and generating an intermediate representation. Linking: Joining together object files and resolving external references. Loading: Mapping virtual addresses to physical address space.","title":"Steps in System Context:"},{"location":"lectures/01_Compiler-Basic/#compile-passes","text":"Definition: A compile pass is a stage in the compilation process where the source program undergoes transformations specified by its phases, producing intermediate output. Single-pass Compiler: Scans the complete source code only once. Example: Pascal compiler. Multi-pass Compiler: Processes the source code multiple times, improving the code pass by pass until the final pass emits the final code.","title":"Compile Passes"},{"location":"lectures/01_Compiler-Basic/#compiler-errors","text":"A good compiler assists the programmer in locating and tracking down errors: Compile-time Errors: Occur during program compilation. Lexical errors Syntactic errors Semantic errors Run-time Errors: Occur while the program is running after being successfully compiled. Crashes Logical errors","title":"Compiler Errors"},{"location":"lectures/01_Compiler-Basic/#types-of-errors-in-compilers","text":"Common programming errors can occur at various levels: Lexical Errors: Misspellings of identifiers, keywords, or operators. Syntactic Errors: Misplaced semicolons, extra or missing braces. Semantic Errors: Type mismatches between operators and operands. Logical Errors: Incorrect reasoning by the programmer.","title":"Types of Errors in Compilers"},{"location":"lectures/01_Compiler-Basic/#error-recovery","text":"Definition: Error recovery is a process that takes action against errors to reduce negative effects as much as possible. Common error-recovery strategies for parser error handling: Panic-mode Phrase-level Error-productions Global-correction","title":"Error Recovery"},{"location":"lectures/01_Compiler-Basic/#compiler-types","text":"Decompiler: Translates from a low-level language to a higher-level one. Cross-compiler: Produces code for a different CPU or operating system. Transpiler (Source-to-Source Compiler): Translates between high-level languages. Bootstrap Compiler: Temporary compiler used for compiling a more permanent or better-optimized compiler. Compiler-compiler: Produces a compiler (or part of one) in a generic and reusable way (e.g., ANTLR , FLEX , YACC , BISON ).","title":"Compiler Types"},{"location":"lectures/01_Compiler-Basic/#reminder-interpreter","text":"An interpreter is another computer program like compiler that executes instructions written in a programming language immediately statement by statement.","title":"reminder Interpreter"},{"location":"lectures/01_Compiler-Basic/#just-in-time-jit-compilation","text":"","title":"Just-In-Time (JIT) Compilation"},{"location":"lectures/01_Compiler-Basic/#definition","text":"The JIT compiler, or dynamic translator, reads bytecodes (in a bytecode-compiled system) in many sections (or in full, rarely) and compiles them dynamically into machine code. This can be done per-file, per-function, or even on any arbitrary code fragment. As a result, the program can run faster. Provides lazy/late compiling. The code can be compiled when it is about to be executed and then cached and reused later without needing to be recompiled.","title":"Definition"},{"location":"lectures/01_Compiler-Basic/#advantages-of-jit-compilation","text":"Improved Performance: JIT compilation can result in faster execution of the program since machine code is generated dynamically. Lazy Compilation: Compilation is deferred until the code is about to be executed. This allows for better optimization decisions based on runtime information. Caching and Reuse: Once code is compiled, it can be cached and reused in subsequent executions, reducing the need for repeated compilation. Adaptability: JIT compilation allows for adaptability to the runtime environment, optimizing the code based on the specific characteristics of the executing system.","title":"Advantages of JIT Compilation"},{"location":"lectures/01_Compiler-Basic/#_3","text":"Just-In-Time (JIT) compilation is a technique employed in the field of compiler design to enhance the runtime performance of programs. Unlike traditional ahead-of-time (AOT) compilation, where source code is translated into machine code before execution, JIT compilation occurs dynamically during program execution. This approach combines elements of interpretation and compilation, seeking to leverage the advantages of both. In JIT compilation, the source code is initially translated into an intermediate representation, often referred to as bytecode or an intermediate language. This intermediate code is not directly executed by the hardware but is designed to be more portable and platform-independent than the original source code. During runtime, as the program is executed, the JIT compiler analyzes the intermediate code and translates it into machine code that is specific to the underlying hardware architecture. This process occurs on-the-fly, just before the corresponding code is executed, hence the term \"just-in-time.\" The generated machine code is then executed directly by the hardware, resulting in potentially improved performance compared to interpreting the original source or intermediate code.","title":""},{"location":"lectures/01_Compiler-Basic/#use-cases","text":"Bytecode-Compiled Systems: Commonly used in virtual machines that execute bytecode, such as Java Virtual Machine (JVM) or Common Language Runtime (CLR) in .NET. Execution Speed Optimization: JIT compilation is employed to improve the execution speed of programs, especially in environments where interpretation of high-level code would be too slow. Memory Efficiency: By compiling only the necessary portions of code during runtime, JIT compilation can contribute to more efficient memory usage.","title":"Use Cases"},{"location":"lectures/02_Bootstrapping/","text":"Bootstrapping Definition: Bootstrapping is a technique where a simple program initiates a more complex system of programs. In the context of compilers, it often involves the process of writing a compiler for a programming language using another compiler written in the same language. This self-compiling process allows for the creation of more sophisticated compilers. Example: Consider the BIOS (Basic Input/Output System), which initializes and tests hardware, peripherals, and external memory devices when a computer boots. In compiler design, bootstrapping might involve writing a compiler for a language A using a compiler written in the same language A . T-Diagrams: Definition: T-diagrams are a graphical notation used to represent the relationships between different programming languages in the context of compiler design. The notation takes the form of A ---(C)---> B , where a compiler written in language C processes source code in language A and produces executable code in language B . Full Bootstrapping: Goal: The goal of full bootstrapping is to implement a compiler for language X on a machine M , given the presence of a language C compiler/assembler on the same machine. Steps: Write Compiler for Subset X in C : Begin by writing a compiler for a small part (subset) of language X using the language C . Compile this on machine M , where the C compiler already exists. Compile Compiler for Sub X : Use the existing language C compiler to compile the compiler written in Step 1 . This creates a compiler that can process the subset language Sub X . Translate Subset Compiler (Sub X): Translate the subset compiler (Sub X) written in Step 1 into the subset language (Sub X). This is a necessary step in the process. Compile Subset Compiler (Sub X): Compile the subset compiler (Sub X) from Step 3 using the compiler built in Step 2 . This further refines the compiler for Sub X . Extend Subset Language (Sub X): Extend the subset language (Sub X) compiler from Step 3 into a compiler for the full language X . Importantly, this step is still performed using only the subset language Sub X . Compile Full Language Compiler: Finally, compile the full language X compiler using the compiler built in Step 4 . At this point, you have a compiler for the full language X implemented on machine M . This comprehensive process demonstrates the self-sustaining nature of bootstrapping, where each step builds upon the capabilities of the previous one, ultimately leading to the creation of a compiler for a more complex language. Simple Bootstrapping: Process: If a compiler or interpreter already exists for the language A on the target machine M , the process of bootstrapping is simplified.","title":"Unit 2 Bootstrapping"},{"location":"lectures/02_Bootstrapping/#bootstrapping","text":"","title":" Bootstrapping"},{"location":"lectures/02_Bootstrapping/#_1","text":"Definition: Bootstrapping is a technique where a simple program initiates a more complex system of programs. In the context of compilers, it often involves the process of writing a compiler for a programming language using another compiler written in the same language. This self-compiling process allows for the creation of more sophisticated compilers. Example: Consider the BIOS (Basic Input/Output System), which initializes and tests hardware, peripherals, and external memory devices when a computer boots. In compiler design, bootstrapping might involve writing a compiler for a language A using a compiler written in the same language A .","title":""},{"location":"lectures/02_Bootstrapping/#t-diagrams","text":"Definition: T-diagrams are a graphical notation used to represent the relationships between different programming languages in the context of compiler design. The notation takes the form of A ---(C)---> B , where a compiler written in language C processes source code in language A and produces executable code in language B .","title":"T-Diagrams:"},{"location":"lectures/02_Bootstrapping/#full-bootstrapping","text":"Goal: The goal of full bootstrapping is to implement a compiler for language X on a machine M , given the presence of a language C compiler/assembler on the same machine. Steps: Write Compiler for Subset X in C : Begin by writing a compiler for a small part (subset) of language X using the language C . Compile this on machine M , where the C compiler already exists.","title":"Full Bootstrapping:"},{"location":"lectures/02_Bootstrapping/#_2","text":"Compile Compiler for Sub X : Use the existing language C compiler to compile the compiler written in Step 1 . This creates a compiler that can process the subset language Sub X .","title":""},{"location":"lectures/02_Bootstrapping/#_3","text":"Translate Subset Compiler (Sub X): Translate the subset compiler (Sub X) written in Step 1 into the subset language (Sub X). This is a necessary step in the process.","title":""},{"location":"lectures/02_Bootstrapping/#_4","text":"Compile Subset Compiler (Sub X): Compile the subset compiler (Sub X) from Step 3 using the compiler built in Step 2 . This further refines the compiler for Sub X .","title":""},{"location":"lectures/02_Bootstrapping/#_5","text":"Extend Subset Language (Sub X): Extend the subset language (Sub X) compiler from Step 3 into a compiler for the full language X . Importantly, this step is still performed using only the subset language Sub X .","title":""},{"location":"lectures/02_Bootstrapping/#_6","text":"Compile Full Language Compiler: Finally, compile the full language X compiler using the compiler built in Step 4 . At this point, you have a compiler for the full language X implemented on machine M .","title":""},{"location":"lectures/02_Bootstrapping/#_7","text":"This comprehensive process demonstrates the self-sustaining nature of bootstrapping, where each step builds upon the capabilities of the previous one, ultimately leading to the creation of a compiler for a more complex language.","title":""},{"location":"lectures/02_Bootstrapping/#simple-bootstrapping","text":"Process: If a compiler or interpreter already exists for the language A on the target machine M , the process of bootstrapping is simplified.","title":"Simple Bootstrapping:"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/","text":"Formal Languages and Lexical Analysis Lexical Analysis Goal Goal: The primary objective of lexical analysis is to partition an input string into meaningful elements called tokens. Tasks of Lexical Analyzer: 1. Recognize substrings corresponding to tokens. 2. Return tokens with their categories. Main Tasks: - Read input characters of the source program. - Group them into lexemes. - Produce, as output, a sequence of tokens for each lexeme in the source program. Output: - The output of lexical analysis is a stream of tokens, which serves as the input to the parser. Formal Languages Definition: - A language over $ \u03a3 $ is a subset of $ \u03a3 $ (set of all words over $ \u03a3 $ ). - Alphabet $ \u03a3: A $ finite set of elements. - For the lexer: Characters. - For the parser: Token classes/symbol types. - Words (strings): Sequences of elements from the alphabet \u03a3. - Example: If $ \u03a3 = {\ud835\udc4e, \ud835\udc4f} $, then $ \u03a3 = {\\epsilon, \ud835\udc4e, \ud835\udc4f, \ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4f, \ud835\udc4f\ud835\udc4e, \ud835\udc4f\ud835\udc4f, \ud835\udc4e\ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e, ...} $ - Example of an infinite language over $\u03a3: \ud835\udc3f1 = {\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, ...} = {{(\ud835\udc4e\ud835\udc4f)}^\ud835\udc5b | \ud835\udc5b \u2265 1} $ Formal Languages Description Notations Sets: $ \ud835\udc3f1 = { {}\ud835\udc4e^{\ud835\udc5b} \ud835\udc4f | \ud835\udc5b \u2265 0 } $ Grammars: <\ud835\udc34> ::= \ud835\udc4e\ud835\udc34 | \ud835\udc4f Automata: Regular Expressions (Regex): Used only for regular languages. Example: \ud835\udc4e* Formal Grammars Definition: - \ud835\udc3a = {\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46}, where - \ud835\udc41: A finite set \ud835\udc41 of nonterminal symbols, disjoint from the strings formed from \ud835\udc3a. - \u03a3: A finite set of terminal symbols, disjoint from \ud835\udc41. - \ud835\udc43: A finite set \ud835\udc43 of production rules, each rule of the form \ud835\udefc \u2192 \ud835\udefd. - \ud835\udc46: A distinguished symbol \ud835\udc46 \u2208 \ud835\udc41, the start symbol. Convention: - Use small letters for terminals and capital letters for non-terminals or variables when writing grammar production rules. Regular Grammars Definition: - A grammar \ud835\udc3a = (\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46) is right-linear if all productions are of the form: - \ud835\udc34 \u2192 \ud835\udc65\ud835\udc35 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3* Definition: - A grammar is left-linear if all productions are of the form: - \ud835\udc34 \u2192 \ud835\udc35\ud835\udc65 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3* Regular Grammar: - A regular grammar is one that is either right-linear or left-linear. Deterministic Finite Acceptor (DFA) Definition: A deterministic finite acceptor or DFA is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - $( Q )$ is a finite set of internal states, - $( \\Sigma )$ is a finite set of symbols called the input alphabet, - $( \\delta: Q \\times \\Sigma \\rightarrow Q )$ is a total function called the transition function, - $( q_0 \\in Q )$ is the initial state, - $( F \\subseteq Q )$ is a set of final states. Non-deterministic Finite Acceptor (NFA) Definition: A non-deterministic finite acceptor or NFA is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - $( Q )$ is a finite set of internal states, - $( \\Sigma )$ is a finite set of symbols called the input alphabet, - $( \\delta: Q \\times (\\Sigma \\cup {\\varepsilon}) \\rightarrow 2^Q )$ is a total function called the transition function, - $( q_0 \\in Q )$ is the initial state, - $( F \\subseteq Q )$ is a set of final states. NFA vs. DFA DFA Transition Function: $( \\delta: Q \\times \\Sigma \\rightarrow Q )$ NFA Transition Function: $( \\delta: Q \\times (\\Sigma \\cup {\\varepsilon}) \\rightarrow 2^Q )$ NFA can have multiple transitions for one input in a given state. NFA can have no transition for an input in a given state. NFA can make a transition without consuming an input symbol (\u03bb or \u03b5-transition). Computations of a DFA For each input string, there is exactly one path in a DFA (O(n)). $$ [ L(M) = { w \\in \\Sigma^ : \\delta^ (q_0, w) \\in F } ] $$ Computations of an NFA and Language Acceptance For an input string, there are multiple possible computation paths in an NFA $(O(2^n))$. $$[ L(M) = { w \\in \\Sigma^ : \\delta^ (q_0, w) \\cap F = \\emptyset } ] $$ NFA vs. DFA Implementation DFA s are generally simpler to implement due to their deterministic nature. NFA s may require additional mechanisms to handle non-deterministic transitions. Simulation of NFA requires tracking multiple possible states simultaneously.","title":"Unit 3 Lexical analysis"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#formal-languages-and-lexical-analysis","text":"","title":" Formal Languages and Lexical Analysis"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#_1","text":"","title":""},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#lexical-analysis-goal","text":"Goal: The primary objective of lexical analysis is to partition an input string into meaningful elements called tokens. Tasks of Lexical Analyzer: 1. Recognize substrings corresponding to tokens. 2. Return tokens with their categories. Main Tasks: - Read input characters of the source program. - Group them into lexemes. - Produce, as output, a sequence of tokens for each lexeme in the source program. Output: - The output of lexical analysis is a stream of tokens, which serves as the input to the parser.","title":"Lexical Analysis Goal"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#formal-languages","text":"Definition: - A language over $ \u03a3 $ is a subset of $ \u03a3 $ (set of all words over $ \u03a3 $ ). - Alphabet $ \u03a3: A $ finite set of elements. - For the lexer: Characters. - For the parser: Token classes/symbol types. - Words (strings): Sequences of elements from the alphabet \u03a3. - Example: If $ \u03a3 = {\ud835\udc4e, \ud835\udc4f} $, then $ \u03a3 = {\\epsilon, \ud835\udc4e, \ud835\udc4f, \ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4f, \ud835\udc4f\ud835\udc4e, \ud835\udc4f\ud835\udc4f, \ud835\udc4e\ud835\udc4e\ud835\udc4e, \ud835\udc4e\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e, ...} $ - Example of an infinite language over $\u03a3: \ud835\udc3f1 = {\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, \ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f\ud835\udc4e\ud835\udc4f, ...} = {{(\ud835\udc4e\ud835\udc4f)}^\ud835\udc5b | \ud835\udc5b \u2265 1} $","title":"Formal Languages"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#formal-languages-description-notations","text":"Sets: $ \ud835\udc3f1 = { {}\ud835\udc4e^{\ud835\udc5b} \ud835\udc4f | \ud835\udc5b \u2265 0 } $ Grammars: <\ud835\udc34> ::= \ud835\udc4e\ud835\udc34 | \ud835\udc4f Automata:","title":"Formal Languages Description Notations"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#_2","text":"Regular Expressions (Regex): Used only for regular languages. Example: \ud835\udc4e*","title":""},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#formal-grammars","text":"Definition: - \ud835\udc3a = {\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46}, where - \ud835\udc41: A finite set \ud835\udc41 of nonterminal symbols, disjoint from the strings formed from \ud835\udc3a. - \u03a3: A finite set of terminal symbols, disjoint from \ud835\udc41. - \ud835\udc43: A finite set \ud835\udc43 of production rules, each rule of the form \ud835\udefc \u2192 \ud835\udefd. - \ud835\udc46: A distinguished symbol \ud835\udc46 \u2208 \ud835\udc41, the start symbol. Convention: - Use small letters for terminals and capital letters for non-terminals or variables when writing grammar production rules.","title":"Formal Grammars"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#regular-grammars","text":"Definition: - A grammar \ud835\udc3a = (\ud835\udc41, \u03a3, \ud835\udc43, \ud835\udc46) is right-linear if all productions are of the form: - \ud835\udc34 \u2192 \ud835\udc65\ud835\udc35 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3* Definition: - A grammar is left-linear if all productions are of the form: - \ud835\udc34 \u2192 \ud835\udc35\ud835\udc65 | \ud835\udc65 | \ud835\udf16, where \ud835\udc34, \ud835\udc35 \u2208 \ud835\udc41 and \ud835\udc65 \u2208 \u03a3* Regular Grammar: - A regular grammar is one that is either right-linear or left-linear.","title":"Regular Grammars"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#deterministic-finite-acceptor-dfa","text":"Definition: A deterministic finite acceptor or DFA is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - $( Q )$ is a finite set of internal states, - $( \\Sigma )$ is a finite set of symbols called the input alphabet, - $( \\delta: Q \\times \\Sigma \\rightarrow Q )$ is a total function called the transition function, - $( q_0 \\in Q )$ is the initial state, - $( F \\subseteq Q )$ is a set of final states.","title":"Deterministic Finite Acceptor (DFA)"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#non-deterministic-finite-acceptor-nfa","text":"Definition: A non-deterministic finite acceptor or NFA is defined by the quintuple $$ [ M = (Q, \\Sigma, \\delta, q_0, F) ] $$ where - $( Q )$ is a finite set of internal states, - $( \\Sigma )$ is a finite set of symbols called the input alphabet, - $( \\delta: Q \\times (\\Sigma \\cup {\\varepsilon}) \\rightarrow 2^Q )$ is a total function called the transition function, - $( q_0 \\in Q )$ is the initial state, - $( F \\subseteq Q )$ is a set of final states.","title":"Non-deterministic Finite Acceptor (NFA)"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#nfa-vs-dfa","text":"DFA Transition Function: $( \\delta: Q \\times \\Sigma \\rightarrow Q )$ NFA Transition Function: $( \\delta: Q \\times (\\Sigma \\cup {\\varepsilon}) \\rightarrow 2^Q )$ NFA can have multiple transitions for one input in a given state. NFA can have no transition for an input in a given state. NFA can make a transition without consuming an input symbol (\u03bb or \u03b5-transition).","title":"NFA vs. DFA"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#computations-of-a-dfa","text":"For each input string, there is exactly one path in a DFA (O(n)). $$ [ L(M) = { w \\in \\Sigma^ : \\delta^ (q_0, w) \\in F } ] $$","title":"Computations of a DFA"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#computations-of-an-nfa-and-language-acceptance","text":"For an input string, there are multiple possible computation paths in an NFA $(O(2^n))$. $$[ L(M) = { w \\in \\Sigma^ : \\delta^ (q_0, w) \\cap F = \\emptyset } ] $$","title":"Computations of an NFA and Language Acceptance"},{"location":"lectures/03_Regular-Expressions-and-Lexical-Analysis/#nfa-vs-dfa-implementation","text":"DFA s are generally simpler to implement due to their deterministic nature. NFA s may require additional mechanisms to handle non-deterministic transitions. Simulation of NFA requires tracking multiple possible states simultaneously.","title":"NFA vs. DFA Implementation"},{"location":"lectures/04_Manual_Construction_of_Lexers/","text":"Manual Construction of Lexers Recognition of Tokens The manual construction of a lexical analyzer involves several steps: Describe Lexical Patterns: Define regular expressions (RE) to describe the lexical pattern of each token type. Construct NFAs: Create Non-deterministic Finite Automata (NFAs) for each regular expression. Convert NFAs to DFAs: Convert the NFAs to Deterministic Finite Automata (DFAs) for efficiency. Minimize DFA States: Minimize the number of states in the DFAs where possible. Construct Transition Diagrams: Build lexical analyzer transition diagrams from the DFAs. Implement Transition Diagrams: Translate the transition diagrams into actual code for the lexical analyzer. Transition Diagrams: Notations As an intermediate step, patterns are converted into stylized flowcharts called \"transition diagrams.\" These diagrams incorporate DFAs for recognizing tokens. If it's necessary to retract the forward pointer one position (i.e., the lexeme doesn't include the symbol that got us to the accepting state), a '*' is placed near that accepting state. Transition Diagram Examples: Relational Operations (RELOPs): Diagram for recognizing relational operators like < , <> , = , >= , <= , == , etc. Reserved Words and Identifiers: Diagram for recognizing reserved words and identifiers in the source code. Unsigned Numbers: Diagram for recognizing unsigned numerical values. Lexer Input and Output: The lexical analyzer takes the source code as input and produces a stream of tokens as output. This token stream is then passed to the parser for further syntactic analysis. Static Scope and Block Structure: The scope of a declaration is implicitly determined by where it appears in the program. Code blocks group declarations and statements, often delimited by braces {} or keywords like begin and end . Static scope and block structure in C++ White Spaces: Whitespaces are defined as tokens using space characters (' '), tabs ('\\t), and end-of-line characters ('\\r', '\\n'). In most languages, whitespaces and comments can occur between any two tokens and are generally ignored by the parser. Comments: Comments are detected and discarded by the lexer. They can be single-line or multi-line. Lexical analyzers always find the next non-whitespace, non-comment token. Lexical Errors and Error Recovery: Lexical errors occur when no token pattern matches the remaining input. A \"panic mode\" recovery strategy involves deleting characters until a well-formed token is found. Other recovery actions include deleting, inserting, replacing, or transposing characters. Panic Mode Recovery Panic mode recovery is one of the error recovery strategies used in compiler design. It is commonly used by most parsing methods. In this strategy, when an error is discovered, the parser discards input symbols one at a time until it finds a designated set of synchronizing tokens. These tokens are delimiters such as semicolons or ends, which indicate the end of an input statement. Here is a simple example of how panic mode recovery works: int a, 5abcd, sum, $2; In this case, the parser would discard the input symbols one at a time until it finds a synchronizing token (like a semicolon). However, this strategy may lead to semantic or runtime errors in further stages. The panic mode recovery process can be implemented in a high-level parsing function. This function is responsible for detecting parsing errors and re-synchronizing the input stream by skipping tokens until a suitable spot to resume parsing is found. For a grammar that ends statements with semicolons, the semicolon becomes the synchronizing token. Here is an example of a top-level parsing function that uses panic mode recovery: static int doParsing(void){ initialize errorcounter to zero WHILE TYPEOFTOKEN is not EOF DO SWITCH TYPEOFTOKEN CASE ID: -- ID is in the FIRST set of assignment() returnStatus = assignment() break CASE PRINT: -- PRINT is in the FIRST set of print() returnStatus = print() break CASE ... -- Other cases can go here, for other statement types break DEFAULT: eprintf(\"File %s Line %ld: Expecting %s or %s;\" \" found: %s '%s'\", filename, LINENUMBER, tokenType(ID), tokenType(PRINT), tokenType(TYPEOFTOKEN), LEXEMESTR ); returnStatus = FALSE break END SWITCH IF returnStatus is FALSE THEN CALL panic() increment errorcounter ENDIF END WHILE return errorcounter } In this example, each parsing function is a Boolean function. Each parsing function may succeed, in which case we continue parsing, or fail, in which case we stop parsing and return the failure indication to our parent function. Lexical Analysis Challenges: In some languages like Fortran, whitespace is insignificant, making lexical analysis challenging. Lookahead is required to distinguish between tokens, and language design should aim to minimize lookahead. Lookahead: Lookahead is necessary to decide where one token ends and the next begins. It is required to disambiguate between similar constructs (e.g., == and = ). Some languages, like PL/1, where keywords are not reserved, may require more extensive lookahead for lexical analysis.","title":"Unit 4 Manual constructions of lexers"},{"location":"lectures/04_Manual_Construction_of_Lexers/#manual-construction-of-lexers","text":"","title":" Manual Construction of Lexers"},{"location":"lectures/04_Manual_Construction_of_Lexers/#_1","text":"","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#recognition-of-tokens","text":"The manual construction of a lexical analyzer involves several steps: Describe Lexical Patterns: Define regular expressions (RE) to describe the lexical pattern of each token type. Construct NFAs: Create Non-deterministic Finite Automata (NFAs) for each regular expression. Convert NFAs to DFAs: Convert the NFAs to Deterministic Finite Automata (DFAs) for efficiency. Minimize DFA States: Minimize the number of states in the DFAs where possible. Construct Transition Diagrams: Build lexical analyzer transition diagrams from the DFAs. Implement Transition Diagrams: Translate the transition diagrams into actual code for the lexical analyzer.","title":"Recognition of Tokens"},{"location":"lectures/04_Manual_Construction_of_Lexers/#transition-diagrams-notations","text":"As an intermediate step, patterns are converted into stylized flowcharts called \"transition diagrams.\" These diagrams incorporate DFAs for recognizing tokens. If it's necessary to retract the forward pointer one position (i.e., the lexeme doesn't include the symbol that got us to the accepting state), a '*' is placed near that accepting state.","title":"Transition Diagrams: Notations"},{"location":"lectures/04_Manual_Construction_of_Lexers/#transition-diagram-examples","text":"Relational Operations (RELOPs): Diagram for recognizing relational operators like < , <> , = , >= , <= , == , etc.","title":"Transition Diagram Examples:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#_2","text":"Reserved Words and Identifiers: Diagram for recognizing reserved words and identifiers in the source code.","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#_3","text":"Unsigned Numbers: Diagram for recognizing unsigned numerical values.","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#_4","text":"","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#lexer-input-and-output","text":"The lexical analyzer takes the source code as input and produces a stream of tokens as output. This token stream is then passed to the parser for further syntactic analysis.","title":"Lexer Input and Output:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#_5","text":"","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#static-scope-and-block-structure","text":"The scope of a declaration is implicitly determined by where it appears in the program. Code blocks group declarations and statements, often delimited by braces {} or keywords like begin and end .","title":"Static Scope and Block Structure:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#static-scope-and-block-structure-in-c","text":"","title":"Static scope and block structure in C++"},{"location":"lectures/04_Manual_Construction_of_Lexers/#_6","text":"","title":""},{"location":"lectures/04_Manual_Construction_of_Lexers/#white-spaces","text":"Whitespaces are defined as tokens using space characters (' '), tabs ('\\t), and end-of-line characters ('\\r', '\\n'). In most languages, whitespaces and comments can occur between any two tokens and are generally ignored by the parser.","title":"White Spaces:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#comments","text":"Comments are detected and discarded by the lexer. They can be single-line or multi-line. Lexical analyzers always find the next non-whitespace, non-comment token.","title":"Comments:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#lexical-errors-and-error-recovery","text":"Lexical errors occur when no token pattern matches the remaining input. A \"panic mode\" recovery strategy involves deleting characters until a well-formed token is found. Other recovery actions include deleting, inserting, replacing, or transposing characters.","title":"Lexical Errors and Error Recovery:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#panic-mode-recovery","text":"Panic mode recovery is one of the error recovery strategies used in compiler design. It is commonly used by most parsing methods. In this strategy, when an error is discovered, the parser discards input symbols one at a time until it finds a designated set of synchronizing tokens. These tokens are delimiters such as semicolons or ends, which indicate the end of an input statement. Here is a simple example of how panic mode recovery works: int a, 5abcd, sum, $2; In this case, the parser would discard the input symbols one at a time until it finds a synchronizing token (like a semicolon). However, this strategy may lead to semantic or runtime errors in further stages. The panic mode recovery process can be implemented in a high-level parsing function. This function is responsible for detecting parsing errors and re-synchronizing the input stream by skipping tokens until a suitable spot to resume parsing is found. For a grammar that ends statements with semicolons, the semicolon becomes the synchronizing token. Here is an example of a top-level parsing function that uses panic mode recovery: static int doParsing(void){ initialize errorcounter to zero WHILE TYPEOFTOKEN is not EOF DO SWITCH TYPEOFTOKEN CASE ID: -- ID is in the FIRST set of assignment() returnStatus = assignment() break CASE PRINT: -- PRINT is in the FIRST set of print() returnStatus = print() break CASE ... -- Other cases can go here, for other statement types break DEFAULT: eprintf(\"File %s Line %ld: Expecting %s or %s;\" \" found: %s '%s'\", filename, LINENUMBER, tokenType(ID), tokenType(PRINT), tokenType(TYPEOFTOKEN), LEXEMESTR ); returnStatus = FALSE break END SWITCH IF returnStatus is FALSE THEN CALL panic() increment errorcounter ENDIF END WHILE return errorcounter } In this example, each parsing function is a Boolean function. Each parsing function may succeed, in which case we continue parsing, or fail, in which case we stop parsing and return the failure indication to our parent function.","title":"Panic Mode Recovery"},{"location":"lectures/04_Manual_Construction_of_Lexers/#lexical-analysis-challenges","text":"In some languages like Fortran, whitespace is insignificant, making lexical analysis challenging. Lookahead is required to distinguish between tokens, and language design should aim to minimize lookahead.","title":"Lexical Analysis Challenges:"},{"location":"lectures/04_Manual_Construction_of_Lexers/#lookahead","text":"Lookahead is necessary to decide where one token ends and the next begins. It is required to disambiguate between similar constructs (e.g., == and = ). Some languages, like PL/1, where keywords are not reserved, may require more extensive lookahead for lexical analysis.","title":"Lookahead:"},{"location":"lectures/05_Automatic_construction_of_Lexers/","text":"Automatic construction of Lexers Lexer Construction Steps Input: Token Specifications - A list of regular expressions (RE) in priority order that define the patterns of tokens in a programming language. Output: Lexer - A program that reads an input stream and breaks it up into tokens based on the specified regular expressions. Algorithm: Convert REs into NFAs: Transform regular expressions into Non-deterministic Finite Automata (NFAs). Each regular expression corresponds to an NFA that recognizes the language defined by that expression. Convert NFAs to DFA: Convert NFAs to Deterministic Finite Automata (DFAs) for efficiency. Create a DFA that accepts the same language as the original NFA. Convert DFA into Transition Table: Create a transition table that represents the DFA. The table indicates the next state for each combination of current state and input symbol. The Lex/Flex Tool Flex, also known as Fast Lexical Analyzer Generator, is a tool used for generating lexical analyzers, also known as \"scanners\" or \"lexers\". It was written in C around 1987 by Vern Paxson and is often used in conjunction with other tools like Berkeley Yacc parser generator or GNU Bison parser generator. Structure of Lex Programs The structure of a lex program is as follows: {% manifest constants %} declarations %% translation rules %% auxiliary functions Manifest constants : These are enclosed in {} brackets and can include definitions for variables, regular definitions, and manifest constants. Declarations : These are the variable declarations that are used in the program. Translation rules : Each rule has the form: Pattern { Action } . Patterns are regular expressions, and actions are code fragments, typically in C. Auxiliary functions : These are additional functions that can be compiled separately and loaded with the lexical analyzer. Lexical Analysis Lexical analysis, or lexing, is the process of converting a sequence of characters into a sequence of tokens. The lexical analyzer takes in a stream of input characters and returns a stream of tokens. Running a Lex Program To run a lex program, you need to follow these steps: Write an input file that describes the lexical analyzer to be generated. This file should be named lex.l and written in the lex language. The lex compiler transforms lex.l into a C program, in a file that is always named lex.yy.c . Compile the lex.yy.c file into an executable file. This can be done using the C compiler. Run the executable file. The output file will take a stream of input characters and produce a stream of tokens. Here is an example of how to run a lex program: flex filename.l # or flex filename.lex depending on the extension file is saved with gcc lex.yy.c ./a.out Then provide the input to the program if it is required. Press Ctrl+D or use some rule to stop taking inputs from the user. Advantages and Disadvantages of Flex Flex has several advantages: Efficiency : Flex-generated lexical analyzers are very fast and efficient, which can improve the overall performance of the programming language. Portability : Flex is available on many different platforms, making it easy to use on a wide range of systems. Flexibility : Flex is very flexible and can be customized to support many different types of programming languages and input formats. Easy to Use : Flex is relatively easy to learn and use, especially for programmers with experience in regular expressions. However, there are also some disadvantages: Steep Learning Curve : While Flex is relatively easy to use, it can have a steep learning curve for programmers who are not familiar with regular expressions. Limited Error Detection : Flex-generated lexical analyzers can only detect certain types of errors, such as syntax errors and misspelled keywords. Limited Support for Unicode : Flex has limited support for Unicode, which can make it difficult to work with non-ASCII characters. Code Maintenance : Flex-generated code can be difficult to maintain over time, especially as the programming language evolves and changes. This can make it challenging to keep the lexical analyzer up to date with the latest version of the language. Lex Architecture Describes how Lex works in terms of token recognition. Lex takes a set of regular expressions and corresponding actions to create a lexer. Regular Expression to NFA Illustrates the process of converting a regular expression to a Non-deterministic Finite Automaton (NFA). Each construct in the regular expression corresponds to a state transition in the NFA. Regular Expressions (RE): A regular expression is a concise way to describe a set of strings. It consists of: - Alphabet: A set of symbols (characters). - Operators: Specify operations to combine and manipulate sets of strings. - Special Symbols: Representing operations like concatenation, union, and closure. Nondeterministic Finite Automaton (NFA): An NFA is a type of finite automaton that allows multiple transitions from a state on a given input symbol. It has states, transitions, and an initial and final state. Steps to Convert RE to NFA: Base Cases: Empty String (\ud835\udf16): Create an NFA with two states (initial and final) and an \ud835\udf16 transition between them. Single Symbol (a): Create an NFA with two states, one initial and one final, with a transition labeled by the symbol. Concatenation (AB): If RE is AB, create NFAs for A and B. Connect the final state of A to the initial state of B. Union (A | B): If RE is A | B, create NFAs for A and B. Create a new initial state with \ud835\udf16 transitions to the initial states of A and B. Create a new final state with \ud835\udf16 transitions from the final states of A and B. Kleene Closure (A*): If RE is A*, create an NFA for A. Add a new initial state with \ud835\udf16 transitions to the initial state of A and a \ud835\udf16 transition from the final state of A to the initial state of A. Example: Let's convert the regular expression (a|b)*abb to an NFA: Base Cases: a : NFA1 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (a)) b : NFA2 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (b)) \ud835\udf16: NFA\ud835\udf16 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (\ud835\udf16)) Concatenation (ab): Concatenate NFA1 and NFA2. Union (a|b): Create a new initial state with \ud835\udf16 transitions to the initial states of NFA1 and NFA2. Create a new final state with \ud835\udf16 transitions from the final states of NFA1 and NFA2. Kleene Closure ((a|b)*): Add a new initial state with \ud835\udf16 transitions to the initial state of the union NFA. Add a \ud835\udf16 transition from the final state of the union NFA to its initial state. Concatenation with abb : Concatenate the Kleene Closure NFA with the NFA for abb . The resulting NFA accepts the language described by the regular expression. Converting NFA to DFA Demonstrates the algorithm to convert NFAs to DFAs. Each set of possible states in the NFA becomes one state in the DFA, resulting in a more efficient representation. DFA Minimization DFA minimization is the process of converting a given Deterministic Finite Automaton (DFA) to an equivalent DFA with the minimum number of states. This process is also known as the optimization of DFA and uses a partitioning algorithm. The steps to minimize a DFA are as follows: Partitioning : Divide the set of states (Q) into two sets. One set will contain all final states and the other set will contain non-final states. This partition is called P0. Initialization : Initialize k = 1. Finding Pk : Find Pk by partitioning the different sets of Pk-1. In each set of Pk-1, take all possible pairs of states. If two states of a set are distinguishable, split the sets into different sets in Pk. Stopping Condition : Stop when Pk = Pk-1 (No change in partition). Merging States : All states of one set are merged into one. The number of states in the minimized DFA will be equal to the number of sets in Pk. Two states (qi, qj) are distinguishable in partition Pk if for any input symbol a, \u03b4(qi, a) and \u03b4(qj, a) are in different sets in partition Pk-1. Here's a pseudocode example of Hopcroft's algorithm, one of the algorithms for DFA minimization: P := {F, Q \\ F} W := {F, Q \\ F} while (W is not empty) do choose and remove a set A from W for each c in \u03a3 do let X be the set of states for which a transition on c leads to a state in A for each set Y in P for which X \u2229 Y is nonempty and Y \\ X is nonempty do replace Y in P by the two sets X \u2229 Y and Y \\ X if Y is in W replace Y in W by the same two sets else if |X \u2229 Y| <= |Y \\ X| add X \u2229 Y to W else add Y \\ X to W This algorithm starts with a partition that is too coarse: every pair of states that are equivalent according to the Nerode congruence belong to the same set in the partition, but pairs that are inequivalent might also belong to the same set. It gradually refines the partition into a larger number of smaller sets, at each step splitting sets of states into pairs of subsets that are necessarily inequivalent. Advantages of DFA minimization include reduced complexity, optimal space utilization, improved performance, and language equivalence. However, it also has some disadvantages such as increased computational complexity, additional design and analysis effort, loss of readability, and it's limited to deterministic automata. In Sammary: Intuition: - Two DFA states are equivalent if all subsequent behavior from those states is the same. Procedure: 1. Create a table of state pairs. 2. Mark cells where one state is final and the other is non-final. 3. Mark pairs where transitions on the same symbol lead to marked pairs. 4. Repeat step 3 until no unmarked pairs remain. 5. Merge unmarked states to achieve a minimized DFA. Resolving Ambiguities in Lexers Regular Expression Ambiguity: - Ambiguity arises when regular expressions can match input in multiple ways. Conflict Resolution in Lex: 1. Longest/Maximal Match Rule: - Prefer a longer prefix over a shorter one. 2. Declaration Priority: - If the longest prefix matches multiple patterns, prefer the one listed first in the Lex program. The Flex Manual An essential reference for using the Flex tool. It provides detailed information on the Flex tool and its capabilities. Refer to the provided appendix slide for in-depth insights.","title":"Unit 5 Automatic constructions of lexers"},{"location":"lectures/05_Automatic_construction_of_Lexers/#automatic-construction-of-lexers","text":"","title":" Automatic construction of Lexers"},{"location":"lectures/05_Automatic_construction_of_Lexers/#_1","text":"","title":""},{"location":"lectures/05_Automatic_construction_of_Lexers/#lexer-construction-steps","text":"Input: Token Specifications - A list of regular expressions (RE) in priority order that define the patterns of tokens in a programming language. Output: Lexer - A program that reads an input stream and breaks it up into tokens based on the specified regular expressions. Algorithm: Convert REs into NFAs: Transform regular expressions into Non-deterministic Finite Automata (NFAs). Each regular expression corresponds to an NFA that recognizes the language defined by that expression. Convert NFAs to DFA: Convert NFAs to Deterministic Finite Automata (DFAs) for efficiency. Create a DFA that accepts the same language as the original NFA. Convert DFA into Transition Table: Create a transition table that represents the DFA. The table indicates the next state for each combination of current state and input symbol.","title":"Lexer Construction Steps"},{"location":"lectures/05_Automatic_construction_of_Lexers/#the-lexflex-tool","text":"Flex, also known as Fast Lexical Analyzer Generator, is a tool used for generating lexical analyzers, also known as \"scanners\" or \"lexers\". It was written in C around 1987 by Vern Paxson and is often used in conjunction with other tools like Berkeley Yacc parser generator or GNU Bison parser generator.","title":"The Lex/Flex Tool"},{"location":"lectures/05_Automatic_construction_of_Lexers/#structure-of-lex-programs","text":"The structure of a lex program is as follows: {% manifest constants %} declarations %% translation rules %% auxiliary functions Manifest constants : These are enclosed in {} brackets and can include definitions for variables, regular definitions, and manifest constants. Declarations : These are the variable declarations that are used in the program. Translation rules : Each rule has the form: Pattern { Action } . Patterns are regular expressions, and actions are code fragments, typically in C. Auxiliary functions : These are additional functions that can be compiled separately and loaded with the lexical analyzer.","title":"Structure of Lex Programs"},{"location":"lectures/05_Automatic_construction_of_Lexers/#lexical-analysis","text":"Lexical analysis, or lexing, is the process of converting a sequence of characters into a sequence of tokens. The lexical analyzer takes in a stream of input characters and returns a stream of tokens.","title":"Lexical Analysis"},{"location":"lectures/05_Automatic_construction_of_Lexers/#running-a-lex-program","text":"To run a lex program, you need to follow these steps: Write an input file that describes the lexical analyzer to be generated. This file should be named lex.l and written in the lex language. The lex compiler transforms lex.l into a C program, in a file that is always named lex.yy.c . Compile the lex.yy.c file into an executable file. This can be done using the C compiler. Run the executable file. The output file will take a stream of input characters and produce a stream of tokens. Here is an example of how to run a lex program: flex filename.l # or flex filename.lex depending on the extension file is saved with gcc lex.yy.c ./a.out Then provide the input to the program if it is required. Press Ctrl+D or use some rule to stop taking inputs from the user.","title":"Running a Lex Program"},{"location":"lectures/05_Automatic_construction_of_Lexers/#advantages-and-disadvantages-of-flex","text":"Flex has several advantages: Efficiency : Flex-generated lexical analyzers are very fast and efficient, which can improve the overall performance of the programming language. Portability : Flex is available on many different platforms, making it easy to use on a wide range of systems. Flexibility : Flex is very flexible and can be customized to support many different types of programming languages and input formats. Easy to Use : Flex is relatively easy to learn and use, especially for programmers with experience in regular expressions. However, there are also some disadvantages: Steep Learning Curve : While Flex is relatively easy to use, it can have a steep learning curve for programmers who are not familiar with regular expressions. Limited Error Detection : Flex-generated lexical analyzers can only detect certain types of errors, such as syntax errors and misspelled keywords. Limited Support for Unicode : Flex has limited support for Unicode, which can make it difficult to work with non-ASCII characters. Code Maintenance : Flex-generated code can be difficult to maintain over time, especially as the programming language evolves and changes. This can make it challenging to keep the lexical analyzer up to date with the latest version of the language.","title":"Advantages and Disadvantages of Flex"},{"location":"lectures/05_Automatic_construction_of_Lexers/#_2","text":"","title":""},{"location":"lectures/05_Automatic_construction_of_Lexers/#lex-architecture","text":"Describes how Lex works in terms of token recognition. Lex takes a set of regular expressions and corresponding actions to create a lexer.","title":"Lex Architecture"},{"location":"lectures/05_Automatic_construction_of_Lexers/#_3","text":"","title":""},{"location":"lectures/05_Automatic_construction_of_Lexers/#regular-expression-to-nfa","text":"Illustrates the process of converting a regular expression to a Non-deterministic Finite Automaton (NFA). Each construct in the regular expression corresponds to a state transition in the NFA.","title":"Regular Expression to NFA"},{"location":"lectures/05_Automatic_construction_of_Lexers/#regular-expressions-re","text":"A regular expression is a concise way to describe a set of strings. It consists of: - Alphabet: A set of symbols (characters). - Operators: Specify operations to combine and manipulate sets of strings. - Special Symbols: Representing operations like concatenation, union, and closure.","title":"Regular Expressions (RE):"},{"location":"lectures/05_Automatic_construction_of_Lexers/#nondeterministic-finite-automaton-nfa","text":"An NFA is a type of finite automaton that allows multiple transitions from a state on a given input symbol. It has states, transitions, and an initial and final state.","title":"Nondeterministic Finite Automaton (NFA):"},{"location":"lectures/05_Automatic_construction_of_Lexers/#steps-to-convert-re-to-nfa","text":"Base Cases: Empty String (\ud835\udf16): Create an NFA with two states (initial and final) and an \ud835\udf16 transition between them. Single Symbol (a): Create an NFA with two states, one initial and one final, with a transition labeled by the symbol. Concatenation (AB): If RE is AB, create NFAs for A and B. Connect the final state of A to the initial state of B. Union (A | B): If RE is A | B, create NFAs for A and B. Create a new initial state with \ud835\udf16 transitions to the initial states of A and B. Create a new final state with \ud835\udf16 transitions from the final states of A and B. Kleene Closure (A*): If RE is A*, create an NFA for A. Add a new initial state with \ud835\udf16 transitions to the initial state of A and a \ud835\udf16 transition from the final state of A to the initial state of A.","title":"Steps to Convert RE to NFA:"},{"location":"lectures/05_Automatic_construction_of_Lexers/#example","text":"Let's convert the regular expression (a|b)*abb to an NFA: Base Cases: a : NFA1 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (a)) b : NFA2 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (b)) \ud835\udf16: NFA\ud835\udf16 (States: 2, Initial: 1, Final: 2, Transition: 1->2 (\ud835\udf16)) Concatenation (ab): Concatenate NFA1 and NFA2. Union (a|b): Create a new initial state with \ud835\udf16 transitions to the initial states of NFA1 and NFA2. Create a new final state with \ud835\udf16 transitions from the final states of NFA1 and NFA2. Kleene Closure ((a|b)*): Add a new initial state with \ud835\udf16 transitions to the initial state of the union NFA. Add a \ud835\udf16 transition from the final state of the union NFA to its initial state. Concatenation with abb : Concatenate the Kleene Closure NFA with the NFA for abb . The resulting NFA accepts the language described by the regular expression.","title":"Example:"},{"location":"lectures/05_Automatic_construction_of_Lexers/#converting-nfa-to-dfa","text":"Demonstrates the algorithm to convert NFAs to DFAs. Each set of possible states in the NFA becomes one state in the DFA, resulting in a more efficient representation.","title":"Converting NFA to DFA"},{"location":"lectures/05_Automatic_construction_of_Lexers/#dfa-minimization","text":"DFA minimization is the process of converting a given Deterministic Finite Automaton (DFA) to an equivalent DFA with the minimum number of states. This process is also known as the optimization of DFA and uses a partitioning algorithm. The steps to minimize a DFA are as follows: Partitioning : Divide the set of states (Q) into two sets. One set will contain all final states and the other set will contain non-final states. This partition is called P0. Initialization : Initialize k = 1. Finding Pk : Find Pk by partitioning the different sets of Pk-1. In each set of Pk-1, take all possible pairs of states. If two states of a set are distinguishable, split the sets into different sets in Pk. Stopping Condition : Stop when Pk = Pk-1 (No change in partition). Merging States : All states of one set are merged into one. The number of states in the minimized DFA will be equal to the number of sets in Pk. Two states (qi, qj) are distinguishable in partition Pk if for any input symbol a, \u03b4(qi, a) and \u03b4(qj, a) are in different sets in partition Pk-1. Here's a pseudocode example of Hopcroft's algorithm, one of the algorithms for DFA minimization: P := {F, Q \\ F} W := {F, Q \\ F} while (W is not empty) do choose and remove a set A from W for each c in \u03a3 do let X be the set of states for which a transition on c leads to a state in A for each set Y in P for which X \u2229 Y is nonempty and Y \\ X is nonempty do replace Y in P by the two sets X \u2229 Y and Y \\ X if Y is in W replace Y in W by the same two sets else if |X \u2229 Y| <= |Y \\ X| add X \u2229 Y to W else add Y \\ X to W This algorithm starts with a partition that is too coarse: every pair of states that are equivalent according to the Nerode congruence belong to the same set in the partition, but pairs that are inequivalent might also belong to the same set. It gradually refines the partition into a larger number of smaller sets, at each step splitting sets of states into pairs of subsets that are necessarily inequivalent. Advantages of DFA minimization include reduced complexity, optimal space utilization, improved performance, and language equivalence. However, it also has some disadvantages such as increased computational complexity, additional design and analysis effort, loss of readability, and it's limited to deterministic automata. In Sammary: Intuition: - Two DFA states are equivalent if all subsequent behavior from those states is the same. Procedure: 1. Create a table of state pairs. 2. Mark cells where one state is final and the other is non-final. 3. Mark pairs where transitions on the same symbol lead to marked pairs. 4. Repeat step 3 until no unmarked pairs remain. 5. Merge unmarked states to achieve a minimized DFA.","title":"DFA Minimization"},{"location":"lectures/05_Automatic_construction_of_Lexers/#resolving-ambiguities-in-lexers","text":"Regular Expression Ambiguity: - Ambiguity arises when regular expressions can match input in multiple ways. Conflict Resolution in Lex: 1. Longest/Maximal Match Rule: - Prefer a longer prefix over a shorter one. 2. Declaration Priority: - If the longest prefix matches multiple patterns, prefer the one listed first in the Lex program.","title":"Resolving Ambiguities in Lexers"},{"location":"lectures/05_Automatic_construction_of_Lexers/#the-flex-manual","text":"An essential reference for using the Flex tool. It provides detailed information on the Flex tool and its capabilities. Refer to the provided appendix slide for in-depth insights.","title":"The Flex Manual"},{"location":"lectures/06_Syntax-Analysis/","text":"Syntax Analysis When an input string (source code or a program in some language) is given to a compiler, the compiler processes it in several phases, starting from lexical analysis (As mentioned scans the input and divides it into tokens) to target code generation. Syntax analysis or parsing constitutes the second phase within the compiler's workflow. This chapter delves into fundamental concepts crucial for constructing a parser. As previously explored, a lexical analyzer proficiently identifies tokens using regular expressions and pattern rules. However, its capacity is constrained when it comes to scrutinizing the syntax of a given sentence, particularly in tasks involving balancing tokens like parentheses. To overcome this limitation, syntax analysis employs context-free grammar (CFG) , a construct recognized by push-down automata. Syntax Analysis , situated after the lexical analysis , scrutinizes the syntactical structure of the input. It verifies whether the provided input adheres to the correct syntax of the relevant programming language. This verification process involves constructing a data structure known as a Parse tree or Syntax tree . By leveraging the predefined Grammar of the language and the input string, the parse tree is crafted. Successful derivation of the input string from this syntax tree indicates correct syntax usage. Conversely, any deviation triggers an error report from the syntax analyzer. Syntax analysis , often referred to as parsing , is a critical phase in compiler design where the compiler assesses whether the source code aligns with the grammatical rules of the programming language. Typically occurring as the second stage in the compilation process, following lexical analysis , the primary objective is to generate a parse tree or abstract syntax tree (AST) . This hierarchical representation mirrors the grammatical structure of the program encapsulated in the source code. So, let's to Learn... CFG, on the other hand, is a superset of Regular Grammar, as depicted below: which are fully explained in the next lesson Syntax Analyzers A syntax analyzer, also known as a parser, receives input in the form of token streams from a lexical analyzer. Its primary role is to examine the source code (presented as a token stream) against production rules, aiming to identify and flag any errors within the code. The outcome of this process is the creation of a parse tree. In essence, the parser performs two essential tasks: 1. parsing the code to detect errors 2. generating a parse tree as the result of this analysis. It's important to note that parsers are designed to handle the entire code , even in the presence of errors . To achieve this, parsers employ error recovery strategies, which we will delve into later in this chapter. These strategies enable parsers to effectively navigate and process code, providing a comprehensive analysis and aiding in the identification and handling of errors within a program. Let's Dive into Derivations! In the world of compiler design, there are two types of derivations that we often encounter: left-most and right-most derivations. These are like the two sides of a coin, each with its own unique characteristics. Production Rules Let's start with some production rules. These are like the recipes that our compiler follows to understand and process the input string. Here are some example production rules: E \u2192 E + E E \u2192 E * E E \u2192 id And here's the input string that we'll be working with: id + id * id Left-most Derivation Now, let's see how the compiler would process this input string using a left-most derivation. This is like saying, \"Hey compiler, let's start from the left and work our way to the right.\" Here's how it looks: E \u2192 E * E E \u2192 E + E * E E \u2192 id + E * E E \u2192 id + id * E E \u2192 id + id * id Notice that the left-most non-terminal is always processed first. It's like the compiler is saying, \"I'll handle the leftmost thing first, then move on to the next one on the left.\" Right-most Derivation Now, let's see how the compiler would process the same input string using a right-most derivation. This is like saying, \"Hey compiler, let's start from the right and work our way to the left.\" Here's how it looks: E \u2192 E + E E \u2192 E + E * E E \u2192 E + E * id E \u2192 E + id * id E \u2192 id + id * id And that's it! We've now explored both left-most and right-most derivations. Remember, these are just the two sides of a coin. Depending on the parsing strategy that the compiler uses, it might prefer one side over the other. Understanding Parse Trees Parse trees are like a roadmap for your compiler. They are graphical representations of a derivation, showing how strings are derived from the start symbol. The start symbol becomes the root of the parse tree, and it's great to visualize this process. Let's take a look at an example using the left-most derivation of a + b * c . The Left-most Derivation For example for write parse tree for this left-most derivation: E \u2192 E * E E \u2192 E + E * E E \u2192 id + E * E E \u2192 id + id * E E \u2192 id + id * id Step-by-Step Parse Tree Construction Now, let's build the parse tree step-by-step: step 1: E \u2192 E * E step 2: E \u2192 E + E * E step 3: E \u2192 id + E * E step 4: E \u2192 id + id * E step 5: E \u2192 id + id * id Parse Tree Characteristics In a parse tree, we have: All leaf nodes are terminals. All interior nodes are non-terminals. In-order traversal gives the original input string. The parse tree shows the associativity and precedence of operators. The deepest sub-tree is traversed first, so the operator in that sub-tree gets precedence over the operator in the parent nodes. Ambiguity in Grammar A grammar is said to be ambiguous if it has more than one parse tree (left or right derivation) for at least one string. For example, consider the following grammar: E \u2192 E + E E \u2192 E \u2013 E E \u2192 id For the string id + id \u2013 id , the above grammar generates two parse trees. The language generated by an ambiguous grammar is said to be inherently ambiguous. While no method can automatically detect and remove ambiguity, it can be manually removed by re-writing the whole grammar without ambiguity, or by setting and following associativity and precedence constraints. in another word: Parse trees and derivations are key concepts in compiler design. They help us understand how to process and evaluate expressions. Context-Free Grammar and Parse Trees A context-free grammar (CFG) is a type of grammar where every production rule is of the form A \u2192 \u03b1 , where A is a single non-terminal and \u03b1 is a string of terminals and/or non-terminals. A parse tree, on the other hand, is a tree structure that represents the syntactic structure of a string according to some grammar. In the context of a CFG, a parse tree is a derivation or parse tree for G if and only if it has the following properties: The root is labeled S . Every leaf has a label from T \u222a {\u03b5} . Every interior vertex (a vertex that is not a leaf) has a label from V . If a vertex has label A \u2208 V , and its children are labeled (from left to right) a1, a2, ..., an , then P must contain a production of the form A \u2192 a1a2...an . A leaf labeled \u03b5 has no siblings, that is, a vertex with a child labeled \u03b5 can have no other children. Example of Derivation (Parse) Trees Consider the following grammar and string: Grammar: E \u2192 E + E | E * E | -E | (E) | id String: -(id + id) The leftmost derivation for this grammar and string is: E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(id + E) \u21d2 -(id + id) The rightmost derivation for the same grammar and string is: E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(E + id) \u21d2 -(id + id) Both derivations result in the same parse tree. Derivation and Parse Trees There is a many-to-one relationship between derivations and parse trees. Indeed, no information on the order of derivation steps is associated with the final parse tree. Parse trees and abstract syntax tree (AST) An AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). Understanding Associativity Associativity is like a rule that helps us decide the order of operations when an operand has operators on both sides. If the operation is left-associative, the operand will be taken by the left operator. If it's right-associative, the right operator will take the operand. Left Associative Operations Operations like Addition, Multiplication, Subtraction, and Division are left associative. This means that when an expression contains more than one of these operations, the operations are performed from left to right. For example, if we have an expression like id op id op id , it will be evaluated as (id op id) op id . To illustrate, consider the expression (id + id) + id . Right Associative Operations Operations like Exponentiation are right associative. This means that when an expression contains more than one of these operations, the operations are performed from right to left. For example, if we have an expression like id op (id op id) , it will be evaluated as id op (id op id) . To illustrate, consider the expression id ^ (id ^ id) . Important Points Here are a few key points to remember about associativity: All operators with the same precedence have the same associativity. This is necessary because it helps the compiler decide the order of operations when an expression has two operators of the same precedence. The associativity of postfix and prefix operators is different. The associativity of postfix is left to right, while the associativity of prefix is right to left. The comma operator has the lowest precedence among all operators. It's important to use it carefully to avoid unexpected results. Precedence Precedence is like a rule that helps us decide which operation to perform first when two different operators share a common operand. For example, in the expression 2+3*4 , both addition and multiplication are operators that share the operand 3 . By setting precedence among operators, we can easily decide which operation to perform first. Mathematically, multiplication (*) has precedence over addition (+), so the expression 2+3*4 will always be interpreted as (2 + (3 * 4)) . Left Recursion Left recursion is a situation where a grammar has a non-terminal that appears as the left-most symbol in its own derivation. This can cause problems for top-down parsers, which start parsing from the start symbol and can get stuck in an infinite loop when they encounter the same non-terminal in their derivation. For example, consider the following grammar: A => A\u03b1 | \u03b2 S => A\u03b1 | \u03b2 A => Sd The first example is an example of immediate left recursion, where A is any non-terminal symbol and \u03b1 represents a string of non-terminals. The second example is an example of indirect left recursion. In a top-down parser, it will first parse A , which in turn will yield a string consisting of A itself and the parser may go into an infinite loop. By understanding and managing precedence and left recursion, we can make sure that our compiler can correctly parse and evaluate expressions. Summary Understanding Syntax Analyzers Syntax analyzers play a crucial role in the field of compiler design. They validate the syntax of the source code written in a programming language using a component called a parser. The aim is to test whether a source code (\ud835\udc64) belongs to a programming language (\ud835\udc3f) with grammar (\ud835\udc3a). The answer is a simple \"yes\" or \"no\". However, the syntax analyzer in a compiler must do more than just validate the syntax. It must also generate a syntax tree and handle errors gracefully if the string is not in the language. The parser uses the stream of tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts the grammatical structure of the token stream. The parser also reports any syntax errors in an intelligible fashion and recovers from commonly occurring errors to continue processing the remainder of the program. Prerequisites for Syntax Analysis Syntax analysis requires two main components: An expressive description technique to describe the syntax. An acceptor mechanism to determine if the input token stream satisfies the syntax description. For lexical analysis, regular expressions are used to describe tokens, and finite automata is used as an acceptor for regular expressions. Limitations of Regular Expressions for Syntax Analysis General-purpose programming languages like C, C++, C#, Java, etc., are not regular languages, so they cannot be described by regular expressions. Consider nested constructs (blocks, expressions, statements), and you'll see that regular expressions fall short. For example, the syntax of the '{' construct in the second code snippet can be described with a language like \ud835\udc3f = {\ufe00\ud835\udc4e\ud835\udc5b\ud835\udc4f\ud835\udc5b|\ud835\udc5b \u2265 0}, which is a context-free language, not regular. Non-Context-Free Language Constructs Not all constructs found in typical programming languages can be specified using Context-Free Grammar (CFG) grammars alone. For instance, the declaration of identifiers before their use and checking that the number of formal parameters in the declaration of a function agrees with the number of actual parameters are examples of constructs that cannot be specified using CFG grammars alone. Syntax Analysis Scope Syntax analysis cannot check whether variables are of types on which operations are allowed, whether a variable has been declared before use, or whether a variable has been initialized. These issues will be handled in the semantic analysis phase. For now, let's focus on syntax analysis. Context-Free Grammars for Programming Languages Programming languages grammar uses Context-Free Grammar (CFG) instead of regular grammar to precisely describe the syntactic properties of the programming languages. A specification of the balanced-parenthesis language using context-free grammar is a good example of this. Example Unambiguous, with precedence and associativity rules honored: Ambiguous: E -> E + E | E * E | (E) | num | id Unambiguous: E -> E + T | T T -> T * F | F F -> (E) | num | id For another example for operation(+, -, *, /, ^), we have: E -> E + T | T T -> T * F | T * F | F F -> G ^ F | G G -> num | id | (E) E /|\\ / | \\ / | \\ E + T | /|\\ | / | \\ num T * F | | | 1 F G /|\\ | G ^ F num | | | num G 3 | | 2 num | 3","title":"Unit 6 Syntax analysis"},{"location":"lectures/06_Syntax-Analysis/#syntax-analysis","text":"","title":" Syntax Analysis"},{"location":"lectures/06_Syntax-Analysis/#_1","text":"When an input string (source code or a program in some language) is given to a compiler, the compiler processes it in several phases, starting from lexical analysis (As mentioned scans the input and divides it into tokens) to target code generation. Syntax analysis or parsing constitutes the second phase within the compiler's workflow. This chapter delves into fundamental concepts crucial for constructing a parser. As previously explored, a lexical analyzer proficiently identifies tokens using regular expressions and pattern rules. However, its capacity is constrained when it comes to scrutinizing the syntax of a given sentence, particularly in tasks involving balancing tokens like parentheses. To overcome this limitation, syntax analysis employs context-free grammar (CFG) , a construct recognized by push-down automata. Syntax Analysis , situated after the lexical analysis , scrutinizes the syntactical structure of the input. It verifies whether the provided input adheres to the correct syntax of the relevant programming language. This verification process involves constructing a data structure known as a Parse tree or Syntax tree . By leveraging the predefined Grammar of the language and the input string, the parse tree is crafted. Successful derivation of the input string from this syntax tree indicates correct syntax usage. Conversely, any deviation triggers an error report from the syntax analyzer. Syntax analysis , often referred to as parsing , is a critical phase in compiler design where the compiler assesses whether the source code aligns with the grammatical rules of the programming language. Typically occurring as the second stage in the compilation process, following lexical analysis , the primary objective is to generate a parse tree or abstract syntax tree (AST) . This hierarchical representation mirrors the grammatical structure of the program encapsulated in the source code. So, let's to Learn... CFG, on the other hand, is a superset of Regular Grammar, as depicted below:","title":""},{"location":"lectures/06_Syntax-Analysis/#_2","text":"which are fully explained in the next lesson","title":""},{"location":"lectures/06_Syntax-Analysis/#syntax-analyzers","text":"A syntax analyzer, also known as a parser, receives input in the form of token streams from a lexical analyzer. Its primary role is to examine the source code (presented as a token stream) against production rules, aiming to identify and flag any errors within the code. The outcome of this process is the creation of a parse tree. In essence, the parser performs two essential tasks: 1. parsing the code to detect errors 2. generating a parse tree as the result of this analysis. It's important to note that parsers are designed to handle the entire code , even in the presence of errors . To achieve this, parsers employ error recovery strategies, which we will delve into later in this chapter. These strategies enable parsers to effectively navigate and process code, providing a comprehensive analysis and aiding in the identification and handling of errors within a program.","title":"Syntax Analyzers"},{"location":"lectures/06_Syntax-Analysis/#_3","text":"","title":""},{"location":"lectures/06_Syntax-Analysis/#lets-dive-into-derivations","text":"In the world of compiler design, there are two types of derivations that we often encounter: left-most and right-most derivations. These are like the two sides of a coin, each with its own unique characteristics. Production Rules Let's start with some production rules. These are like the recipes that our compiler follows to understand and process the input string. Here are some example production rules: E \u2192 E + E E \u2192 E * E E \u2192 id And here's the input string that we'll be working with: id + id * id Left-most Derivation Now, let's see how the compiler would process this input string using a left-most derivation. This is like saying, \"Hey compiler, let's start from the left and work our way to the right.\" Here's how it looks: E \u2192 E * E E \u2192 E + E * E E \u2192 id + E * E E \u2192 id + id * E E \u2192 id + id * id Notice that the left-most non-terminal is always processed first. It's like the compiler is saying, \"I'll handle the leftmost thing first, then move on to the next one on the left.\" Right-most Derivation Now, let's see how the compiler would process the same input string using a right-most derivation. This is like saying, \"Hey compiler, let's start from the right and work our way to the left.\" Here's how it looks: E \u2192 E + E E \u2192 E + E * E E \u2192 E + E * id E \u2192 E + id * id E \u2192 id + id * id And that's it! We've now explored both left-most and right-most derivations. Remember, these are just the two sides of a coin. Depending on the parsing strategy that the compiler uses, it might prefer one side over the other.","title":"Let's Dive into Derivations!"},{"location":"lectures/06_Syntax-Analysis/#understanding-parse-trees","text":"Parse trees are like a roadmap for your compiler. They are graphical representations of a derivation, showing how strings are derived from the start symbol. The start symbol becomes the root of the parse tree, and it's great to visualize this process. Let's take a look at an example using the left-most derivation of a + b * c . The Left-most Derivation For example for write parse tree for this left-most derivation: E \u2192 E * E E \u2192 E + E * E E \u2192 id + E * E E \u2192 id + id * E E \u2192 id + id * id Step-by-Step Parse Tree Construction Now, let's build the parse tree step-by-step: step 1: E \u2192 E * E","title":"Understanding Parse Trees"},{"location":"lectures/06_Syntax-Analysis/#_4","text":"step 2: E \u2192 E + E * E","title":""},{"location":"lectures/06_Syntax-Analysis/#_5","text":"step 3: E \u2192 id + E * E","title":""},{"location":"lectures/06_Syntax-Analysis/#_6","text":"step 4: E \u2192 id + id * E","title":""},{"location":"lectures/06_Syntax-Analysis/#_7","text":"step 5: E \u2192 id + id * id","title":""},{"location":"lectures/06_Syntax-Analysis/#_8","text":"Parse Tree Characteristics In a parse tree, we have: All leaf nodes are terminals. All interior nodes are non-terminals. In-order traversal gives the original input string. The parse tree shows the associativity and precedence of operators. The deepest sub-tree is traversed first, so the operator in that sub-tree gets precedence over the operator in the parent nodes. Ambiguity in Grammar A grammar is said to be ambiguous if it has more than one parse tree (left or right derivation) for at least one string. For example, consider the following grammar: E \u2192 E + E E \u2192 E \u2013 E E \u2192 id For the string id + id \u2013 id , the above grammar generates two parse trees. The language generated by an ambiguous grammar is said to be inherently ambiguous. While no method can automatically detect and remove ambiguity, it can be manually removed by re-writing the whole grammar without ambiguity, or by setting and following associativity and precedence constraints. in another word: Parse trees and derivations are key concepts in compiler design. They help us understand how to process and evaluate expressions. Context-Free Grammar and Parse Trees A context-free grammar (CFG) is a type of grammar where every production rule is of the form A \u2192 \u03b1 , where A is a single non-terminal and \u03b1 is a string of terminals and/or non-terminals. A parse tree, on the other hand, is a tree structure that represents the syntactic structure of a string according to some grammar. In the context of a CFG, a parse tree is a derivation or parse tree for G if and only if it has the following properties: The root is labeled S . Every leaf has a label from T \u222a {\u03b5} . Every interior vertex (a vertex that is not a leaf) has a label from V . If a vertex has label A \u2208 V , and its children are labeled (from left to right) a1, a2, ..., an , then P must contain a production of the form A \u2192 a1a2...an . A leaf labeled \u03b5 has no siblings, that is, a vertex with a child labeled \u03b5 can have no other children. Example of Derivation (Parse) Trees Consider the following grammar and string: Grammar: E \u2192 E + E | E * E | -E | (E) | id String: -(id + id) The leftmost derivation for this grammar and string is: E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(id + E) \u21d2 -(id + id) The rightmost derivation for the same grammar and string is: E \u21d2 -E \u21d2 -(E) \u21d2 -(E + E) \u21d2 -(E + id) \u21d2 -(id + id) Both derivations result in the same parse tree. Derivation and Parse Trees There is a many-to-one relationship between derivations and parse trees. Indeed, no information on the order of derivation steps is associated with the final parse tree.","title":""},{"location":"lectures/06_Syntax-Analysis/#parse-trees-and-abstract-syntax-tree-ast","text":"An AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.).","title":"Parse trees and abstract syntax tree (AST)"},{"location":"lectures/06_Syntax-Analysis/#_9","text":"","title":""},{"location":"lectures/06_Syntax-Analysis/#understanding-associativity","text":"Associativity is like a rule that helps us decide the order of operations when an operand has operators on both sides. If the operation is left-associative, the operand will be taken by the left operator. If it's right-associative, the right operator will take the operand. Left Associative Operations Operations like Addition, Multiplication, Subtraction, and Division are left associative. This means that when an expression contains more than one of these operations, the operations are performed from left to right. For example, if we have an expression like id op id op id , it will be evaluated as (id op id) op id . To illustrate, consider the expression (id + id) + id . Right Associative Operations Operations like Exponentiation are right associative. This means that when an expression contains more than one of these operations, the operations are performed from right to left. For example, if we have an expression like id op (id op id) , it will be evaluated as id op (id op id) . To illustrate, consider the expression id ^ (id ^ id) . Important Points Here are a few key points to remember about associativity: All operators with the same precedence have the same associativity. This is necessary because it helps the compiler decide the order of operations when an expression has two operators of the same precedence. The associativity of postfix and prefix operators is different. The associativity of postfix is left to right, while the associativity of prefix is right to left. The comma operator has the lowest precedence among all operators. It's important to use it carefully to avoid unexpected results.","title":"Understanding Associativity"},{"location":"lectures/06_Syntax-Analysis/#precedence","text":"Precedence is like a rule that helps us decide which operation to perform first when two different operators share a common operand. For example, in the expression 2+3*4 , both addition and multiplication are operators that share the operand 3 . By setting precedence among operators, we can easily decide which operation to perform first. Mathematically, multiplication (*) has precedence over addition (+), so the expression 2+3*4 will always be interpreted as (2 + (3 * 4)) .","title":"Precedence"},{"location":"lectures/06_Syntax-Analysis/#left-recursion","text":"Left recursion is a situation where a grammar has a non-terminal that appears as the left-most symbol in its own derivation. This can cause problems for top-down parsers, which start parsing from the start symbol and can get stuck in an infinite loop when they encounter the same non-terminal in their derivation. For example, consider the following grammar: A => A\u03b1 | \u03b2 S => A\u03b1 | \u03b2 A => Sd The first example is an example of immediate left recursion, where A is any non-terminal symbol and \u03b1 represents a string of non-terminals. The second example is an example of indirect left recursion.","title":"Left Recursion"},{"location":"lectures/06_Syntax-Analysis/#_10","text":"In a top-down parser, it will first parse A , which in turn will yield a string consisting of A itself and the parser may go into an infinite loop. By understanding and managing precedence and left recursion, we can make sure that our compiler can correctly parse and evaluate expressions.","title":""},{"location":"lectures/06_Syntax-Analysis/#summary","text":"","title":"Summary"},{"location":"lectures/06_Syntax-Analysis/#understanding-syntax-analyzers","text":"Syntax analyzers play a crucial role in the field of compiler design. They validate the syntax of the source code written in a programming language using a component called a parser. The aim is to test whether a source code (\ud835\udc64) belongs to a programming language (\ud835\udc3f) with grammar (\ud835\udc3a). The answer is a simple \"yes\" or \"no\". However, the syntax analyzer in a compiler must do more than just validate the syntax. It must also generate a syntax tree and handle errors gracefully if the string is not in the language. The parser uses the stream of tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts the grammatical structure of the token stream. The parser also reports any syntax errors in an intelligible fashion and recovers from commonly occurring errors to continue processing the remainder of the program.","title":"Understanding Syntax Analyzers"},{"location":"lectures/06_Syntax-Analysis/#prerequisites-for-syntax-analysis","text":"Syntax analysis requires two main components: An expressive description technique to describe the syntax. An acceptor mechanism to determine if the input token stream satisfies the syntax description. For lexical analysis, regular expressions are used to describe tokens, and finite automata is used as an acceptor for regular expressions.","title":"Prerequisites for Syntax Analysis"},{"location":"lectures/06_Syntax-Analysis/#limitations-of-regular-expressions-for-syntax-analysis","text":"General-purpose programming languages like C, C++, C#, Java, etc., are not regular languages, so they cannot be described by regular expressions. Consider nested constructs (blocks, expressions, statements), and you'll see that regular expressions fall short. For example, the syntax of the '{' construct in the second code snippet can be described with a language like \ud835\udc3f = {\ufe00\ud835\udc4e\ud835\udc5b\ud835\udc4f\ud835\udc5b|\ud835\udc5b \u2265 0}, which is a context-free language, not regular.","title":"Limitations of Regular Expressions for Syntax Analysis"},{"location":"lectures/06_Syntax-Analysis/#non-context-free-language-constructs","text":"Not all constructs found in typical programming languages can be specified using Context-Free Grammar (CFG) grammars alone. For instance, the declaration of identifiers before their use and checking that the number of formal parameters in the declaration of a function agrees with the number of actual parameters are examples of constructs that cannot be specified using CFG grammars alone.","title":"Non-Context-Free Language Constructs"},{"location":"lectures/06_Syntax-Analysis/#syntax-analysis-scope","text":"Syntax analysis cannot check whether variables are of types on which operations are allowed, whether a variable has been declared before use, or whether a variable has been initialized. These issues will be handled in the semantic analysis phase. For now, let's focus on syntax analysis.","title":"Syntax Analysis Scope"},{"location":"lectures/06_Syntax-Analysis/#context-free-grammars-for-programming-languages","text":"Programming languages grammar uses Context-Free Grammar (CFG) instead of regular grammar to precisely describe the syntactic properties of the programming languages. A specification of the balanced-parenthesis language using context-free grammar is a good example of this.","title":"Context-Free Grammars for Programming Languages"},{"location":"lectures/06_Syntax-Analysis/#example","text":"Unambiguous, with precedence and associativity rules honored:","title":"Example"},{"location":"lectures/06_Syntax-Analysis/#ambiguous","text":"E -> E + E | E * E | (E) | num | id","title":"Ambiguous:"},{"location":"lectures/06_Syntax-Analysis/#unambiguous","text":"E -> E + T | T T -> T * F | F F -> (E) | num | id For another example for operation(+, -, *, /, ^), we have: E -> E + T | T T -> T * F | T * F | F F -> G ^ F | G G -> num | id | (E) E /|\\ / | \\ / | \\ E + T | /|\\ | / | \\ num T * F | | | 1 F G /|\\ | G ^ F num | | | num G 3 | | 2 num | 3","title":"Unambiguous:"},{"location":"lectures/07_Types-of-Grammar/","text":"Types of Grammar 1. Regular Expressions Definition Regular Expressions (Regex): A regular expression is a sequence of characters that defines a search pattern. It is commonly used for string matching within a text. Example The regular expression (a + b * c)* describes the language of all strings over the alphabet {a, b, c} where any combination of a , b , and c is allowed, including the empty string. Operations on Regular Expressions Concatenation ( r1 r2 ): Combines two regular expressions. Union ( r1 + r2 ): Represents alternatives between two regular expressions. Kleene Star ( r* ): Denotes zero or more repetitions of the preceding regular expression. 2. Recursive Definition Primitive Regular Expressions Empty Set ( \u2205 ): Represents the language containing no strings. Empty String ( \u03bb ): Represents the language containing only the empty string. Atomic Symbol ( \u03b1 ): Represents a single character from the alphabet. Operations Union ( r1 + r2 ): Represents the union of two languages. Concatenation ( r1 r2 ): Represents the concatenation of two languages. Kleene Star ( r* ): Represents zero or more repetitions of a language. 3. Examples Example 1 Regular Expression: (a + b) * a* Language: All strings with any combination of a and b , followed by zero or more a . Example 2 Regular Expression: (a + b*) * (c + \u2205) Language: All strings with any combination of a and zero or more b , followed by either c or the empty string. 4. Languages of Regular Expressions Definition For a regular expression r , the language L(r) is the set of all strings that can be generated by r . Example For the regular expression (a + b * c)* , L((a + b * c)*) is the set of all strings over {a, b, c} . 5. Conversion from Finite Automaton (FA) to Regular Expression Generalized Transition Graph Represents the transitions of a finite automaton. The final regular expression is obtained by combining the regular expressions associated with the transitions. Example Transition graph with states q0 , q1 , q2 , q3 , q4 , and transitions labeled with regular expressions. The final regular expression r is obtained by combining the expressions associated with transitions.","title":"Unit 7 Type of grammars"},{"location":"lectures/07_Types-of-Grammar/#types-of-grammar","text":"","title":" Types of Grammar"},{"location":"lectures/07_Types-of-Grammar/#_1","text":"","title":""},{"location":"lectures/07_Types-of-Grammar/#1-regular-expressions","text":"","title":"1. Regular Expressions"},{"location":"lectures/07_Types-of-Grammar/#definition","text":"Regular Expressions (Regex): A regular expression is a sequence of characters that defines a search pattern. It is commonly used for string matching within a text.","title":"Definition"},{"location":"lectures/07_Types-of-Grammar/#example","text":"The regular expression (a + b * c)* describes the language of all strings over the alphabet {a, b, c} where any combination of a , b , and c is allowed, including the empty string.","title":"Example"},{"location":"lectures/07_Types-of-Grammar/#operations-on-regular-expressions","text":"Concatenation ( r1 r2 ): Combines two regular expressions. Union ( r1 + r2 ): Represents alternatives between two regular expressions. Kleene Star ( r* ): Denotes zero or more repetitions of the preceding regular expression.","title":"Operations on Regular Expressions"},{"location":"lectures/07_Types-of-Grammar/#2-recursive-definition","text":"","title":"2. Recursive Definition"},{"location":"lectures/07_Types-of-Grammar/#primitive-regular-expressions","text":"Empty Set ( \u2205 ): Represents the language containing no strings. Empty String ( \u03bb ): Represents the language containing only the empty string. Atomic Symbol ( \u03b1 ): Represents a single character from the alphabet.","title":"Primitive Regular Expressions"},{"location":"lectures/07_Types-of-Grammar/#operations","text":"Union ( r1 + r2 ): Represents the union of two languages. Concatenation ( r1 r2 ): Represents the concatenation of two languages. Kleene Star ( r* ): Represents zero or more repetitions of a language.","title":"Operations"},{"location":"lectures/07_Types-of-Grammar/#3-examples","text":"","title":"3. Examples"},{"location":"lectures/07_Types-of-Grammar/#example-1","text":"Regular Expression: (a + b) * a* Language: All strings with any combination of a and b , followed by zero or more a .","title":"Example 1"},{"location":"lectures/07_Types-of-Grammar/#example-2","text":"Regular Expression: (a + b*) * (c + \u2205) Language: All strings with any combination of a and zero or more b , followed by either c or the empty string.","title":"Example 2"},{"location":"lectures/07_Types-of-Grammar/#4-languages-of-regular-expressions","text":"","title":"4. Languages of Regular Expressions"},{"location":"lectures/07_Types-of-Grammar/#definition_1","text":"For a regular expression r , the language L(r) is the set of all strings that can be generated by r .","title":"Definition"},{"location":"lectures/07_Types-of-Grammar/#example_1","text":"For the regular expression (a + b * c)* , L((a + b * c)*) is the set of all strings over {a, b, c} .","title":"Example"},{"location":"lectures/07_Types-of-Grammar/#5-conversion-from-finite-automaton-fa-to-regular-expression","text":"","title":"5. Conversion from Finite Automaton (FA) to Regular Expression"},{"location":"lectures/07_Types-of-Grammar/#generalized-transition-graph","text":"Represents the transitions of a finite automaton. The final regular expression is obtained by combining the regular expressions associated with the transitions.","title":"Generalized Transition Graph"},{"location":"lectures/07_Types-of-Grammar/#example_2","text":"Transition graph with states q0 , q1 , q2 , q3 , q4 , and transitions labeled with regular expressions. The final regular expression r is obtained by combining the expressions associated with transitions.","title":"Example"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/","text":"Ambiguity Problems and Eliminating the Ambiguity Understanding Ambiguity in Compilers Ambiguity in compilers can lead to significant problems. It occurs when the meaning of a program can be incorrect due to the lack of precision in the language syntax. One classic example of such ambiguity is the \"dangling else\" problem. Dangling Else Problem Consider the following statement: if E1 then if E2 then S1 else S2 Here, E1 , E2 , S1 , and S2 represent any expressions or statements. The dangling else problem arises because it's unclear to which if statement the else clause should be attached. Example Let's consider the following values: E1 = false , E2 = true , S1 = z := 10 , and S2 = z := 0 . If we parse this statement using the top tree, the z variable doesn't get set, which is incorrect. If we parse using the bottom tree, z = 0 , which is also incorrect. Both trees are valid parse trees for the given statement, leading to ambiguity. Resolving Ambiguity To resolve this ambiguity, we can use braces {} and indentation to clearly indicate the structure of the if statements. For example: if E1 { if E2 { S1 } else { S2 } } In this case, it's clear that the else clause belongs to the inner if statement. Another solution is to use the if-else if-else format, which specifically indicates which else belongs to which if . Note: In practice, if there is no clear way to resolve the ambiguity, compilers usually associate the else with the nearest if . stmt -> matched_stmt | open_stmt matched_stmt -> if expr then matched_stmt else mathed_stmt | other open_stmt -> if expr then stmt | if expr then matched_stmt else open_stmt Parsing reminder Parsing, also known as syntax analysis, is a critical phase in the process of compiling or interpreting a programming language. It's the process of determining whether a string of terminals can be generated by a grammar. In simpler terms, parsing is the task of checking if a given sequence of tokens (the output from the lexical analysis phase) conforms to the syntactic rules of a programming language. A parser is a program that performs this syntax analysis. It takes as input the tokens from the lexical analyzer (the previous phase of the compilation process) and treats the token names as terminal symbols of a context-free grammar. The parser's main job is to construct a parse tree, which is a hierarchical representation of the source code that reflects the grammatical structure of the program. The parse tree can be constructed in two ways: Guratively : This involves going through the corresponding derivation steps. It starts from the start symbol of the grammar and applies the production rules in a top-down manner to generate the parse tree. Literally : This involves constructing the parse tree directly from the input string. It starts from the leaves of the parse tree and applies the production rules in a bottom-up manner to generate the parse tree There are several types of parsing algorithms used in syntax analysis, including LL parsing, LR parsing, LR(1) parsing, and LALR parsing. Each of these algorithms has its own strengths and weaknesses, and the choice of algorithm depends on the specific requirements of the programming language being compiled or interpreted. Parsing is an essential step in the compilation process because it allows the compiler to check if the source code follows the grammatical rules of the programming language. This helps to detect and report errors in the source code, and it also enables the compiler to generate more efficient and optimized code. For example: For this sentence: She loves animals $\\rightarrow$ Parser $\\rightarrow$ Sentence / | \\ / | \\ / | \\ pornoun verb noun | | | She loves animals Types of Parsing Syntax analyzers follow production rules defined by means of context-free grammar. The way the production rules are implemented (derivation) divides parsing into two types : top-down parsing and bottom-up parsing. Top-down Parsing When the parser starts constructing the parse tree from the start symbol and then tries to transform the start symbol to the input, it is called top-down parsing. Recursive descent parsing : It is a common form of top-down parsing. It is called recursive as it uses recursive procedures to process the input. Recursive descent parsing suffers from backtracking. Backtracking : It means, if one derivation of a production fails, the syntax analyzer restarts the process using different rules of same production. This technique may process the input string more than once to determine the right production. Bottom-up Parsing As the name suggests, bottom-up parsing starts with the input symbols and tries to construct the parse tree up to the start symbol. Example : Input string : a + b * c Production rules: S \u2192 E E \u2192 E + T E \u2192 E * T E \u2192 T T \u2192 id Let us start bottom-up parsing a + b * c Read the input and check if any production matches with the input: a + b * c T + b * c E + b * c E + T * c E * c E * T E S","title":"Unit 8 Eliminating ambiguity"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#ambiguity-problems-and-eliminating-the-ambiguity","text":"","title":"Ambiguity Problems and Eliminating the Ambiguity"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#_1","text":"","title":""},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#understanding-ambiguity-in-compilers","text":"Ambiguity in compilers can lead to significant problems. It occurs when the meaning of a program can be incorrect due to the lack of precision in the language syntax. One classic example of such ambiguity is the \"dangling else\" problem. Dangling Else Problem Consider the following statement: if E1 then if E2 then S1 else S2 Here, E1 , E2 , S1 , and S2 represent any expressions or statements. The dangling else problem arises because it's unclear to which if statement the else clause should be attached.","title":"Understanding Ambiguity in Compilers"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#_2","text":"Example Let's consider the following values: E1 = false , E2 = true , S1 = z := 10 , and S2 = z := 0 . If we parse this statement using the top tree, the z variable doesn't get set, which is incorrect. If we parse using the bottom tree, z = 0 , which is also incorrect. Both trees are valid parse trees for the given statement, leading to ambiguity. Resolving Ambiguity To resolve this ambiguity, we can use braces {} and indentation to clearly indicate the structure of the if statements. For example: if E1 { if E2 { S1 } else { S2 } } In this case, it's clear that the else clause belongs to the inner if statement. Another solution is to use the if-else if-else format, which specifically indicates which else belongs to which if . Note: In practice, if there is no clear way to resolve the ambiguity, compilers usually associate the else with the nearest if . stmt -> matched_stmt | open_stmt matched_stmt -> if expr then matched_stmt else mathed_stmt | other open_stmt -> if expr then stmt | if expr then matched_stmt else open_stmt","title":""},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#parsing-reminder","text":"Parsing, also known as syntax analysis, is a critical phase in the process of compiling or interpreting a programming language. It's the process of determining whether a string of terminals can be generated by a grammar. In simpler terms, parsing is the task of checking if a given sequence of tokens (the output from the lexical analysis phase) conforms to the syntactic rules of a programming language. A parser is a program that performs this syntax analysis. It takes as input the tokens from the lexical analyzer (the previous phase of the compilation process) and treats the token names as terminal symbols of a context-free grammar. The parser's main job is to construct a parse tree, which is a hierarchical representation of the source code that reflects the grammatical structure of the program. The parse tree can be constructed in two ways: Guratively : This involves going through the corresponding derivation steps. It starts from the start symbol of the grammar and applies the production rules in a top-down manner to generate the parse tree. Literally : This involves constructing the parse tree directly from the input string. It starts from the leaves of the parse tree and applies the production rules in a bottom-up manner to generate the parse tree There are several types of parsing algorithms used in syntax analysis, including LL parsing, LR parsing, LR(1) parsing, and LALR parsing. Each of these algorithms has its own strengths and weaknesses, and the choice of algorithm depends on the specific requirements of the programming language being compiled or interpreted. Parsing is an essential step in the compilation process because it allows the compiler to check if the source code follows the grammatical rules of the programming language. This helps to detect and report errors in the source code, and it also enables the compiler to generate more efficient and optimized code. For example: For this sentence: She loves animals $\\rightarrow$ Parser $\\rightarrow$ Sentence / | \\ / | \\ / | \\ pornoun verb noun | | | She loves animals","title":"Parsing reminder"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#types-of-parsing","text":"Syntax analyzers follow production rules defined by means of context-free grammar. The way the production rules are implemented (derivation) divides parsing into two types : top-down parsing and bottom-up parsing.","title":"Types of Parsing"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#top-down-parsing","text":"When the parser starts constructing the parse tree from the start symbol and then tries to transform the start symbol to the input, it is called top-down parsing. Recursive descent parsing : It is a common form of top-down parsing. It is called recursive as it uses recursive procedures to process the input. Recursive descent parsing suffers from backtracking. Backtracking : It means, if one derivation of a production fails, the syntax analyzer restarts the process using different rules of same production. This technique may process the input string more than once to determine the right production.","title":"Top-down Parsing"},{"location":"lectures/08_Ambiguity-Problems-and-Eliminating-the-Ambiguity/#bottom-up-parsing","text":"As the name suggests, bottom-up parsing starts with the input symbols and tries to construct the parse tree up to the start symbol. Example : Input string : a + b * c Production rules: S \u2192 E E \u2192 E + T E \u2192 E * T E \u2192 T T \u2192 id Let us start bottom-up parsing a + b * c Read the input and check if any production matches with the input: a + b * c T + b * c E + b * c E + T * c E * c E * T E S","title":"Bottom-up Parsing"},{"location":"lectures/09_Top-down/","text":"Top Dawn Parsing We have learnt in the last chapter that the top-down parsing technique parses the input, and starts constructing a parse tree from the root node gradually moving down to the leaf nodes. The types of top-down parsing are depicted below: Top-Down | Recursive Descent | | Back-tracking Non back-tracking | Predictive Parser | LL Parser Recursive Descent Parsing Recursive descent is a top-down parsing technique that constructs the parse tree from the top and the input is read from left to right. It uses procedures for every terminal and non-terminal entity. This parsing technique recursively parses the input to make a parse tree, which may or may not require back-tracking. But the grammar associated with it (if not left factored) cannot avoid back-tracking. A form of recursive-descent parsing that does not require any back-tracking is known as predictive parsing . This parsing technique is regarded recursive as it uses context-free grammar which is recursive in nature. Back-tracking Top- down parsers start from the root node (start symbol) and match the input string against the production rules to replace them (if matched). To understand this, take the following example of CFG: S \u2192 rXd | rZd X \u2192 oa | ea Z \u2192 ai For an input string: read, a top-down parser, will behave like this: It will start with S from the production rules and will match its yield to the left-most letter of the input, i.e. r . The very production of S (S \u2192 rXd) matches with it. So the top-down parser advances to the next input letter (i.e. e ). The parser tries to expand non-terminal X and checks its production from the left (X \u2192 oa) . It does not match with the next input symbol. So the top-down parser backtracks to obtain the next production rule of X , (X \u2192 ea) . Now the parser matches all the input letters in an ordered manner. The string is accepted. Predictive Parser Predictive parser is a recursive descent parser, which has the capability to predict which production is to be used to replace the input string. The predictive parser does not suffer from backtracking. To accomplish its tasks, the predictive parser uses a look-ahead pointer, which points to the next input symbols. To make the parser back-tracking free, the predictive parser puts some constraints on the grammar and accepts only a class of grammar known as LL(k) grammar. Predictive parsing uses a stack and a parsing table to parse the input and generate a parse tree. Both the stack and the input contains an end symbol $ to denote that the stack is empty and the input is consumed. The parser refers to the parsing table to take any decision on the input and stack element combination. In recursive descent parsing, the parser may have more than one production to choose from for a single instance of input, whereas in predictive parser, each step has at most one production to choose. There might be instances where there is no production matching the input string, making the parsing procedure to fail. LL Parser An LL Parser accepts LL grammar. LL grammar is a subset of context-free grammar but with some restrictions to get the simplified version, in order to achieve easy implementation. LL grammar can be implemented by means of both algorithms namely, recursive-descent or table-driven. LL parser is denoted as LL(k). The first L in LL(k) is parsing the input from left to right, the second L in LL(k) stands for left-most derivation and k itself represents the number of look aheads. Generally k = 1, so LL(k) may also be written as LL(1). LL Parsing Algorithm We may stick to deterministic LL(1) for parser explanation, as the size of table grows exponentially with the value of k. Secondly, if a given grammar is not LL(1), then usually, it is not LL(k), for any given k. A grammar G is LL(1) if A \u2192 \u03b1 | \u03b2 are two distinct productions of G: for no terminal, both \u03b1 and \u03b2 derive strings beginning with a. at most one of \u03b1 and \u03b2 can derive empty string. if \u03b2 \u2192 t, then \u03b1 does not derive any string beginning with a terminal in FOLLOW(A). In summary Top-down (goal driven) Start from the start non-terminal, Grow parse tree downwards to match the input word, Easier to understand and program manually. For Example input: num + num grammar: E -> E + T E -> T T -> num Step 1: E E Remaining Input: num + num Step 2: E E + T E /|\\ / | \\ E | T | | | + Remaining Input: num + num Step 3: E E + T T + T E /|\\ / | \\ E | T | | T | | + Remaining Input: num + num Step 4: E E + T T + T num + T E /|\\ / | \\ E | T | | T | | | num + Remaining Input: num + num Step 5: E E + T T + T num + T E /|\\ / | \\ E | T | | T | | | num + Remaining Input: + num Step 6: E E + T T + T num + T E /|\\ / | \\ E | T | | | T | | | | | num + num Remaining Input: num Step 7: E E + T T + T num + T E /|\\ / | \\ E | T | | | T | | | | | num + num Remaining Input: Accepted! Note: Top-Down is easier to understand and program manually ________________________1________________________ | | | 2 | 21 | | | ____3____ | ____22____ | | | | | | | | | ____7____ | | | 26 | | | | | | | | | | | 8 | __13__ | | | 27 | | | | | | | | | | | | | 9 | 14 | 18 | | | 28 | | | | | | | | | | | 4 | 10 | 15 | | | 23 | | | | | | | | | | | | | 5 6 11 12 16 17 19 20 24 25 29","title":"Unit 9 Top-down parsing"},{"location":"lectures/09_Top-down/#top-dawn-parsing","text":"","title":"Top Dawn Parsing"},{"location":"lectures/09_Top-down/#_1","text":"We have learnt in the last chapter that the top-down parsing technique parses the input, and starts constructing a parse tree from the root node gradually moving down to the leaf nodes. The types of top-down parsing are depicted below: Top-Down | Recursive Descent | | Back-tracking Non back-tracking | Predictive Parser | LL Parser","title":""},{"location":"lectures/09_Top-down/#recursive-descent-parsing","text":"Recursive descent is a top-down parsing technique that constructs the parse tree from the top and the input is read from left to right. It uses procedures for every terminal and non-terminal entity. This parsing technique recursively parses the input to make a parse tree, which may or may not require back-tracking. But the grammar associated with it (if not left factored) cannot avoid back-tracking. A form of recursive-descent parsing that does not require any back-tracking is known as predictive parsing . This parsing technique is regarded recursive as it uses context-free grammar which is recursive in nature.","title":"Recursive Descent Parsing"},{"location":"lectures/09_Top-down/#back-tracking","text":"Top- down parsers start from the root node (start symbol) and match the input string against the production rules to replace them (if matched). To understand this, take the following example of CFG: S \u2192 rXd | rZd X \u2192 oa | ea Z \u2192 ai For an input string: read, a top-down parser, will behave like this: It will start with S from the production rules and will match its yield to the left-most letter of the input, i.e. r . The very production of S (S \u2192 rXd) matches with it. So the top-down parser advances to the next input letter (i.e. e ). The parser tries to expand non-terminal X and checks its production from the left (X \u2192 oa) . It does not match with the next input symbol. So the top-down parser backtracks to obtain the next production rule of X , (X \u2192 ea) . Now the parser matches all the input letters in an ordered manner. The string is accepted.","title":"Back-tracking"},{"location":"lectures/09_Top-down/#_2","text":"","title":""},{"location":"lectures/09_Top-down/#predictive-parser","text":"Predictive parser is a recursive descent parser, which has the capability to predict which production is to be used to replace the input string. The predictive parser does not suffer from backtracking. To accomplish its tasks, the predictive parser uses a look-ahead pointer, which points to the next input symbols. To make the parser back-tracking free, the predictive parser puts some constraints on the grammar and accepts only a class of grammar known as LL(k) grammar.","title":"Predictive Parser"},{"location":"lectures/09_Top-down/#_3","text":"Predictive parsing uses a stack and a parsing table to parse the input and generate a parse tree. Both the stack and the input contains an end symbol $ to denote that the stack is empty and the input is consumed. The parser refers to the parsing table to take any decision on the input and stack element combination.","title":""},{"location":"lectures/09_Top-down/#_4","text":"In recursive descent parsing, the parser may have more than one production to choose from for a single instance of input, whereas in predictive parser, each step has at most one production to choose. There might be instances where there is no production matching the input string, making the parsing procedure to fail.","title":""},{"location":"lectures/09_Top-down/#ll-parser","text":"An LL Parser accepts LL grammar. LL grammar is a subset of context-free grammar but with some restrictions to get the simplified version, in order to achieve easy implementation. LL grammar can be implemented by means of both algorithms namely, recursive-descent or table-driven. LL parser is denoted as LL(k). The first L in LL(k) is parsing the input from left to right, the second L in LL(k) stands for left-most derivation and k itself represents the number of look aheads. Generally k = 1, so LL(k) may also be written as LL(1).","title":"LL Parser"},{"location":"lectures/09_Top-down/#_5","text":"","title":""},{"location":"lectures/09_Top-down/#ll-parsing-algorithm","text":"We may stick to deterministic LL(1) for parser explanation, as the size of table grows exponentially with the value of k. Secondly, if a given grammar is not LL(1), then usually, it is not LL(k), for any given k. A grammar G is LL(1) if A \u2192 \u03b1 | \u03b2 are two distinct productions of G: for no terminal, both \u03b1 and \u03b2 derive strings beginning with a. at most one of \u03b1 and \u03b2 can derive empty string. if \u03b2 \u2192 t, then \u03b1 does not derive any string beginning with a terminal in FOLLOW(A).","title":"LL Parsing Algorithm"},{"location":"lectures/09_Top-down/#in-summary","text":"Top-down (goal driven) Start from the start non-terminal, Grow parse tree downwards to match the input word, Easier to understand and program manually.","title":"In summary"},{"location":"lectures/09_Top-down/#for-example","text":"input: num + num grammar: E -> E + T E -> T T -> num Step 1: E E Remaining Input: num + num Step 2: E E + T E /|\\ / | \\ E | T | | | + Remaining Input: num + num Step 3: E E + T T + T E /|\\ / | \\ E | T | | T | | + Remaining Input: num + num Step 4: E E + T T + T num + T E /|\\ / | \\ E | T | | T | | | num + Remaining Input: num + num Step 5: E E + T T + T num + T E /|\\ / | \\ E | T | | T | | | num + Remaining Input: + num Step 6: E E + T T + T num + T E /|\\ / | \\ E | T | | | T | | | | | num + num Remaining Input: num Step 7: E E + T T + T num + T E /|\\ / | \\ E | T | | | T | | | | | num + num Remaining Input: Accepted! Note: Top-Down is easier to understand and program manually","title":"For Example"},{"location":"lectures/09_Top-down/#_6","text":"________________________1________________________ | | | 2 | 21 | | | ____3____ | ____22____ | | | | | | | | | ____7____ | | | 26 | | | | | | | | | | | 8 | __13__ | | | 27 | | | | | | | | | | | | | 9 | 14 | 18 | | | 28 | | | | | | | | | | | 4 | 10 | 15 | | | 23 | | | | | | | | | | | | | 5 6 11 12 16 17 19 20 24 25 29","title":""},{"location":"lectures/10_Bottom-up_I/","text":"Bottom down Parsing Bottom-up parsing starts from the leaf nodes of a tree and works in upward direction till it reaches the root node. Here, we start from a sentence and then apply production rules in reverse manner in order to reach the start symbol. The image given below depicts the bottom-up parsers available. Shift-Reduce Parsing Shift-reduce parsing uses two unique steps for bottom-up parsing. These steps are known as shift-step and reduce-step. Shift step: The shift step refers to the advancement of the input pointer to the next input symbol, which is called the shifted symbol. This symbol is pushed onto the stack. The shifted symbol is treated as a single node of the parse tree. Reduce step : When the parser finds a complete grammar rule (RHS) and replaces it to (LHS), it is known as reduce-step. This occurs when the top of the stack contains a handle. To reduce, a POP function is performed on the stack which pops off the handle and replaces it with LHS non-terminal symbol. LR Parser The LR parser is a non-recursive, shift-reduce, bottom-up parser. It uses a wide class of context-free grammar which makes it the most efficient syntax analysis technique. LR parsers are also known as LR(k) parsers, where L stands for left-to-right scanning of the input stream; R stands for the construction of right-most derivation in reverse, and k denotes the number of lookahead symbols to make decisions. There are three widely used algorithms available for constructing an LR parser: SLR(1) \u2013 Simple LR Parser: Works on smallest class of grammar Few number of states, hence very small table Simple and fast construction LR(1) \u2013 LR Parser: Works on complete set of LR(1) Grammar Generates large table and large number of states Slow construction LALR(1) \u2013 Look-Ahead LR Parser: Works on intermediate size of grammar Number of states are same as in SLR(1) LL LR Does a leftmost derivation. Does a rightmost derivation in reverse. Starts with the root nonterminal on the stack. Ends with the root nonterminal on the stack. Ends when the stack is empty. Starts with an empty stack. Uses the stack for designating what is still to be expected. Uses the stack for designating what is already seen. Builds the parse tree top-down. Builds the parse tree bottom-up. Continuously pops a nonterminal off the stack, and pushes the corresponding right hand side. Tries to recognize a right hand side on the stack, pops it, and pushes the corresponding nonterminal. Expands the non-terminals. Reduces the non-terminals. Reads the terminals when it pops one off the stack. Reads the terminals while it pushes them on the stack. Pre-order traversal of the parse tree. Post-order traversal of the parse tree. in summary Bottom-up (data driven) Start from the input word, Build up parse tree which has start non-terminal as root, More powerful and used by most parser generators. For Example input: num + num grammar: E -> E + T E -> T T -> num Step 1: num + num num Remaining Input: num + num Step 2: T + num num + num T | num Remaining Input: + num Step 3: E + num T + num num + num E | T | num Remaining Input: + num Step 4: E + T E + num T + num num + num E T | | T | | | num + num Remaining Input: Step 5: E E + T E + num T + num num + num E / | \\ E | T | | | T | | | | | num + num Remaining Input: Accepted Note: Bottom-Up Don't need to figure out as much of the parse tree for a given amount of input (more powerful) ________________________29_______________________ | | | 18 | 28 | | | ____17____ | ____27____ | | | | | | | | | ____16____ | | | 26 | | | | | | | | | | | 7 | __15__ | | | 25 | | | | | | | | | | | | | 6 | 11 | 14 | | | 24 | | | | | | | | | | | 2 | 5 | 10 | | | 21 | | | | | | | | | | | | | 1 3 4 8 9 12 13 19 20 22 23 parsing Complexity For certain classes of constrained CFGs, we can always parse in linear time LL parsers (Use a top-down strategy) LR parsers (Use a Bottom-up strategy) The first L means the parser reads input from L eft to right without backing up LL: Left-to-right scan, Leftmost dervation LR: Left-to-right scan, rightmost dervation in revese Any ambiguous CFG can neithr be LL nor LR Deterministic: they produs a single correct parse without guessing or backtracking","title":"Unit 10 Bottom-up parsing (I)"},{"location":"lectures/10_Bottom-up_I/#bottom-down-parsing","text":"","title":"Bottom down Parsing"},{"location":"lectures/10_Bottom-up_I/#_1","text":"Bottom-up parsing starts from the leaf nodes of a tree and works in upward direction till it reaches the root node. Here, we start from a sentence and then apply production rules in reverse manner in order to reach the start symbol. The image given below depicts the bottom-up parsers available.","title":""},{"location":"lectures/10_Bottom-up_I/#shift-reduce-parsing","text":"Shift-reduce parsing uses two unique steps for bottom-up parsing. These steps are known as shift-step and reduce-step. Shift step: The shift step refers to the advancement of the input pointer to the next input symbol, which is called the shifted symbol. This symbol is pushed onto the stack. The shifted symbol is treated as a single node of the parse tree. Reduce step : When the parser finds a complete grammar rule (RHS) and replaces it to (LHS), it is known as reduce-step. This occurs when the top of the stack contains a handle. To reduce, a POP function is performed on the stack which pops off the handle and replaces it with LHS non-terminal symbol.","title":"Shift-Reduce Parsing"},{"location":"lectures/10_Bottom-up_I/#lr-parser","text":"The LR parser is a non-recursive, shift-reduce, bottom-up parser. It uses a wide class of context-free grammar which makes it the most efficient syntax analysis technique. LR parsers are also known as LR(k) parsers, where L stands for left-to-right scanning of the input stream; R stands for the construction of right-most derivation in reverse, and k denotes the number of lookahead symbols to make decisions. There are three widely used algorithms available for constructing an LR parser: SLR(1) \u2013 Simple LR Parser: Works on smallest class of grammar Few number of states, hence very small table Simple and fast construction LR(1) \u2013 LR Parser: Works on complete set of LR(1) Grammar Generates large table and large number of states Slow construction LALR(1) \u2013 Look-Ahead LR Parser: Works on intermediate size of grammar Number of states are same as in SLR(1) LL LR Does a leftmost derivation. Does a rightmost derivation in reverse. Starts with the root nonterminal on the stack. Ends with the root nonterminal on the stack. Ends when the stack is empty. Starts with an empty stack. Uses the stack for designating what is still to be expected. Uses the stack for designating what is already seen. Builds the parse tree top-down. Builds the parse tree bottom-up. Continuously pops a nonterminal off the stack, and pushes the corresponding right hand side. Tries to recognize a right hand side on the stack, pops it, and pushes the corresponding nonterminal. Expands the non-terminals. Reduces the non-terminals. Reads the terminals when it pops one off the stack. Reads the terminals while it pushes them on the stack. Pre-order traversal of the parse tree. Post-order traversal of the parse tree.","title":"LR Parser"},{"location":"lectures/10_Bottom-up_I/#in-summary","text":"Bottom-up (data driven) Start from the input word, Build up parse tree which has start non-terminal as root, More powerful and used by most parser generators.","title":"in summary"},{"location":"lectures/10_Bottom-up_I/#for-example","text":"input: num + num grammar: E -> E + T E -> T T -> num Step 1: num + num num Remaining Input: num + num Step 2: T + num num + num T | num Remaining Input: + num Step 3: E + num T + num num + num E | T | num Remaining Input: + num Step 4: E + T E + num T + num num + num E T | | T | | | num + num Remaining Input: Step 5: E E + T E + num T + num num + num E / | \\ E | T | | | T | | | | | num + num Remaining Input: Accepted Note: Bottom-Up Don't need to figure out as much of the parse tree for a given amount of input (more powerful)","title":"For Example"},{"location":"lectures/10_Bottom-up_I/#_2","text":"________________________29_______________________ | | | 18 | 28 | | | ____17____ | ____27____ | | | | | | | | | ____16____ | | | 26 | | | | | | | | | | | 7 | __15__ | | | 25 | | | | | | | | | | | | | 6 | 11 | 14 | | | 24 | | | | | | | | | | | 2 | 5 | 10 | | | 21 | | | | | | | | | | | | | 1 3 4 8 9 12 13 19 20 22 23","title":""},{"location":"lectures/10_Bottom-up_I/#parsing-complexity","text":"For certain classes of constrained CFGs, we can always parse in linear time LL parsers (Use a top-down strategy) LR parsers (Use a Bottom-up strategy) The first L means the parser reads input from L eft to right without backing up LL: Left-to-right scan, Leftmost dervation LR: Left-to-right scan, rightmost dervation in revese Any ambiguous CFG can neithr be LL nor LR Deterministic: they produs a single correct parse without guessing or backtracking","title":"parsing Complexity"},{"location":"lectures/10_Bottom-up_I/#_3","text":"","title":""},{"location":"lectures/11_Bottom-up_II/","text":"Bottom-Up Parsing Process Bottom-up parsing is the process of reducing an input string to the start symbol S of the grammar. At each reduction step, a specific substring matching the body of a production (RHS) is replaced by the nonterminal at the head of that production (LHS). The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. Handle Pruning Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. A \"handle\" is a substring that matches the body of a production, and whose reduction represents one step along the reverse of a rightmost derivation. Shift-Reduce Parsing Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. Primary operations are shift and reduce. During a left-to-right scan of the input string, the parser shifts zero or more input symbols onto the stack, until it is ready to reduce a string of grammar symbols on top of the stack. It then reduces to the head of the appropriate production with number Rx. The parser repeats this cycle until it has detected an error or until the stack contains the start symbol and the input is empty. Shift-Reduce Parsing Notations We use $ to mark the bottom of the stack and also the right end of the input. Split string into two substrings, right and left, the dividing point is marked by a |. Right substring is still not examined by parser (a string of terminals). Left substring has both terminals and non-terminals. Shift-Reduce Parsing Example Let's consider the following productions: r1 S->E r4 E->T r2 E ->E+T r3 T->(E) r5 T ->num And the input string |num+(num+num)$ . The parsing process would go as follows: |num+(num+num) shift num|+(num+num) reduce(r5) T|+(num+num) reduce(r4) E|+(num+num) shift E+|(num+num) shift E+(|num+num) shift E+(num|+num) reduce(r5) E+(T|+num) reduce(r4) E+(E|+num) shift E+(E+|num) shift E+((E+num|) reduce(r5) E+(E+T|) reduce(r2) E+(E|) shift E+(E) reduce(r3) E+T reduce(r2) E| reduce(r1) S| Here, shift means pushing the next input symbol onto the top of the stack, and reduce means popping zero or more symbols off the stack (production RHS) and pushing a non-terminal on the stack (production LHS) Shift-Reduce Parsing Stack Left substring can be implemented by a stack. Top of the stack is the | symbol. Shift pushes a terminal on the stack. Reduce pops Zero or more symbols of the stack (production RHS) and pushes a non-terminal on the stack (production LHS). Conflicts During Shift-Reduce Parsing There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide: Whether to shift or to reduce (a shift/reduce conflict) Or cannot decide which of several reductions to make (a reduce/reduce conflict) Final Thoughts Remember, mastering these concepts takes time and practice. Don't hesitate to ask questions or seek clarification if something is unclear. Happy studying! Compiler Design Lesson: Bottom-Up Parsing Process and LR Parsing Introduction Bottom-up parsing is a fundamental concept in compiler design. It involves reducing an input string to the start symbol S of the grammar. At each reduction step, a specific substring matching the body of a production (RHS) is replaced by the nonterminal at the head of that production (LHS). The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. LR Parsing and LR Grammars LR parsing is the most prevalent type of bottom-up parser today. The term \"LR\" stands for left-to-right scanning of the input and constructing a rightmost derivation in reverse. The \"k\" in LR(k) refers to the number of input symbols of lookahead that are used in making parsing decisions. The cases where k = 0 or k = 1 are of practical interest. When (k) is omitted, k is assumed to be 1. LR parsers are table-driven, similar to nonrecursive LL parsers. LR(k) Parsers and Grammars A grammar for which we can construct a parsing table using an LR parsing algorithm is said to be an LR grammar. The class of grammars that can be parsed using LR methods is a proper superset of the class of grammars that can be parsed with predictive or LL methods. For a grammar to be LL(k), we must be able to recognize the use of a production by seeing only the first k symbols of what its right side derives. For a grammar to be LR(k), we must be able to recognize the occurrence of the right side of a production in a right-sentential form, with k input symbols of lookahead. This requirement is far less stringent than the one for LL(k) grammars. The principal drawback of the LR method is that it is too much work to construct an LR parser by hand for a typical programming-language grammar. A specialized tool, an LR parser generator, is needed. Variants of LR Parsers There are several variants of LR parsers: SLR parsers, LALR parsers, canonical LR(1) parsers, minimal LR(1) parsers, and generalized LR parsers (GLR parsers). LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages. An LR parser reads input text from left to right without backing up and produces a rightmost derivation in reverse. The name \"LR\" is often followed by a numeric qualifier, as in \"LR(1)\" or sometimes \"LR(k)\". To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. Conclusion Mastering these concepts takes time and practice. Understanding the difference between LL and LR parsing, and the various types of LR parsers, is crucial for effective compiler design. Don't hesitate to ask questions or seek clarification if something is unclear. Happy studying!","title":"Unit 11 Bottom-up parsing (II)"},{"location":"lectures/11_Bottom-up_II/#bottom-up-parsing-process","text":"","title":"Bottom-Up Parsing Process"},{"location":"lectures/11_Bottom-up_II/#_1","text":"Bottom-up parsing is the process of reducing an input string to the start symbol S of the grammar. At each reduction step, a specific substring matching the body of a production (RHS) is replaced by the nonterminal at the head of that production (LHS). The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds.","title":""},{"location":"lectures/11_Bottom-up_II/#handle-pruning","text":"Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. A \"handle\" is a substring that matches the body of a production, and whose reduction represents one step along the reverse of a rightmost derivation.","title":"Handle Pruning"},{"location":"lectures/11_Bottom-up_II/#shift-reduce-parsing","text":"Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. Primary operations are shift and reduce. During a left-to-right scan of the input string, the parser shifts zero or more input symbols onto the stack, until it is ready to reduce a string of grammar symbols on top of the stack. It then reduces to the head of the appropriate production with number Rx. The parser repeats this cycle until it has detected an error or until the stack contains the start symbol and the input is empty.","title":"Shift-Reduce Parsing"},{"location":"lectures/11_Bottom-up_II/#shift-reduce-parsing-notations","text":"We use $ to mark the bottom of the stack and also the right end of the input. Split string into two substrings, right and left, the dividing point is marked by a |. Right substring is still not examined by parser (a string of terminals). Left substring has both terminals and non-terminals.","title":"Shift-Reduce Parsing Notations"},{"location":"lectures/11_Bottom-up_II/#shift-reduce-parsing-example","text":"Let's consider the following productions: r1 S->E r4 E->T r2 E ->E+T r3 T->(E) r5 T ->num And the input string |num+(num+num)$ . The parsing process would go as follows: |num+(num+num) shift num|+(num+num) reduce(r5) T|+(num+num) reduce(r4) E|+(num+num) shift E+|(num+num) shift E+(|num+num) shift E+(num|+num) reduce(r5) E+(T|+num) reduce(r4) E+(E|+num) shift E+(E+|num) shift E+((E+num|) reduce(r5) E+(E+T|) reduce(r2) E+(E|) shift E+(E) reduce(r3) E+T reduce(r2) E| reduce(r1) S| Here, shift means pushing the next input symbol onto the top of the stack, and reduce means popping zero or more symbols off the stack (production RHS) and pushing a non-terminal on the stack (production LHS)","title":"Shift-Reduce Parsing Example"},{"location":"lectures/11_Bottom-up_II/#shift-reduce-parsing-stack","text":"Left substring can be implemented by a stack. Top of the stack is the | symbol. Shift pushes a terminal on the stack. Reduce pops Zero or more symbols of the stack (production RHS) and pushes a non-terminal on the stack (production LHS).","title":"Shift-Reduce Parsing Stack"},{"location":"lectures/11_Bottom-up_II/#conflicts-during-shift-reduce-parsing","text":"There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide: Whether to shift or to reduce (a shift/reduce conflict) Or cannot decide which of several reductions to make (a reduce/reduce conflict)","title":"Conflicts During Shift-Reduce Parsing"},{"location":"lectures/11_Bottom-up_II/#final-thoughts","text":"Remember, mastering these concepts takes time and practice. Don't hesitate to ask questions or seek clarification if something is unclear. Happy studying!","title":"Final Thoughts"},{"location":"lectures/11_Bottom-up_II/#compiler-design-lesson-bottom-up-parsing-process-and-lr-parsing","text":"","title":"Compiler Design Lesson: Bottom-Up Parsing Process and LR Parsing"},{"location":"lectures/11_Bottom-up_II/#introduction","text":"Bottom-up parsing is a fundamental concept in compiler design. It involves reducing an input string to the start symbol S of the grammar. At each reduction step, a specific substring matching the body of a production (RHS) is replaced by the nonterminal at the head of that production (LHS). The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds.","title":"Introduction"},{"location":"lectures/11_Bottom-up_II/#lr-parsing-and-lr-grammars","text":"LR parsing is the most prevalent type of bottom-up parser today. The term \"LR\" stands for left-to-right scanning of the input and constructing a rightmost derivation in reverse. The \"k\" in LR(k) refers to the number of input symbols of lookahead that are used in making parsing decisions. The cases where k = 0 or k = 1 are of practical interest. When (k) is omitted, k is assumed to be 1. LR parsers are table-driven, similar to nonrecursive LL parsers.","title":"LR Parsing and LR Grammars"},{"location":"lectures/11_Bottom-up_II/#lrk-parsers-and-grammars","text":"A grammar for which we can construct a parsing table using an LR parsing algorithm is said to be an LR grammar. The class of grammars that can be parsed using LR methods is a proper superset of the class of grammars that can be parsed with predictive or LL methods. For a grammar to be LL(k), we must be able to recognize the use of a production by seeing only the first k symbols of what its right side derives. For a grammar to be LR(k), we must be able to recognize the occurrence of the right side of a production in a right-sentential form, with k input symbols of lookahead. This requirement is far less stringent than the one for LL(k) grammars. The principal drawback of the LR method is that it is too much work to construct an LR parser by hand for a typical programming-language grammar. A specialized tool, an LR parser generator, is needed.","title":"LR(k) Parsers and Grammars"},{"location":"lectures/11_Bottom-up_II/#variants-of-lr-parsers","text":"There are several variants of LR parsers: SLR parsers, LALR parsers, canonical LR(1) parsers, minimal LR(1) parsers, and generalized LR parsers (GLR parsers). LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages. An LR parser reads input text from left to right without backing up and produces a rightmost derivation in reverse. The name \"LR\" is often followed by a numeric qualifier, as in \"LR(1)\" or sometimes \"LR(k)\". To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned.","title":"Variants of LR Parsers"},{"location":"lectures/11_Bottom-up_II/#conclusion","text":"Mastering these concepts takes time and practice. Understanding the difference between LL and LR parsing, and the various types of LR parsers, is crucial for effective compiler design. Don't hesitate to ask questions or seek clarification if something is unclear. Happy studying!","title":"Conclusion"},{"location":"lectures/12_LR0_parsing/","text":"Expanding the Concepts Representing Item Sets In the context of LR(0) parsing, an item set represents the current state of the parser. Each item in the set corresponds to a production in the grammar, with a dot indicating the current position in the production. For example, if we have a production A -> BC , there will be four items corresponding to this production: A -> .BC A -> B.C A -> BC. A -> B.C. These items represent the different stages of recognizing the production A -> BC . Closure and Goto of Item Sets The closure operation takes a set of items and produces a new set containing all items that can be derived from the original set. This is done through a series of steps: Add every item in the original set to the closure set. For each item in the closure set that is of the form A -> \u03b1.B\u03b2 , check if there is a production B -> \u03b3 in the grammar. If so, add the item B -> .\u03b3\u03b2 to the closure set. Repeat step 2 until no more new items can be added to the closure set. The GOTO operation, on the other hand, is used to determine the next state based on the current state and the next input symbol. It is defined as the closure of the set of all items of the form A -> \u03b1B.\u03b2 , where A -> \u03b1.B\u03b2 is in the current state. Closure and Goto of Item Sets: Example Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id We start with the initial state I0 = {E -> .E + T} . We apply the closure operation to get the closure of I0 : I0 = {E -> .E + T, E -> E + .T} Then, we apply the GOTO operation with the input symbol E to get the next state: GOTO(I0, E) = {E -> E + .T, E -> .T} We continue this process until we reach the final state, which indicates that the input string is valid according to the grammar. Canonical Collection of Sets of LR(0) Items To construct the parsing table, we need to compute the canonical collection of sets of LR(0) items for an augmented grammar. This involves creating a set of states, where each state represents a set of items. The transitions between states are determined by the GOTO operations. LR(0) Item Sets: Example Let's consider a slightly more complex grammar: S -> CC C -> aC | d We start by augmenting the grammar to handle left recursion: S' -> SC' C' -> aC' | d We then compute the item set for the augmented grammar: I0 = {S' -> .SC', C' -> .aC' | d} We continue this process until we reach the final state. Structure of the LR Parsing Table The LR parsing table consists of two parts: a parsing-action function ACTION and a goto function GOTO . The ACTION function determines what action the parser should take based on the current state and the next input symbol. The GOTO function determines the next state based on the current state and the next input symbol. LR(0) Parsing: Example Consider the following parsing table generated from the grammar E -> E + T | T : ACTION GOTO State int+;()ET 0 s9 s8,13 1 Accept 2 Reduce E -> T + E 3 s5,s4 4 Reduce E -> T; 5 s9,s8,23 6 Reduce T -> (E) 7 s6 8 s9,s8,73 9 Reduce T -> int This table shows that if the parser is in state 0 and reads an integer, it shifts the integer and moves to state 9. If it reads a plus sign, it reduces E -> T and stays in state 1. If it reads a left parenthesis, it reduces T -> (E) and moves to state 6. And so on. LR(0) DFA Construction: Example Let's consider a slightly more complex grammar: S -> E E -> T; E -> T + E T -> int T -> (E) We start by adding the initial item S -> .E to the initial state: State 0: S -> .E We then Structure of the LR Parsing Table: Example Let's consider the grammar G' from the previous example: (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d The parsing table for this grammar would be constructed based on the item sets and the GOTO and ACTION functions. The exact contents of the table would depend on the specific implementation of the LR parser, but it might look something like this: State Input Action Next State 0 S' Shift 1 1 C Shift 2 2 a Shift 2 2 d Reduce ... ... ... ... The 'Action' column indicates whether the parser should shift (read the next input symbol and push it onto the stack), reduce (apply a grammar rule to the top of the stack), or go to (move to a new state without consuming any input symbols). LR-parsing algorithm i The LR-parsing algorithm works as follows: Initialize the stack with the start symbol of the grammar and the special end marker $ . Read the first input symbol. Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform. Perform the action: if it's a shift, push the input symbol onto the stack and move to the next state. If it's a reduce, replace the top of the stack with the left-hand side of the grammar rule. If it's a go to, move to the next state without changing the stack. Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker. LR-parsing algorithm ii The second LR-parsing algorithm is similar to the first one, but instead of using the ACTION field of the parsing table, it uses the GOTO field to determine the next state. This allows the parser to avoid reading the next input symbol until it knows whether it needs to shift or go to. LR(0) parsing table: Example The LR(0) parsing table provides the same functionality as the LR parsing algorithms described above, but in a more compact form. It consists of two parts: the ACTION table and the GOTO table. The ACTION table maps each state and input symbol to an action (shift, reduce, accept, or error). The GOTO table maps each state and grammar symbol to a next state. Here is an example of an LR(0) parsing table for a simple grammar: State Input Action Next State 0 a Shift 1 1 b Reduce 0 b Error This table tells us that if we are in state 0 and read an 'a', we should shift it onto the stack and move to state 1. If we are in state 1 and read a 'b', we should reduce by the rule A -> aB . If we are in state 0 and read a 'b', we should report an error. LR(0) Parsing: Example Let's consider the following parsing table for the grammar G : (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d And the example input add$ . We start by initializing the stack with the start symbol S' and the end marker $ . Then we read the first input symbol 'a'. According to the parsing table, we shift 'a' onto the stack and move to state 1. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected. All LR parsers (LR(0), LR(1), LALR(1), and SLR(1)) behave in this fashion All LR parsers follow a similar process: they initialize the stack with the start symbol and the end marker, then read the input symbols one by one, looking up actions in the parsing table to decide whether to shift, reduce, or go to. The only difference between them lies in the construction of the parsing table, which depends on the specific variant of LR parsing being used. LR(0) Parsing LR(0) parsing is a type of bottom-up parsing method used in compiler design. It uses a finite automaton to guide the parsing process. The LR(0) parser reads the input symbols from left to right (L), applies the productions in reverse order (R), and looks ahead zero symbols (0) to decide what action to take. The LR(0) parser uses two tables during the parsing process: the ACTION table and the GOTO table. The ACTION table maps each state and input symbol to an action (shift, reduce, accept, or error). The GOTO table maps each state and grammar symbol to a next state. LR(0) Parser Scope The scope of an LR(0) parser refers to the set of languages it can recognize. An LR(0) parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion. LR(0) Limitations One limitation of LR(0) parsing is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities. LR(0) Parsing Table with Conflicts Conflicts in the LR(0) parsing table occur when there are multiple possible actions for a given state and input symbol. These conflicts must be resolved before the parsing table can be used effectively. One common way to resolve conflicts is by using operator precedence and associativity rules, or by transforming the grammar to remove the ambiguity. LR(0) Grammar: Exercises To construct the LR(0) parsing table for a given grammar, you would typically follow these steps: Augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker $ . Compute the closure of the set of items derived from the new start symbol. Repeat step 2 for each new state until no more new states can be added. For each state and input symbol, determine the action to take (shift, reduce, or go to) based on the transitions in the state machine. Write the resulting actions into the ACTION and GOTO fields of the parsing table. The exercise asks you to construct the LR(0) parsing table for several different grammars. You would need to follow the steps above for each grammar. Which one is LR(0)? To determine whether a grammar is suitable for LR(0) parsing, you would need to check whether the grammar is unambiguous and does not contain left recursion. If both conditions are met, then the grammar is suitable for LR(0) parsing. From the given grammars, the ones that are suitable for LR(0) parsing are: S \u2192 AB S \u2192 Ab A \u2192 \u03b5 B \u2192 b S \u2192 A S \u2192 aa A \u2192 a The remaining grammars either contain left recursion ( S \u2192 AA and A \u2192 aA ) or are ambiguous ( S \u2192 A ), making them unsuitable for LR(0) parsing.","title":"Unit 12 LR(0) parsing"},{"location":"lectures/12_LR0_parsing/#expanding-the-concepts","text":"","title":"Expanding the Concepts"},{"location":"lectures/12_LR0_parsing/#_1","text":"","title":""},{"location":"lectures/12_LR0_parsing/#representing-item-sets","text":"In the context of LR(0) parsing, an item set represents the current state of the parser. Each item in the set corresponds to a production in the grammar, with a dot indicating the current position in the production. For example, if we have a production A -> BC , there will be four items corresponding to this production: A -> .BC A -> B.C A -> BC. A -> B.C. These items represent the different stages of recognizing the production A -> BC .","title":"Representing Item Sets"},{"location":"lectures/12_LR0_parsing/#closure-and-goto-of-item-sets","text":"The closure operation takes a set of items and produces a new set containing all items that can be derived from the original set. This is done through a series of steps: Add every item in the original set to the closure set. For each item in the closure set that is of the form A -> \u03b1.B\u03b2 , check if there is a production B -> \u03b3 in the grammar. If so, add the item B -> .\u03b3\u03b2 to the closure set. Repeat step 2 until no more new items can be added to the closure set. The GOTO operation, on the other hand, is used to determine the next state based on the current state and the next input symbol. It is defined as the closure of the set of all items of the form A -> \u03b1B.\u03b2 , where A -> \u03b1.B\u03b2 is in the current state.","title":"Closure and Goto of Item Sets"},{"location":"lectures/12_LR0_parsing/#closure-and-goto-of-item-sets-example","text":"Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id We start with the initial state I0 = {E -> .E + T} . We apply the closure operation to get the closure of I0 : I0 = {E -> .E + T, E -> E + .T} Then, we apply the GOTO operation with the input symbol E to get the next state: GOTO(I0, E) = {E -> E + .T, E -> .T} We continue this process until we reach the final state, which indicates that the input string is valid according to the grammar.","title":"Closure and Goto of Item Sets: Example"},{"location":"lectures/12_LR0_parsing/#canonical-collection-of-sets-of-lr0-items","text":"To construct the parsing table, we need to compute the canonical collection of sets of LR(0) items for an augmented grammar. This involves creating a set of states, where each state represents a set of items. The transitions between states are determined by the GOTO operations.","title":"Canonical Collection of Sets of LR(0) Items"},{"location":"lectures/12_LR0_parsing/#lr0-item-sets-example","text":"Let's consider a slightly more complex grammar: S -> CC C -> aC | d We start by augmenting the grammar to handle left recursion: S' -> SC' C' -> aC' | d We then compute the item set for the augmented grammar: I0 = {S' -> .SC', C' -> .aC' | d} We continue this process until we reach the final state.","title":"LR(0) Item Sets: Example"},{"location":"lectures/12_LR0_parsing/#structure-of-the-lr-parsing-table","text":"The LR parsing table consists of two parts: a parsing-action function ACTION and a goto function GOTO . The ACTION function determines what action the parser should take based on the current state and the next input symbol. The GOTO function determines the next state based on the current state and the next input symbol.","title":"Structure of the LR Parsing Table"},{"location":"lectures/12_LR0_parsing/#lr0-parsing-example","text":"Consider the following parsing table generated from the grammar E -> E + T | T : ACTION GOTO State int+;()ET 0 s9 s8,13 1 Accept 2 Reduce E -> T + E 3 s5,s4 4 Reduce E -> T; 5 s9,s8,23 6 Reduce T -> (E) 7 s6 8 s9,s8,73 9 Reduce T -> int This table shows that if the parser is in state 0 and reads an integer, it shifts the integer and moves to state 9. If it reads a plus sign, it reduces E -> T and stays in state 1. If it reads a left parenthesis, it reduces T -> (E) and moves to state 6. And so on.","title":"LR(0) Parsing: Example"},{"location":"lectures/12_LR0_parsing/#lr0-dfa-construction-example","text":"Let's consider a slightly more complex grammar: S -> E E -> T; E -> T + E T -> int T -> (E) We start by adding the initial item S -> .E to the initial state: State 0: S -> .E We then","title":"LR(0) DFA Construction: Example"},{"location":"lectures/12_LR0_parsing/#structure-of-the-lr-parsing-table-example","text":"Let's consider the grammar G' from the previous example: (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d The parsing table for this grammar would be constructed based on the item sets and the GOTO and ACTION functions. The exact contents of the table would depend on the specific implementation of the LR parser, but it might look something like this: State Input Action Next State 0 S' Shift 1 1 C Shift 2 2 a Shift 2 2 d Reduce ... ... ... ... The 'Action' column indicates whether the parser should shift (read the next input symbol and push it onto the stack), reduce (apply a grammar rule to the top of the stack), or go to (move to a new state without consuming any input symbols).","title":"Structure of the LR Parsing Table: Example"},{"location":"lectures/12_LR0_parsing/#lr-parsing-algorithm-i","text":"The LR-parsing algorithm works as follows: Initialize the stack with the start symbol of the grammar and the special end marker $ . Read the first input symbol. Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform. Perform the action: if it's a shift, push the input symbol onto the stack and move to the next state. If it's a reduce, replace the top of the stack with the left-hand side of the grammar rule. If it's a go to, move to the next state without changing the stack. Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker.","title":"LR-parsing algorithm i"},{"location":"lectures/12_LR0_parsing/#lr-parsing-algorithm-ii","text":"The second LR-parsing algorithm is similar to the first one, but instead of using the ACTION field of the parsing table, it uses the GOTO field to determine the next state. This allows the parser to avoid reading the next input symbol until it knows whether it needs to shift or go to.","title":"LR-parsing algorithm ii"},{"location":"lectures/12_LR0_parsing/#lr0-parsing-table-example","text":"The LR(0) parsing table provides the same functionality as the LR parsing algorithms described above, but in a more compact form. It consists of two parts: the ACTION table and the GOTO table. The ACTION table maps each state and input symbol to an action (shift, reduce, accept, or error). The GOTO table maps each state and grammar symbol to a next state. Here is an example of an LR(0) parsing table for a simple grammar: State Input Action Next State 0 a Shift 1 1 b Reduce 0 b Error This table tells us that if we are in state 0 and read an 'a', we should shift it onto the stack and move to state 1. If we are in state 1 and read a 'b', we should reduce by the rule A -> aB . If we are in state 0 and read a 'b', we should report an error.","title":"LR(0) parsing table: Example"},{"location":"lectures/12_LR0_parsing/#lr0-parsing-example_1","text":"Let's consider the following parsing table for the grammar G : (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d And the example input add$ . We start by initializing the stack with the start symbol S' and the end marker $ . Then we read the first input symbol 'a'. According to the parsing table, we shift 'a' onto the stack and move to state 1. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.","title":"LR(0) Parsing: Example"},{"location":"lectures/12_LR0_parsing/#all-lr-parsers-lr0-lr1-lalr1-and-slr1-behave-in-this-fashion","text":"All LR parsers follow a similar process: they initialize the stack with the start symbol and the end marker, then read the input symbols one by one, looking up actions in the parsing table to decide whether to shift, reduce, or go to. The only difference between them lies in the construction of the parsing table, which depends on the specific variant of LR parsing being used.","title":"All LR parsers (LR(0), LR(1), LALR(1), and SLR(1)) behave in this fashion"},{"location":"lectures/12_LR0_parsing/#lr0-parsing","text":"LR(0) parsing is a type of bottom-up parsing method used in compiler design. It uses a finite automaton to guide the parsing process. The LR(0) parser reads the input symbols from left to right (L), applies the productions in reverse order (R), and looks ahead zero symbols (0) to decide what action to take. The LR(0) parser uses two tables during the parsing process: the ACTION table and the GOTO table. The ACTION table maps each state and input symbol to an action (shift, reduce, accept, or error). The GOTO table maps each state and grammar symbol to a next state.","title":"LR(0) Parsing"},{"location":"lectures/12_LR0_parsing/#lr0-parser-scope","text":"The scope of an LR(0) parser refers to the set of languages it can recognize. An LR(0) parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion.","title":"LR(0) Parser Scope"},{"location":"lectures/12_LR0_parsing/#lr0-limitations","text":"One limitation of LR(0) parsing is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities.","title":"LR(0) Limitations"},{"location":"lectures/12_LR0_parsing/#lr0-parsing-table-with-conflicts","text":"Conflicts in the LR(0) parsing table occur when there are multiple possible actions for a given state and input symbol. These conflicts must be resolved before the parsing table can be used effectively. One common way to resolve conflicts is by using operator precedence and associativity rules, or by transforming the grammar to remove the ambiguity.","title":"LR(0) Parsing Table with Conflicts"},{"location":"lectures/12_LR0_parsing/#lr0-grammar-exercises","text":"To construct the LR(0) parsing table for a given grammar, you would typically follow these steps: Augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker $ . Compute the closure of the set of items derived from the new start symbol. Repeat step 2 for each new state until no more new states can be added. For each state and input symbol, determine the action to take (shift, reduce, or go to) based on the transitions in the state machine. Write the resulting actions into the ACTION and GOTO fields of the parsing table. The exercise asks you to construct the LR(0) parsing table for several different grammars. You would need to follow the steps above for each grammar.","title":"LR(0) Grammar: Exercises"},{"location":"lectures/12_LR0_parsing/#which-one-is-lr0","text":"To determine whether a grammar is suitable for LR(0) parsing, you would need to check whether the grammar is unambiguous and does not contain left recursion. If both conditions are met, then the grammar is suitable for LR(0) parsing. From the given grammars, the ones that are suitable for LR(0) parsing are: S \u2192 AB S \u2192 Ab A \u2192 \u03b5 B \u2192 b S \u2192 A S \u2192 aa A \u2192 a The remaining grammars either contain left recursion ( S \u2192 AA and A \u2192 aA ) or are ambiguous ( S \u2192 A ), making them unsuitable for LR(0) parsing.","title":"Which one is LR(0)?"},{"location":"lectures/13_SLR_parsing/","text":"Simple LR Parsing (SLR or SLR(1)) Simple LR parsing, also known as SLR or SLR(1), is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. For each reduction item A \u2192 \u03b3\u00b7 , the parser looks at the lookahead symbol c . It applies the reduction only if c is in the FOLLOW(A) set, which contains all the terminal symbols that can appear immediately after A in a sentential form derived from the start symbol. SLR Parsing Table The SLR parsing table eliminates some conflicts compared to the LR(0) table. It is essentially the same as the LR(0) table, but with reduced rows. Reductions do not fill entire rows. Instead, reductions A \u2192 \u03b3\u00b7 are added only in the columns of symbols in FOLLOW(A) . SLR Grammar An SLR grammar is a context-free grammar for which the SLR parsing table does not have any conflicts. In other words, an SLR grammar is one that can be parsed by an SLR parser without encountering any shift/reduce or reduce/reduce conflicts. SLR Parsing: Example The SLR parsing algorithm is similar to the LR(0) parsing algorithm, but with the addition of considering the lookahead symbol when deciding whether to shift or reduce. Here is a simplified version of the algorithm: Initialize the stack with the start symbol of the grammar and the special end marker $ . Read the first input symbol. Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform. If the action is a shift, push the input symbol onto the stack and move to the next state. If the action is a reduce, replace the top of the stack with the left-hand side of the grammar rule. If the action is a go to, move to the next state without changing the stack. Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker. SLR Parsing: Example Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id And the example input id + id . We start by initializing the stack with the start symbol E and the end marker $ . Then we read the first input symbol id . According to the parsing table, we shift id onto the stack and move to state 2. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected. Expanding the Concepts SLR Parsing SLR (Simple LR) parsing is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. For each reduction item A \u2192 \u03b3\u00b7 , the parser looks at the lookahead symbol c . It applies the reduction only if c is in the FOLLOW(A) set, which contains all the terminal symbols that can appear immediately after A in a sentential form derived from the start symbol. SLR Parsing Scope The scope of an SLR parser refers to the set of languages it can recognize. An SLR parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion. However, unlike LR(0) parsers, SLR parsers can handle certain types of ambiguous grammars by considering the lookahead symbol when deciding whether to shift or reduce. SLR Parsing Limitations Despite its advantages, SLR parsing still has limitations. One of the main limitations is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities. Additionally, the SLR parsing algorithm requires additional computational resources compared to LR(0) parsing due to the need to calculate the FOLLOW(A) set for each non-terminal. Show That the Following Grammar Is Not SLR Let's consider the following grammar: (0) S' \u2192 S$ (1) S \u2192 AaAb (2) S \u2192 BbBa (3) A \u2192 \u03b5 (4) B \u2192 \u03b5 If we try to construct the SLR parsing table for this grammar, we will encounter two reduce-reduce conflicts in the action cells for state 0, a and state 0, b . This is because both A \u2192 \u03b5 and B \u2192 \u03b5 are applicable in these cases, and neither can be decided upon solely based on the lookahead symbol. Therefore, this grammar is not SLR. In general, to show that a grammar is not SLR, we need to construct the SLR parsing table and check for any reduce-reduce conflicts. If we find any such conflicts, then the grammar is not SLR. SLR Parsing: Example Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id And the example input id + id . We start by initializing the stack with the start symbol E and the end marker $ . Then we read the first input symbol id . According to the parsing table, we shift id onto the stack and move to state 2. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.","title":"Unit 13 SLR parsing"},{"location":"lectures/13_SLR_parsing/#simple-lr-parsing-slr-or-slr1","text":"","title":"Simple LR Parsing (SLR or SLR(1))"},{"location":"lectures/13_SLR_parsing/#_1","text":"Simple LR parsing, also known as SLR or SLR(1), is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. For each reduction item A \u2192 \u03b3\u00b7 , the parser looks at the lookahead symbol c . It applies the reduction only if c is in the FOLLOW(A) set, which contains all the terminal symbols that can appear immediately after A in a sentential form derived from the start symbol.","title":""},{"location":"lectures/13_SLR_parsing/#slr-parsing-table","text":"The SLR parsing table eliminates some conflicts compared to the LR(0) table. It is essentially the same as the LR(0) table, but with reduced rows. Reductions do not fill entire rows. Instead, reductions A \u2192 \u03b3\u00b7 are added only in the columns of symbols in FOLLOW(A) .","title":"SLR Parsing Table"},{"location":"lectures/13_SLR_parsing/#slr-grammar","text":"An SLR grammar is a context-free grammar for which the SLR parsing table does not have any conflicts. In other words, an SLR grammar is one that can be parsed by an SLR parser without encountering any shift/reduce or reduce/reduce conflicts.","title":"SLR Grammar"},{"location":"lectures/13_SLR_parsing/#slr-parsing-example","text":"The SLR parsing algorithm is similar to the LR(0) parsing algorithm, but with the addition of considering the lookahead symbol when deciding whether to shift or reduce. Here is a simplified version of the algorithm: Initialize the stack with the start symbol of the grammar and the special end marker $ . Read the first input symbol. Look up the current state and input symbol in the ACTION field of the parsing table to get the action to perform. If the action is a shift, push the input symbol onto the stack and move to the next state. If the action is a reduce, replace the top of the stack with the left-hand side of the grammar rule. If the action is a go to, move to the next state without changing the stack. Repeat steps 2-4 until the entire input has been read and the stack contains only the start symbol and the end marker.","title":"SLR Parsing: Example"},{"location":"lectures/13_SLR_parsing/#slr-parsing-example_1","text":"Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id And the example input id + id . We start by initializing the stack with the start symbol E and the end marker $ . Then we read the first input symbol id . According to the parsing table, we shift id onto the stack and move to state 2. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.","title":"SLR Parsing: Example"},{"location":"lectures/13_SLR_parsing/#expanding-the-concepts","text":"","title":"Expanding the Concepts"},{"location":"lectures/13_SLR_parsing/#slr-parsing","text":"SLR (Simple LR) parsing is an extension of LR(0) parsing. It introduces a single token of lookahead to help resolve conflicts between shift and reduce operations. For each reduction item A \u2192 \u03b3\u00b7 , the parser looks at the lookahead symbol c . It applies the reduction only if c is in the FOLLOW(A) set, which contains all the terminal symbols that can appear immediately after A in a sentential form derived from the start symbol.","title":"SLR Parsing"},{"location":"lectures/13_SLR_parsing/#slr-parsing-scope","text":"The scope of an SLR parser refers to the set of languages it can recognize. An SLR parser can recognize a context-free language if and only if the language is unambiguous and does not contain left recursion. However, unlike LR(0) parsers, SLR parsers can handle certain types of ambiguous grammars by considering the lookahead symbol when deciding whether to shift or reduce.","title":"SLR Parsing Scope"},{"location":"lectures/13_SLR_parsing/#slr-parsing-limitations","text":"Despite its advantages, SLR parsing still has limitations. One of the main limitations is that it cannot handle certain types of ambiguous grammars, such as those with left recursion or grammars that require more than one token of lookahead to resolve ambiguities. Additionally, the SLR parsing algorithm requires additional computational resources compared to LR(0) parsing due to the need to calculate the FOLLOW(A) set for each non-terminal.","title":"SLR Parsing Limitations"},{"location":"lectures/13_SLR_parsing/#show-that-the-following-grammar-is-not-slr","text":"Let's consider the following grammar: (0) S' \u2192 S$ (1) S \u2192 AaAb (2) S \u2192 BbBa (3) A \u2192 \u03b5 (4) B \u2192 \u03b5 If we try to construct the SLR parsing table for this grammar, we will encounter two reduce-reduce conflicts in the action cells for state 0, a and state 0, b . This is because both A \u2192 \u03b5 and B \u2192 \u03b5 are applicable in these cases, and neither can be decided upon solely based on the lookahead symbol. Therefore, this grammar is not SLR. In general, to show that a grammar is not SLR, we need to construct the SLR parsing table and check for any reduce-reduce conflicts. If we find any such conflicts, then the grammar is not SLR.","title":"Show That the Following Grammar Is Not SLR"},{"location":"lectures/13_SLR_parsing/#slr-parsing-example_2","text":"Let's consider a simple grammar: E -> E + T | T T -> F | (E) F -> id And the example input id + id . We start by initializing the stack with the start symbol E and the end marker $ . Then we read the first input symbol id . According to the parsing table, we shift id onto the stack and move to state 2. We continue this process until we have consumed all the input symbols. At the end of the parsing process, if the stack contains only the start symbol and the end marker, the input string is accepted. Otherwise, it is rejected.","title":"SLR Parsing: Example"},{"location":"lectures/14_LR1_parsing/","text":"LR(1) Parsing and LR(1) Grammars LR(1) parsing, also known as canonical LR(1) parsing, is an extension of LR(0) parsing. It uses similar concepts as SLR, but it uses one lookahead symbol instead of none. The idea is to get as much as possible out of one lookahead symbol. The LR(1) item is an LR(0) item combined with lookahead symbols possibly following the production locally within the same item set. For instance, an LR(0) item could be S \u2192 \u00b7S + E , and an LR(1) item could be S \u2192 \u00b7S + E , + . Similar to SLR parsing, lookahead only impacts reduce operations in LR(1). If the LR(1) parsing action function has no multiply defined entries, then the given grammar is called an LR(1) grammar. LR(1) Closure Similar to LR(0) closure, but also keeps track of the lookahead symbol. If I is a set of items, CLOSURE(I) is the set of items such that: Initially, every item in I is in CLOSURE(I) , If A \u2192 \u03b1 \u00b7 B and B \u2192 \u03b3 is a production whose closures are not in I then add the item B \u2192 \u03b3 , FIRST(\u03b2) to CLOSURE(I) . In step (2) if \u03b2 \u2192 \u03b5 then add the item B \u2192 \u03b3 , \u03b4 to CLOSURE(I) . For recursive items with form A \u2192 \u00b7A\u03b1 , \u03b4 and `A \u2192 \u00b7\u03b2 , \u03b4 replace the items with A \u2192 \u00b7A\u03b1 , \u03b4, FIRST(\u03b1) and A \u2192 \u00b7\u03b2 , \u03b4, FIRST(\u03b1) . Apply these steps (2), (3), and (4) until no more new items can be added to CLOSURE(I) . LR(1) GOTO and States Initial state: start with [S' \u2192 S$ , $] as the kernel of I0 , then apply the CLOSURE(I) operation. The GOTO function is analogous to GOTO in LR(0) parsing. LR(1) Items An LR(1) item is a pair [\u03b1; \u03b2] , where \u03b1 is a production from the grammar with a dot at some position in the RHS and \u03b2 is a lookahead string containing one symbol (terminal or EOF). What about LR(1) items? Several LR(1) items may have the same core. For instance, [A ::= X \u00b7 Y Z; a] and [A ::= X \u00b7 Y Z; b] would be represented as [A ::= X \u00b7 Y Z; {a, b}] . LR(1) Parsing and LR(1) Grammars LR(1) parsing, also known as canonical LR(1) parsing, is an extension of LR(0) parsing. It uses similar concepts as SLR, but it uses one lookahead symbol instead of none. The idea is to get as much as possible out of one lookahead symbol. The LR(1) item is an LR(0) item combined with lookahead symbols possibly following the production locally within the same item set. For instance, an LR(0) item could be S \u2192 \u00b7S + E , and an LR(1) item could be S \u2192 \u00b7S + E , + . Similar to SLR parsing, lookahead only impacts reduce operations in LR(1). If the LR(1) parsing action function has no multiply defined entries, then the given grammar is called an LR(1) grammar. LR(1) Parsing Table: Example Let's consider the following grammar: (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d First, we augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker $ . Then we compute the closure of the set of items derived from the new start symbol. We repeat this process for each new state until no more new states can be added. For each state and input symbol, we determine the action to take (shift, reduce, or go to) based on the transitions in the state machine. Finally, we write the resulting actions into the ACTION and GOTO fields of the parsing table. LR(1) Parsing Table: Exercise Let's consider the following grammar: S \u2192 E + S | E E \u2192 num We would follow the same steps as in the example to construct the LR(1) parsing table for this grammar. However, since the grammar is quite simple, the resulting table should also be relatively straightforward. Remember, the goal is to identify any conflicts in the parsing table. A conflict occurs when there are multiple possible actions for a given state and input symbol. If there are no conflicts, then the grammar is suitable for LR(1) parsing.","title":"Unit 14 LR(1) parsing"},{"location":"lectures/14_LR1_parsing/#lr1-parsing-and-lr1-grammars","text":"","title":"LR(1) Parsing and LR(1) Grammars"},{"location":"lectures/14_LR1_parsing/#_1","text":"LR(1) parsing, also known as canonical LR(1) parsing, is an extension of LR(0) parsing. It uses similar concepts as SLR, but it uses one lookahead symbol instead of none. The idea is to get as much as possible out of one lookahead symbol. The LR(1) item is an LR(0) item combined with lookahead symbols possibly following the production locally within the same item set. For instance, an LR(0) item could be S \u2192 \u00b7S + E , and an LR(1) item could be S \u2192 \u00b7S + E , + . Similar to SLR parsing, lookahead only impacts reduce operations in LR(1). If the LR(1) parsing action function has no multiply defined entries, then the given grammar is called an LR(1) grammar.","title":""},{"location":"lectures/14_LR1_parsing/#lr1-closure","text":"Similar to LR(0) closure, but also keeps track of the lookahead symbol. If I is a set of items, CLOSURE(I) is the set of items such that: Initially, every item in I is in CLOSURE(I) , If A \u2192 \u03b1 \u00b7 B and B \u2192 \u03b3 is a production whose closures are not in I then add the item B \u2192 \u03b3 , FIRST(\u03b2) to CLOSURE(I) . In step (2) if \u03b2 \u2192 \u03b5 then add the item B \u2192 \u03b3 , \u03b4 to CLOSURE(I) . For recursive items with form A \u2192 \u00b7A\u03b1 , \u03b4 and `A \u2192 \u00b7\u03b2 , \u03b4 replace the items with A \u2192 \u00b7A\u03b1 , \u03b4, FIRST(\u03b1) and A \u2192 \u00b7\u03b2 , \u03b4, FIRST(\u03b1) . Apply these steps (2), (3), and (4) until no more new items can be added to CLOSURE(I) .","title":"LR(1) Closure"},{"location":"lectures/14_LR1_parsing/#lr1-goto-and-states","text":"Initial state: start with [S' \u2192 S$ , $] as the kernel of I0 , then apply the CLOSURE(I) operation. The GOTO function is analogous to GOTO in LR(0) parsing.","title":"LR(1) GOTO and States"},{"location":"lectures/14_LR1_parsing/#lr1-items","text":"An LR(1) item is a pair [\u03b1; \u03b2] , where \u03b1 is a production from the grammar with a dot at some position in the RHS and \u03b2 is a lookahead string containing one symbol (terminal or EOF). What about LR(1) items? Several LR(1) items may have the same core. For instance, [A ::= X \u00b7 Y Z; a] and [A ::= X \u00b7 Y Z; b] would be represented as [A ::= X \u00b7 Y Z; {a, b}] .","title":"LR(1) Items"},{"location":"lectures/14_LR1_parsing/#lr1-parsing-and-lr1-grammars_1","text":"LR(1) parsing, also known as canonical LR(1) parsing, is an extension of LR(0) parsing. It uses similar concepts as SLR, but it uses one lookahead symbol instead of none. The idea is to get as much as possible out of one lookahead symbol. The LR(1) item is an LR(0) item combined with lookahead symbols possibly following the production locally within the same item set. For instance, an LR(0) item could be S \u2192 \u00b7S + E , and an LR(1) item could be S \u2192 \u00b7S + E , + . Similar to SLR parsing, lookahead only impacts reduce operations in LR(1). If the LR(1) parsing action function has no multiply defined entries, then the given grammar is called an LR(1) grammar.","title":"LR(1) Parsing and LR(1) Grammars"},{"location":"lectures/14_LR1_parsing/#lr1-parsing-table-example","text":"Let's consider the following grammar: (0) S' \u2192 S$ (1) S \u2192 CC (2) C \u2192 aC (3) C \u2192 d First, we augment the grammar by adding a new start symbol and a new production that starts with the old start symbol followed by a special end marker $ . Then we compute the closure of the set of items derived from the new start symbol. We repeat this process for each new state until no more new states can be added. For each state and input symbol, we determine the action to take (shift, reduce, or go to) based on the transitions in the state machine. Finally, we write the resulting actions into the ACTION and GOTO fields of the parsing table.","title":"LR(1) Parsing Table: Example"},{"location":"lectures/14_LR1_parsing/#lr1-parsing-table-exercise","text":"Let's consider the following grammar: S \u2192 E + S | E E \u2192 num We would follow the same steps as in the example to construct the LR(1) parsing table for this grammar. However, since the grammar is quite simple, the resulting table should also be relatively straightforward. Remember, the goal is to identify any conflicts in the parsing table. A conflict occurs when there are multiple possible actions for a given state and input symbol. If there are no conflicts, then the grammar is suitable for LR(1) parsing.","title":"LR(1) Parsing Table: Exercise"},{"location":"lectures/lexical_analysis/","text":"Lexical analysis To be announced.","title":"Lexical analysis"},{"location":"lectures/lexical_analysis/#lexical-analysis","text":"To be announced.","title":"Lexical analysis"},{"location":"projects/","text":"Compiler projects agenda I designed and planned some practical projects about the applications of compiler science in program analysis. The projects shown in Table 1 have been assigned to the students who take the IUST compiler course during different semesters. Click on the link in the \"Project\" column to see the project proposal. Table 1: Compiler projects. Project Description Semesters Courses OpenUnderstand 2 Low-level source code metrics calculation Spring 2022 Compiler OpenUnderstand Symbols table development Fall 2021, Spring 2022, Compiler QualityMeter - Source code quality attribute computation - Refactoring opportunity detection Fall 2021 Advanced compiler CodART 2 Source code smell detection Spring 2021 (Cancelled) Compiler CodART Source code refactoring Fall 2020, Spring 2021, Compiler CodART Refactoring to design pattern at the source code level Fall 2020 Advanced compiler CleanCode Source code smell detection Fall 2019, Spring 2020 Compiler CodA Source code instrumentation and testbed analysis tool Fall 2018 Compiler / Advanced compiler ANTLR MiniJava Parse-tree and intermediate code generation for the MiniJava programming language with ANTLR Fall 2016, Spring 2017 Compiler","title":"Index"},{"location":"projects/#compiler-projects-agenda","text":"I designed and planned some practical projects about the applications of compiler science in program analysis. The projects shown in Table 1 have been assigned to the students who take the IUST compiler course during different semesters. Click on the link in the \"Project\" column to see the project proposal. Table 1: Compiler projects. Project Description Semesters Courses OpenUnderstand 2 Low-level source code metrics calculation Spring 2022 Compiler OpenUnderstand Symbols table development Fall 2021, Spring 2022, Compiler QualityMeter - Source code quality attribute computation - Refactoring opportunity detection Fall 2021 Advanced compiler CodART 2 Source code smell detection Spring 2021 (Cancelled) Compiler CodART Source code refactoring Fall 2020, Spring 2021, Compiler CodART Refactoring to design pattern at the source code level Fall 2020 Advanced compiler CleanCode Source code smell detection Fall 2019, Spring 2020 Compiler CodA Source code instrumentation and testbed analysis tool Fall 2018 Compiler / Advanced compiler ANTLR MiniJava Parse-tree and intermediate code generation for the MiniJava programming language with ANTLR Fall 2016, Spring 2017 Compiler","title":"Compiler projects agenda"},{"location":"projects/core_clean_code_development/","text":"Core clean code development The input to our software tool, CleanCode , is a c# class. cleanCode analyzes the source code and determines how clean the code is. The output is a list of the line numbers of the given class, in which the clean code principles proposed by Robert Martin, in his book, Clean Code, are violated. The current version of cleanCode, checks 14 principles and we are going to add more principles in our next version. Click to visit clean code project .","title":"Clean code"},{"location":"projects/core_clean_code_development/#core-clean-code-development","text":"The input to our software tool, CleanCode , is a c# class. cleanCode analyzes the source code and determines how clean the code is. The output is a list of the line numbers of the given class, in which the clean code principles proposed by Robert Martin, in his book, Clean Code, are violated. The current version of cleanCode, checks 14 principles and we are going to add more principles in our next version. Click to visit clean code project .","title":"Core clean code development"},{"location":"projects/core_code_smell_development/","text":"Core code smell development The following proposal has been initially prepared for the IUST Compiler and Advanced Software Engineering courses in Winter and Spring 2021 . Note: Before reading this proposal ensure that you have read and understood the CodART white-paper . Students may form groups of up to three persons. Each group must develop mechanisms for a subset of code smells listed in Table 2 . The exact list of code smells will be assigned to each group subsequently. The refactoring operations in Table 1 and code smells in Table 2 may update during the semester. To facilitate and organized the development process, this proposal defines the project in various phases. The project is divided into three separate phases. In the first phase, students must read about refactoring and code smells and understand the current state of the CodART completely. As a practice, they are asked to fix the existing issues on the project repository about refactoring operations developed in the first proposal. In the second phase, each group is asked to develop algorithms to automatically detect one or more code smells in a given Java project using ANTLR tool and other compiler techniques. TA team frequently helps the students at this phase to develop their algorithms. In the third phase, each group is asked to connect the code smells detection scripts to the corresponding refactoring and automate the overall quality improvement process. Grading policy for BSc students Table 6 shows the grading policy for the BSc students. It may change in the future. Table 6. grading policy for BSc students Activity Score (100) Understanding the CodART project and Fix the existing issues 30 Implementing smell detection approaches 40 Connecting code smells to refactoring and harnessing the overall process 20 Documenting the new source codes and pushing them to GitHub 10 Testing project on all projects available in CodART benchmarks 20+ (extra bonus) Grading policy for MSc students Table 7 shows the grading policy for the MSc students. It may change in the future. Table 7. grading policy for MSc students Activity Score (100) Understanding the paper and presenting it 20 Implementing the paper 30 Evaluating the implementation 30 Documenting the project 20 Testing project on all projects available in CodART benchmarks 20+ (extra bonus) To follow project's future phases, meet our next proposal: Core search-based development.","title":"Code smell detection"},{"location":"projects/core_code_smell_development/#core-code-smell-development","text":"The following proposal has been initially prepared for the IUST Compiler and Advanced Software Engineering courses in Winter and Spring 2021 . Note: Before reading this proposal ensure that you have read and understood the CodART white-paper . Students may form groups of up to three persons. Each group must develop mechanisms for a subset of code smells listed in Table 2 . The exact list of code smells will be assigned to each group subsequently. The refactoring operations in Table 1 and code smells in Table 2 may update during the semester. To facilitate and organized the development process, this proposal defines the project in various phases. The project is divided into three separate phases. In the first phase, students must read about refactoring and code smells and understand the current state of the CodART completely. As a practice, they are asked to fix the existing issues on the project repository about refactoring operations developed in the first proposal. In the second phase, each group is asked to develop algorithms to automatically detect one or more code smells in a given Java project using ANTLR tool and other compiler techniques. TA team frequently helps the students at this phase to develop their algorithms. In the third phase, each group is asked to connect the code smells detection scripts to the corresponding refactoring and automate the overall quality improvement process.","title":"Core code smell development"},{"location":"projects/core_code_smell_development/#grading-policy-for-bsc-students","text":"Table 6 shows the grading policy for the BSc students. It may change in the future. Table 6. grading policy for BSc students Activity Score (100) Understanding the CodART project and Fix the existing issues 30 Implementing smell detection approaches 40 Connecting code smells to refactoring and harnessing the overall process 20 Documenting the new source codes and pushing them to GitHub 10 Testing project on all projects available in CodART benchmarks 20+ (extra bonus)","title":"Grading policy for BSc students"},{"location":"projects/core_code_smell_development/#grading-policy-for-msc-students","text":"Table 7 shows the grading policy for the MSc students. It may change in the future. Table 7. grading policy for MSc students Activity Score (100) Understanding the paper and presenting it 20 Implementing the paper 30 Evaluating the implementation 30 Documenting the project 20 Testing project on all projects available in CodART benchmarks 20+ (extra bonus) To follow project's future phases, meet our next proposal: Core search-based development.","title":"Grading policy for MSc students"},{"location":"projects/core_refactoring_to_design_patterns_development/","text":"Core refactoring to design patterns development To be announced.","title":"Refactoring to patterns"},{"location":"projects/core_refactoring_to_design_patterns_development/#core-refactoring-to-design-patterns-development","text":"To be announced.","title":"Core refactoring to design patterns development"},{"location":"projects/core_refactorings_development/","text":"Core refactoring development The following proposal was initially prepared for the IUST Compiler and Advanced compiler courses in Fall 2020. Students must form groups of up to three persons, and each group must implement several refactoring operations. The exact list of refactoring will be assigned to each group subsequently. The refactoring operations in Table 1 may update during the semester. As an example of refactoring automation, we have implemented the EncapsulateField refactoring, illustrated in Figure 1. A na\u00efve implementation is available on the project official Github page at https://m-zakeri.github.io/CodART . In addition, 26 refactoring operations in Table 1 have been implemented by MultiRefactor [7] based on RECODER , three of them have been implemented by JDeodrant [8], and other operations have been automated in [3], [6]. RECODER extracts a model of the code that can be used to analyze and modify the code before the changes are applied and written to file. The tool takes Java source code as input and will output the modified source code to a specified folder. The input must be fully compilable and must be accompanied by any necessary library files as compressed jar files. Grading policy for BSc students Table 4 shows the grading policy for the BSc students. It may change in the future. Table 4. grading policy for BSc students Activity Score (100) Refactoring operations implementation (moderate level) 50 Evaluation of the tool on the benchmark projects 30 Documentations 20 Search-based refactoring recommendation 30+ (extra bonus) Grading policy for MSc students Table 5 shows the grading policy for the MSc students. It may change in the future. Table 5. grading policy for MSc students Activity Score (100) Refactoring operations implementation (advanced level) 40 Search-based refactoring recommendation 30 Evaluation of the tool on the benchmark projects 20 Documentations 10 Improving the state-of-the-arts papers 30+ (extra bonus) To follow project's phases, refer to our next proposal: Core code smell development.","title":"Source code refactoring"},{"location":"projects/core_refactorings_development/#core-refactoring-development","text":"The following proposal was initially prepared for the IUST Compiler and Advanced compiler courses in Fall 2020. Students must form groups of up to three persons, and each group must implement several refactoring operations. The exact list of refactoring will be assigned to each group subsequently. The refactoring operations in Table 1 may update during the semester. As an example of refactoring automation, we have implemented the EncapsulateField refactoring, illustrated in Figure 1. A na\u00efve implementation is available on the project official Github page at https://m-zakeri.github.io/CodART . In addition, 26 refactoring operations in Table 1 have been implemented by MultiRefactor [7] based on RECODER , three of them have been implemented by JDeodrant [8], and other operations have been automated in [3], [6]. RECODER extracts a model of the code that can be used to analyze and modify the code before the changes are applied and written to file. The tool takes Java source code as input and will output the modified source code to a specified folder. The input must be fully compilable and must be accompanied by any necessary library files as compressed jar files.","title":"Core refactoring development"},{"location":"projects/core_refactorings_development/#grading-policy-for-bsc-students","text":"Table 4 shows the grading policy for the BSc students. It may change in the future. Table 4. grading policy for BSc students Activity Score (100) Refactoring operations implementation (moderate level) 50 Evaluation of the tool on the benchmark projects 30 Documentations 20 Search-based refactoring recommendation 30+ (extra bonus)","title":"Grading policy for BSc students"},{"location":"projects/core_refactorings_development/#grading-policy-for-msc-students","text":"Table 5 shows the grading policy for the MSc students. It may change in the future. Table 5. grading policy for MSc students Activity Score (100) Refactoring operations implementation (advanced level) 40 Search-based refactoring recommendation 30 Evaluation of the tool on the benchmark projects 20 Documentations 10 Improving the state-of-the-arts papers 30+ (extra bonus) To follow project's phases, refer to our next proposal: Core code smell development.","title":"Grading policy for MSc students"},{"location":"projects/core_software_metrics_development/","text":"Core software metrics development Visit QualityMeter project repository .","title":"Software metrics"},{"location":"projects/core_software_metrics_development/#core-software-metrics-development","text":"Visit QualityMeter project repository .","title":"Core software metrics development"},{"location":"projects/core_source_code_instrumentation_development/","text":"Core source code instrumentation development Visit the CodA project documentation .","title":"Source code instrumentation"},{"location":"projects/core_source_code_instrumentation_development/#core-source-code-instrumentation-development","text":"Visit the CodA project documentation .","title":"Core source code instrumentation development"},{"location":"projects/core_symbol_table_development/","text":"Core symbol table development Visit OpenUnderstand project documentation .","title":"Symbol table"},{"location":"projects/core_symbol_table_development/#core-symbol-table-development","text":"Visit OpenUnderstand project documentation .","title":"Core symbol table development"},{"location":"projects/mini_java_compiler_development/","text":"Mini-Java compiler development The project includes parse three and intermediate code generation for mini-Java programming language with ANTLR.","title":"MiniJava compiler development"},{"location":"projects/mini_java_compiler_development/#mini-java-compiler-development","text":"The project includes parse three and intermediate code generation for mini-Java programming language with ANTLR.","title":"Mini-Java compiler development"}]}